<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"/>

<link rel="stylesheet" href="/css/main.css?v=6.7.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="论文[1] 时空异常探测方法研究综述什么是异常异常探测旨在从海量数据中挖掘不符合普适性规律、表现出“与众不同”特性的数据或模式 “异 常 ”亦 称 为 离 群 点 、孤 立 点（所以说 abnormal 和 outlier 是一回事）  1980 年 ，Hawkins给出异常的本质性定义，即“严重偏离其他对象 的观测数据，以至于令人怀疑它是由不同机制产生 的 ” 1994 年 ，Barnet t">
<meta name="keywords" content="Computer-Vision,Cognition,Attention">
<meta property="og:type" content="article">
<meta property="og:title" content="Notes 2019-01">
<meta property="og:url" content="http://lowrank.science/Notes 2019-01/index.html">
<meta property="og:site_name" content="Grok">
<meta property="og:description" content="论文[1] 时空异常探测方法研究综述什么是异常异常探测旨在从海量数据中挖掘不符合普适性规律、表现出“与众不同”特性的数据或模式 “异 常 ”亦 称 为 离 群 点 、孤 立 点（所以说 abnormal 和 outlier 是一回事）  1980 年 ，Hawkins给出异常的本质性定义，即“严重偏离其他对象 的观测数据，以至于令人怀疑它是由不同机制产生 的 ” 1994 年 ，Barnet t">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Measure%20of%20Memory.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Memory%20Fomula.jpg">
<meta property="og:image" content="http://www.52nlp.cn/wp-content/uploads/2018/12/%E5%9B%BE%E7%89%87-1-11.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Overview.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Decoders.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Eq%2010.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Fig%205.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Eq%205.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Eq%204.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/DES%20Fig%202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/DES%20Fig%201.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/DES%20Fig%203.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Residual%20Attention%20Network%20Fig%202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Residual%20Attention%20Network%20Fig%203.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Residual%20Attention%20Network%20Eq%204%205%206.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Semantic%20Point%20Detector%20Fig%202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Semantic%20Point%20Detector%20Fig%203.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Semantic%20Point%20Detector%20Fig%201.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Huang%20Review%20Fig%201.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Huang%20Review%20Tab%201.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Huang%20Review%20Tab%202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Huang%20Review%20Tab%203.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Yohanandan%20ECCV%2018%20Fig%202a.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Yohanandan%20Selective%20Attention%20Fig%202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Yohanandan%20ECCV%2018%20Fig%202b.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Yohanandan%20Selective%20Attention%20Fig%203.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Alipay_Middle.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Wechat_Middle.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Heitz%20ECCV%202008%20Fig%202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Perko%20WAPCV%202007%20Fig%202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Perko%20WAPCV%202007%20Fig%206.png">
<meta property="og:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Jianan%20Li%20TMM%202017.png">
<meta property="og:updated_time" content="2019-03-11T23:05:40.190Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Notes 2019-01">
<meta name="twitter:description" content="论文[1] 时空异常探测方法研究综述什么是异常异常探测旨在从海量数据中挖掘不符合普适性规律、表现出“与众不同”特性的数据或模式 “异 常 ”亦 称 为 离 群 点 、孤 立 点（所以说 abnormal 和 outlier 是一回事）  1980 年 ，Hawkins给出异常的本质性定义，即“严重偏离其他对象 的观测数据，以至于令人怀疑它是由不同机制产生 的 ” 1994 年 ，Barnet t">
<meta name="twitter:image" content="https://raw.githubusercontent.com/YimianDai/images/master/Measure%20of%20Memory.jpg">






  <link rel="canonical" href="http://lowrank.science/Notes 2019-01/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Notes 2019-01 | Grok</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Grok</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">抱残守缺</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lowrank.science/Notes 2019-01/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimian Dai"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Grok"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Notes 2019-01

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-31 00:00:00" itemprop="dateCreated datePublished" datetime="2019-01-31T00:00:00-07:00">2019-01-31</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-11 16:05:40" itemprop="dateModified" datetime="2019-03-11T16:05:40-07:00">2019-03-11</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Research/" itemprop="url" rel="index"><span itemprop="name">Research</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="1-时空异常探测方法研究综述"><a href="#1-时空异常探测方法研究综述" class="headerlink" title="[1] 时空异常探测方法研究综述"></a>[1] 时空异常探测方法研究综述</h3><h4 id="什么是异常"><a href="#什么是异常" class="headerlink" title="什么是异常"></a>什么是异常</h4><p>异常探测旨在从海量数据中挖掘不符合普适性规律、表现出“与众不同”特性的数据或模式</p>
<p>“异 常 ”亦 称 为 离 群 点 、孤 立 点（所以说 abnormal 和 outlier 是一回事）</p>
<ol>
<li>1980 年 ，Hawkins给出<strong>异常的本质性定义</strong>，即“<strong>严重偏离其他对象 的观测数据，以至于令人怀疑它是由不同机制产生 的</strong> ”</li>
<li>1994 年 ，Barnet t 等 进 一 步 从 <strong>统 计 学 的 角 度</strong> 给出“<strong>异常是指与数据集中其余数据分布不一致的观测数据或观测数据子集</strong>”</li>
<li>2003 年 ，Shekhar 等[6]考虑到空间数据的特性，将传统异常在空间数据中进行了扩展，并将<strong>空间异常</strong>定义为“<strong>专题属性与 其邻近空间实体显著不同，而在整体数据范围内差 异 可 能 不 明 显 的 空 间 实 体</strong> ”</li>
<li>2006 年 ，Cheng 等 在 空 间异常的基础上，从空间域进一步扩展到时空域，给 出<strong>时空异常</strong>的定义，即“<strong>专题属性值严重偏离其时间 或(和)空间邻近域内参考实体的时空实体</strong>”</li>
</ol>
<h4 id="现有方法的局限性"><a href="#现有方法的局限性" class="headerlink" title="现有方法的局限性:"></a>现有方法的局限性:</h4><ol>
<li>现有异常探测方法普遍不适用于高维数据的异常探测;</li>
<li>很多方法需要先验知识的指导，自适应能力差;</li>
<li>缺乏对异常探测结果的有效性评价。</li>
</ol>
<p>时空异常探测的具体例子：在 气象方面，预测台风路径突然变化的原因对提前发 出疏散指令起到至关重要的作用;预测某个地区不 寻常的降水行为将有助于对突如其来的洪涝灾害等 极端事件做好充分准备</p>
<p>时空点事件中的异常主要包括<strong>离群</strong>和<strong>热点</strong>两类。其中，<strong>时空离群</strong>指那些不属于任何时空簇的孤 立点事件以及仅包含少量时空点事件的小簇；时空热点指那些局部聚集程度显著偏大的簇</p>
<h4 id="传统异常探测方法"><a href="#传统异常探测方法" class="headerlink" title="传统异常探测方法"></a>传统异常探测方法</h4><ol>
<li><strong>基于统计的方法</strong>。该方法的基本思想是根 据数据集的特性先假定一个数据分布的概率模型， 然后根据模型的不一致性确定异常。该 类方法的优点是建立在成熟的统计学理论基础上， 异常含义明确;其缺陷是数据集的概率模型一般未 知，估计时难免出现误差甚至背离现实的错误。</li>
<li><strong>基于距离的方法</strong>。该方法的基本思想是以 对象间距离的大小检测异常，<strong>将那些没有足够邻居的对象识别为异常</strong>。</li>
<li>基于密度的方法。为了探测数据集中的局 部 异 常 实 体 ，Breunig 等 [14 ] 在 基 于 距 离 探 测 方 法 的 基础上引入局部密度的概念，提出一种基于密度的 探测方法———LOF算法(图2a)。借助实体的局部可达密度定义局部异常度，异常度与局部密度成反比，将异常度较大的实体识别为异常。</li>
<li>基于角度的方法。该方法通过度量实体与其邻域内其 他实体所构成的角度定义异常度，角度越大，异常度 越 小 ， 反 之 异 常 度 越 大；然而，当数据呈线性分布时，基于角度的方法难以准 确探测异常</li>
<li>基于聚类的方法。该方法的基本思想是将 异常探测过程转换成聚类过程。聚类的目的在于将 数据集划分为若干簇，并且簇内实体间距离尽可能 小，簇间实体间距离尽可能大，<strong>将聚类后那些不隶属于任何簇的实体识别为异常</strong></li>
</ol>
<h3 id="2-弱监督深层神经网络遥感图像目标检测模型"><a href="#2-弱监督深层神经网络遥感图像目标检测模型" class="headerlink" title="[2] 弱监督深层神经网络遥感图像目标检测模型"></a>[2] 弱监督深层神经网络遥感图像目标检测模型</h3><p>中国科学: 信息科学 2018年 </p>
<p>使用全卷积网络提取遥感图像中可能存在待检测目标的候选区域（用 FCN 来做 Region Proposal，说是避免了对图像 的穷举搜索，但感觉很怪，因为 Semantic Segmentation 需要 pixel-wise 的 Annotation，其实是比 BBox 更高的要求，感觉有点倒置，但这里的 Semantic Segmentation 也是用 WSL 的，从而避免了 Annotation 比 Detection 更高这一点）</p>
<p>FCN模型负责把待检测图像转化为粗分割图像,可能含有飞机的区域在粗分割图像中 会被高亮标记. 然后使用滑动窗口方法在标记区域提取出候选区域. CNN 模型用于对所有候选区域进 行特征提取和分类. 分类的输出是二值的, 即是否是含有整个目标的区域. </p>
<p>用图像级标签替换像素级标签完成 FCN 模型的训练</p>
<p>怎么用 WSL 来做 语义分割？</p>
<p>在进行训练时, 把飞机样本图像中的所有像素点的标签置为 1, 把背景样本图像中 的所有像素点的标签置为 0, 即获得 FCN 模型需要的像素级标签. 也就是在获取 FCN 训练样本时, 既不用把表示飞机的像素区别开来, 也不需要使用矩形框定位飞机的位置, 只需要按照图像的内容为 所有像素打上统一标签即可.</p>
<p>使用FCN模型和滑动窗口方法来实现候选区域的提取</p>
<p>待检测图像输入训练好的 FCN 模型 后, 得到的是一幅粗分割图, 其中可能包含飞机的区域被置为 1, 可能是背景的区域被置为 0. 这种类似于显著性检测的机制为目标区域的选取提供了先验信息, 可以避免在原图像中的穷尽搜索.（<strong>但是还是要在 粗分割的目标区域内 穷尽搜索吗？</strong>）</p>
<p>怎么确定候选区域？</p>
<p>在粗分割结果图上使用滑动窗口进行候选区域的选取. 滑窗的大小固定为 60, 步长为 15, 区域选 取的阈值为 0.65, 即只有一个窗口中 65% 以上的像素点的值都为 1 时该窗口才能被确定为是候选区 域。（这些参数是实验调出来的，其实就是 穷尽搜索）</p>
<p>在 WS-DNN 模型中, CNN 模型的作用是实现对候选区域的特征提取和分类, 即判定候选区域中 是否含有飞机 (含飞机为 1, 背景为 0).</p>
<p>Tamura 等[43]则从人类对纹理的视觉感知心理学角度，提出了度量纹理的 6 种属性:粗糙度(coarseness)、对比度(contrast)、方向度(directionality)、 线 像度(linelikeness)、规整度(regularity)和粗略度(roughness)等</p>
<h3 id="3-人工智能的回顾与展望"><a href="#3-人工智能的回顾与展望" class="headerlink" title="[3] 人工智能的回顾与展望"></a>[3] 人工智能的回顾与展望</h3><p>来自 “双清论坛”专题:人工智能基础理论及应用</p>
<p>我理解的人工智能就是 <strong>理解人类认知</strong>并<strong>建立可计算认知模型</strong></p>
<p>从对大脑观测理解中，抽取对人工智能有启 发性的内容，为脑启发计算或生物计算带来启示，是 目前脑科学与人工智能交叉一个活跃方向（也就是说建立的可计算认知模型是在模仿大脑工作原理的基础上）</p>
<h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><h5 id="深度学习的基本动机"><a href="#深度学习的基本动机" class="headerlink" title="深度学习的基本动机"></a>深度学习的基本动机</h5><p>与 依赖于人工经验、通过手工构建的特征不同，深度学习一般从标注数据出发，通过误差后向传播进行参 数调整以实现端到端的区别性特征学习。<strong>深度学习的基本动机</strong>在于构建多层网络来学习隐含在数据内部的模式，从而从数据中可直接学习更具区别力、更泛化的特征而非手工定义。</p>
<h5 id="深度学习的不足"><a href="#深度学习的不足" class="headerlink" title="深度学习的不足"></a>深度学习的不足</h5><p>深度学习依赖于标注数据，缺乏逻辑推理和对因果关系的表达能力，很难处理具有复杂时空关联性的任务</p>
<p>这一犹如“黑盒子”式的学习模型存在过 度依赖于标注数据，难以有效利用逻辑、先验和知识 等信息，适应环境变化能力不足、在对抗环境下易于 被攻击、结果可解释差等不足。</p>
<p>为了弥补上述不足， 一些研究开始重视在深度学习过程中<strong>引入先验知识</strong>或更加<strong>重视中间特征层</strong>，以建立更具解释性的深度学习模型。</p>
<h4 id="类脑计算"><a href="#类脑计算" class="headerlink" title="类脑计算"></a>类脑计算</h4><p>人类大脑具有感知、识别、学习、联想、记忆和 推理等功能，并非全部用符号计算形式来实现。</p>
<p>至今我们对人类认知功能如何从 复杂动态(时空演变)的大脑神经结构中产生，依然<br>没有形成较为完整的认识。</p>
<p>类脑计算最根本挑战是人类大脑信息处理和认知功能深刻的复杂性。</p>
<h4 id="深度学习中引入记忆机制"><a href="#深度学习中引入记忆机制" class="headerlink" title="深度学习中引入记忆机制"></a>深度学习中引入记忆机制</h4><p>神经记忆的特征主要表现在 如下四个方面:</p>
<ol>
<li>分布式表达和存储</li>
<li>输入信息与被检索记忆信息在内容上具有关联性</li>
<li>对记忆信息的存储和检索具有动态性</li>
<li>记忆与信息处理过程紧密结合</li>
</ol>
<p>如何<strong>在深度学习模型中引入注意力机制和外在记忆体结构</strong>，从而更高效<strong>挖掘数据中感兴趣信息和利用外来信息</strong>，是当前人工智能研究的热点。</p>
<p>这一方面代表性工作是在针对序列数据学习的循环神经网络(Recurrent Neural Network，RNN)中引入“短时记忆”，如LSTM和GRU等模型。其思路在于当前时刻状态的输出会受到过往若干时刻状态的影响，这样学习模型具备了“注意力”机制。注意力模型在机器翻译、语音识别和图文生成等领域取得了成功，这一学习输入序列数据和输出序列数据之中若干单元之间相互影响的注意力机制也被称为“<strong>内在记忆</strong>”。</p>
<p>在端到端深度学习中引入注意力机制和外在记忆体结构，可有效利用当前输入数据数据之外的数据和知识，克服了仅依赖于输入数据进行驱动学习的不足，在零样本学习等方面表现出一定的优势。</p>
<h4 id="智能制造"><a href="#智能制造" class="headerlink" title="智能制造"></a>智能制造</h4><p>如何学习处理<strong>不完备小样本数据</strong>中所包含<strong>碎片化隐性知识</strong>，以解决难以表征生产情境、难以计算生产、控制 和决策中复杂信息的关键技术问题</p>
<h4 id="重点资助方向"><a href="#重点资助方向" class="headerlink" title="重点资助方向"></a>重点资助方向</h4><ol>
<li><strong>脑启发的视觉处理计算架构</strong>。借鉴视觉通道特别是视网膜的信息处理能力，以及<strong>大脑神经连接的网络化结构</strong>，设计和研究新型的视觉计算模型和处理架构。这种架构的组成单元包括从帧驱动到事件驱动的信息获取单元(<strong>智能计算前移</strong>)、注意力选择/事件驱动的信息获取方式、时空动态的信息编码、网络化分布式的动态信息处理、结合长时和短时记忆功能的网络结构，以及条件要素的约束和引导的有效控制。实现大脑结构网络、功能网络和有效 网络在视觉处理架构不同层次的映射。</li>
<li><strong>复杂场景自动理解</strong>。研究<strong>从属性、物体到场景的跨层次关系</strong>发现与相应视觉知识的表示和推理方法;研究<strong>对场景的层次化识别</strong>及与之相关的类别与属性自动发现方法; 研究具有触类旁通能力的识别与学习方法;建立视觉对象的时空特征与语言表达之间的对齐，进而和计算语言学相结合，实现从感知到认知的无缝转换。</li>
</ol>
<p>显然，我学习的方向应该是 复杂场景自动理解。</p>
<h4 id="什么是双清论坛？"><a href="#什么是双清论坛？" class="headerlink" title="什么是双清论坛？"></a>什么是双清论坛？</h4><blockquote>
<p>“双清论坛” 是国家自然科学基金委员会为推动创新文化建设、营造良好创新环境而举办的学术性研讨会。旨在立足于科学基金资助工作，集中研讨科学前沿或国家发展战略需求的深层次科学问题、学科交叉与综合的重大基础科学问题、发展与完善科学基金制的重大政策与管理问题。定名为 “双清论坛”，一是<strong>因为国家自然科学基金委地处双清路</strong>；二是，“双” 的含义是指 “科学与民主”，“清” 的含义是 “正本清源”，即通过倡导科学的精神，弘扬民主的作风，汇聚专家学者的智慧，从而为科学基金资助与管理政策提供决策依据。</p>
</blockquote>
<p>因为基金委在双清路，基金委办的论坛就叫双清论坛了。</p>
<h3 id="4-人工智能中的推理-进展与挑战"><a href="#4-人工智能中的推理-进展与挑战" class="headerlink" title="[4] 人工智能中的推理:进展与挑战"></a>[4] 人工智能中的推理:进展与挑战</h3><h4 id="什么是推理？"><a href="#什么是推理？" class="headerlink" title="什么是推理？"></a>什么是推理？</h4><p>推理是进行思维模拟的基本形式之一，是从一个或几个已知的判断(前提)<strong>推出新判断</strong>(结论)的过程。</p>
<p>推理是从一般到个别、一般到一般、个别到一般、个别到个别的过程。</p>
<p>一般可将推理分为：</p>
<ol>
<li>演绎(deductive)推理</li>
<li>归纳(inductive)推理</li>
<li>类比(analogy)推理</li>
<li>假设性(presumptive或abduction)推理</li>
<li>因果(causality)推理</li>
<li>综合(synthesis)推理</li>
</ol>
<p>推理起源于人类尝试拥有<strong>从个别、具体事物中抽象概括出一般、普遍道理的思考能力</strong>，如亚里士多德提出和建立的“<strong>演绎三段论</strong>(syllogisms)”。（由此看出，三段论那种是演绎推理）</p>
<h4 id="因果推理"><a href="#因果推理" class="headerlink" title="因果推理"></a>因果推理</h4><p>图灵奖获得者 Pearl 将因果推理分成 3 个<strong>由下而上</strong>的层次:</p>
<ol>
<li>关联(association): 直接可从数据中计算得到的统计相关;</li>
<li>介入(intervention):无法<strong>直接从观测数据就能得到关系</strong>，如“某个商品涨价会产生什么结果”这个问题不仅与新价格有关，而且会与客户购买行为、用户收入等等因素相关;</li>
<li>反事实(counterfactual):某个事情已经发生了，则在相同环境中，这个事情不发生会带来怎样的新结果</li>
</ol>
<h4 id="记忆驱动的推理"><a href="#记忆驱动的推理" class="headerlink" title="记忆驱动的推理"></a>记忆驱动的推理</h4><p>智能行为多依赖于记忆系统，研究发现人类记忆有<strong>感觉记忆</strong>(sensor memory)、<strong>工作记忆</strong>(working memory)和<strong>长期记忆</strong>(long-term memory)[3-5]。为了应对各种认知任务，大脑要<strong>在短时间内保存和处理各种感兴趣信息</strong>，完成这个过程的大脑系统就是“工作记忆”。<strong>工作记忆是形成语言理解、学习与记忆、推理和计划等复杂认知能力的基础</strong>。</p>
<p>在工作记忆区域中，当前输入信息(由感觉记忆加工的当前数据)以及非当前输入信息(从长期记忆中唤醒的历史信息，如已有知识和过往经验)一起发生作用。也就是说，人脑在进行感知和认知时，不仅要对当前数据进行处理，还需要<strong>调动大脑中存储的相关信息</strong>。因此，<strong>注意力与记忆在人的认知理解过程中扮演了重要的角色</strong>，特别是对于文本、语音与视频等序列数据的知识获取与推理过程至关重要。</p>
<p>人脑在理解当前场景和环境时，有效利用了与当前输入数据相关的信息，这些信息存储在外部记忆体(externalmemory)中</p>
<p>记忆驱动推理反映了人脑智能活动，要重点进行如下研究:</p>
<ol>
<li>感知记忆、工作记忆和长期记忆中逻辑、描述、事实型知识的表示方法，从离散符号到分布式向量表达，为深度神经推理打下基础;</li>
<li><strong>自上而下预测反馈与自底向上注意力相互结合方法，刻画短期记忆、工作记忆和长期记忆之间的交互机制，建立可计算推理手段</strong>;</li>
<li><strong>场景理解目标驱动下记忆激活、自更新和自调整机制，实现知识自适应学习与推理</strong>。</li>
</ol>
<p>后面两个应该是我要关注的重点。</p>
<h4 id="什么是逻辑学派"><a href="#什么是逻辑学派" class="headerlink" title="什么是逻辑学派"></a>什么是逻辑学派</h4><p>逻辑学派主张用形式化方法来描述客观世界， 其认为任何推理是基于已有逻辑化知识而展开，如 一阶逻辑和谓词逻辑及定义在其上的推理演算。</p>
<p>逻辑学派在发展推理过程中始终围绕着如何从 已知命题/谓词出发推导出正确性结论这一核心。</p>
<p>逻辑派学者通过命题或一阶谓词来表示客观世 界 中 简 单 概 念</p>
<h4 id="什么是知识工程学派"><a href="#什么是知识工程学派" class="headerlink" title="什么是知识工程学派"></a>什么是知识工程学派</h4><p>知识工程学派<strong>通过语义网络来表示更为丰富概念与知识</strong>，以<strong>刻画实体之间以及实体与属性之间所存在的关联关系</strong>。早期的知识图谱几乎依赖于专家知识而构建，即知识图谱 中的实体、属性与关系完全由专家人工构造，如 WordNet[1]和 CyC[2]等 。</p>
<p>大数据时代，<strong>基于数据驱动的机器推理方法来进行知识图谱构建</strong>逐渐成为国际知识图谱研究的主 要方向</p>
<p>基于符号规则推导或数据驱动计算的知识图谱推理方法各有优劣，<strong>前者解释性强而泛化能力弱</strong>、<strong>后者黑盒操作难以利用已有知识和先验</strong>。因此，需要加强如下内容研究:</p>
<ol>
<li>有机结合规则引导与数据驱动方法、</li>
<li>面向资源匮乏特定领域的知识推理、</li>
<li>人在回路 的知识推理模型。</li>
</ol>
<p>从 前者解释性强而泛化能力弱、后者黑盒操作难以利用已有知识和先验、有机结合规则引导与数据驱动方法、面向资源匮乏特定领域的知识推理 这些 CV 的模型里面也都是这样的，这就是 规则 VS 数据驱动的区别。</p>
<h3 id="5-人工智能的未来-–-神经科学启发的类脑计算综述"><a href="#5-人工智能的未来-–-神经科学启发的类脑计算综述" class="headerlink" title="[5] 人工智能的未来 – 神经科学启发的类脑计算综述"></a>[5] 人工智能的未来 – 神经科学启发的类脑计算综述</h3><p>知乎专栏文章链接：<a href="https://zhuanlan.zhihu.com/p/35416350" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35416350</a></p>
<h4 id="Neurobiology-and-Computational-Neuroscience"><a href="#Neurobiology-and-Computational-Neuroscience" class="headerlink" title="Neurobiology and Computational Neuroscience"></a>Neurobiology and Computational Neuroscience</h4><p>模拟脑需要在<strong>神经生物学</strong>（Neurobiology）和<strong>计算神经学</strong>（Computational Neuroscience）基础上实现。</p>
<ol>
<li><strong>神经生物学</strong>侧重研究神经元和突触等脑组织的生物学机理；</li>
<li><strong>计算神经学</strong>主要通过生物学机理对神经元以及神经突触等结构进行数学建模，并在模拟环境仿真以求其特征与生物脑相近。</li>
</ol>
<p>一个是 认识生物机理；一个是发展计算模型。</p>
<h4 id="人脑强大但低功耗"><a href="#人脑强大但低功耗" class="headerlink" title="人脑强大但低功耗"></a>人脑强大但低功耗</h4><p>人类大脑是一个极度优化的系统，它的工作耗能仅为 <strong>25 瓦特</strong>，神经元的数量却在 10 的 11 次方(<strong>一千亿，100 billion</strong>)的数量级上，并且这其中的突触也达到了每个神经元有 10000 个。这样庞大的网络却有如此低的能耗，这是使得人类大脑在复杂问题的处理有绝对优势。</p>
<h4 id="三代人工神经网络"><a href="#三代人工神经网络" class="headerlink" title="三代人工神经网络"></a>三代人工神经网络</h4><p>早期的<strong>类脑计算（Brian-like Computing）</strong>也可以狭义的称为<strong>神经计算（Neural Computation）</strong>，将神经元和突触模型作为基础，把这些模型用在许多现实中的识别任务，从而发挥模拟人脑功能</p>
<ol>
<li>感知机（perceptron）是第一代神经网络</li>
<li>多层感知机（Multi-layer Perceptron，MLP）是第二代神经网络</li>
<li>脉冲神经网络（Spiking Neural Network，SNN）是第三代神经网络</li>
</ol>
<p>感知机 和 多层感知机仅保留了神经<strong>网络结构</strong>，而极大<strong>简化了</strong>网络中的<strong>神经元模型</strong></p>
<p>实际上生物神经元对信息的处理是以脉冲形式出现的生物电信号</p>
<p>脉冲神经网络（Spiking Neural Network，SNN）由 W.Maass 在 1997 年首次提出，其底层用脉冲函数模仿生物点信号作为神经元之间的信息传递方式</p>
<p>SNN 的优点是具有更多的生物解释性，一方面可以作为计算神经学对生物脑现象模拟的基础工具；另一方面，由于其信息用脉冲传递的特点，SNN 结构更容易在硬件上实现，如 FPGA 等片上系统（on-chip system）。但是，<strong>脉冲函数不可导，因此 SNN 不能直接应用梯度法进行训练</strong>，对 SNN 的学习算法一直是近年来主要的研究问题。</p>
<p>SNN 的神经元模型总体上来说是一类以微分方程构成的模型，带有时间属性。可以理解为传统的神经元只是当前时刻的输入与权重的加权和，SNN 的神经元则是<strong>在一定宽度的时间窗内</strong>的输入与权重的加权和。</p>
<h4 id="类脑计算的研究趋势"><a href="#类脑计算的研究趋势" class="headerlink" title="类脑计算的研究趋势"></a>类脑计算的研究趋势</h4><ol>
<li>首先是基础的生物脑中的神经元，突触及<strong>记忆，注意等机制的建模</strong>；</li>
<li>第二，基于生物机制建模的神经网络学习算法以及在模式识别等机器学习任务中的应用；</li>
<li>最后，基于生物激励的算法和神经网络的硬件系统研究。</li>
</ol>
<h3 id="6-史忠植：人工智能-第十二章-类脑智能"><a href="#6-史忠植：人工智能-第十二章-类脑智能" class="headerlink" title="[6] 史忠植：人工智能 第十二章 类脑智能"></a>[6] 史忠植：人工智能 第十二章 类脑智能</h3><p>课程 PPT 链接 <a href="http://www.intsci.ac.cn/shizz/ai.html" target="_blank" rel="noopener">http://www.intsci.ac.cn/shizz/ai.html</a></p>
<p>“中国脑计划”的名称为“脑科学与类脑科学研究” 主要有两个研究方向：</p>
<ol>
<li>以探索大脑秘密、攻克大脑疾病为导向的脑科学研究(脑认知与脑医学)</li>
<li>以建立 发展人工智能技术为导向的类脑研究(脑认知与类脑计算)</li>
</ol>
<p>多模态脑影像技术是归在脑认知与脑医学下面的</p>
<h3 id="7-李德毅院士：从脑认知到人工智能"><a href="#7-李德毅院士：从脑认知到人工智能" class="headerlink" title="[7] 李德毅院士：从脑认知到人工智能"></a>[7] 李德毅院士：从脑认知到人工智能</h3><p>链接：<a href="http://www.199it.com/archives/397624.html" target="_blank" rel="noopener">http://www.199it.com/archives/397624.html</a></p>
<h4 id="从功能上逼近脑"><a href="#从功能上逼近脑" class="headerlink" title="从功能上逼近脑"></a>从功能上逼近脑</h4><p>如果每一个模块解决一个特定问题，然后千千万万个这样的模块集成起来，是否就能从功能上逼近人脑？</p>
<h4 id="记忆认知"><a href="#记忆认知" class="headerlink" title="记忆认知"></a>记忆认知</h4><p>脑认知的核心是记忆，不是计算。人类的记忆力强，记忆量大，就是所谓的聪明。（那看论文多，也就是学术水平好，是不是这个道理）</p>
<p>记忆不是简单的存储，有一定的取舍，记忆是计算、简约、抽象</p>
<p>记忆可以分为三大块：瞬间记忆、工作记忆和长期记忆</p>
<p>记忆不是简单的储存，其伴随一定的取舍，而<strong>取舍就是通过计算进行简化和抽象的过程</strong>，记忆和计算总是同时发生的。通常，时间越长所丢失的信息就越多。记忆常常也存在联想和搜索，而模糊信息的联想与搜索恰恰也都是计算。所以无论语言记忆还是图像记忆，他们本质上都是统计记忆，<strong>越是长期的、大量的和反复的，就越难以遗忘。这里就类似于模型的训练，大量数据的训练的模型可以获得一个稳定、鲁棒的系统</strong>。少量数据训练的模型总是存在偏差，系统随着新数据的加入而变得不够稳定</p>
<h4 id="重新认识卷积（利用卷积表达认知和记忆）"><a href="#重新认识卷积（利用卷积表达认知和记忆）" class="headerlink" title="重新认识卷积（利用卷积表达认知和记忆）"></a>重新认识卷积（利用卷积表达认知和记忆）</h4><p>在卷积神经网络里，可以把卷积想象成一种混合信息的手段。想象一下装满信息的两个桶（一个桶是信号，另一个桶是卷积核），我们把它们倒入一个桶中并且通过某种规则搅拌搅拌。也就是说<strong>卷积是一种混合两种信息的流程</strong>。</p>
<p>李院士认为卷积之所以这么重要，不仅在于其能抽取图像特征，更重要的是<strong>卷积能度量记忆</strong>。记忆的可度量性才是对科研和工程最重要的，其表达形式正是已有认知和遗忘的卷积。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Measure%20of%20Memory.jpg" alt=""></p>
<p>如上方程式表达，对于整体认知，遗忘和认知的乘积代表着某一时刻将遗忘这一效果加载到认知中。随时间的流逝，遗忘效果不停地加载到认知上，即每一个时间步认知和遗忘都会乘积一次。并且认知函数 f(x) 对整个记忆 h(t) 的贡献应该是随时间增大而减少的，这一点正好体现在卷积的特性中，即 g(t-τ) 函数中。因此 f(x)×g(t-τ) 的积分便是记忆的累积量。</p>
<p>下面这一组方程将感觉记忆、工作记忆和长期记忆进行了形式化表达</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Memory%20Fomula.jpg" alt=""></p>
<p>记忆包括识记、保持、再认和重现四个过程。也就是最开始由感觉记忆函数学习，但这个时候遗忘的速率是比较快的，随后工作记忆函数采用更慢一点的遗忘速率保持记忆，并在几次循环后达到长期记忆。</p>
<p>蒲慕明院士的报告，实验证明反向传播模式可能真的存在于生物机制中</p>
<p>记忆很重要，是因为在记忆使用一些机制后，其就可成为 Zero-shot 学习，或者是 One-shot 学习，这两种学习对没见过的数据也能处理。</p>
<h4 id="交互认知"><a href="#交互认知" class="headerlink" title="交互认知"></a>交互认知</h4><p>脑认知的<strong>一个重要特点</strong>是脑<strong>不同区域不同粒度的认知可以往返跳跃、并行处理</strong>。大脑皮质中形成的知识积累(<strong>粗粒度</strong>)和海马体中当前学习和思维的问题(<strong>中粒度</strong>),以及视觉神经中残留的感觉和观察(<strong>细粒度</strong>)可同时发生交互与关联,反映在<strong>不同尺度空间的灵活转换</strong>。</p>
<p>脑认知的<strong>另一个重要特点</strong>是脑通过感知和外部世界交互,通过视觉、听觉、触觉、嗅觉、味觉五感, <strong>单模态或者多模态地交互</strong>,在交互过程中和其他自然人、机器人、<strong>外部世界互动</strong>,尤其是理解自然人的表情、心理、意愿、动机等,相互启发、学习, 交互的结果使得认知更准确,自己更聪明。如果没有这种交互,自身难以获得这样的认知。(感觉是在暗示 多模态学习 和 强化学习)</p>
<p>再讲一个知识点，<strong>脑的自定位和自导航</strong>，我们把它叫做 <strong>iSLAM</strong>。<strong>人不断的把外部世界放在自己的坐标系做影射</strong>，这个很重要，人脑里面甚至有一定的盲导航功能。当然小脑发达程度不一样，个人能往前走的程度也不一样。<strong>脑认知的坐标系按照现在物理学定律应该是对数极坐标系</strong>。所以我们的视力表，第一个 E 字那么大，最后一个那么小，它不是线性关系，而是对数关系。</p>
<h4 id="卷积神经网络的三大局限"><a href="#卷积神经网络的三大局限" class="headerlink" title="卷积神经网络的三大局限"></a>卷积神经网络的三大局限</h4><ol>
<li>第一个毛病，到底多少是深层次学习。多少个卷积核，每个卷积多大，怎么样来进行特征提取，都有太多的随意性和适凑性，而且不能保证拓扑结构参数的收敛，这是一个最要命的问题。</li>
<li>第二个毛病，由细尺度特征到大尺度特征的层层提取，只有前馈没有反馈，<strong>已有的认知不能帮助当前的视觉感知和认知，没有体现选择性</strong>（Attention 弥补了这种选择性了么？）。</li>
<li>第三个毛病要求海量训练样本，一万个样本做了半天，最后能够识别一百个东西。一万个样本你让我指定，最后识别了一百个，不划算。尤其从样本的均等性，<strong>没有反映认知的累计性</strong>，所以我觉得在座的这么多听众，如果你们觉得我的报告值钱的话，这张片子最值钱。（要有举一反三的能力，这个举一反三 不是说 训练举一 - 测试反三，而是在训练内部就举一反三 ）</li>
</ol>
<h4 id="脑认知单元-vs-图灵模型"><a href="#脑认知单元-vs-图灵模型" class="headerlink" title="脑认知单元 vs 图灵模型"></a>脑认知单元 vs 图灵模型</h4><p>图灵模型和冯诺依曼计算机，充其量只具有计算机智能。计算机的架构中，<strong>计算、存储和交互相互分离</strong>，导致内容不同区域的数据频繁访问，以及硬盘和内存间数据的频繁访问。</p>
<p>而脑认知的构成单元，<strong>应该同时具有记忆智能、计算智能和交互智能</strong>，大大降低能耗。</p>
<h4 id="脑认知形式化的尺度选择"><a href="#脑认知形式化的尺度选择" class="headerlink" title="脑认知形式化的尺度选择"></a>脑认知形式化的尺度选择</h4><p>脑认知的形式化，最关键的是要懂得<strong>忽略</strong>和<strong>聚焦</strong>，懂得<strong>抽象</strong>和<strong>分离</strong>。</p>
<h3 id="8-郑南宁-人工智能的下一步是什么"><a href="#8-郑南宁-人工智能的下一步是什么" class="headerlink" title="[8] 郑南宁:人工智能的下一步是什么?"></a>[8] 郑南宁:人工智能的下一步是什么?</h3><h4 id="非完整信息处理问题"><a href="#非完整信息处理问题" class="headerlink" title="非完整信息处理问题"></a>非完整信息处理问题</h4><p>当前人工智能的研究前沿之一是如何实现由完整信息到非完整信息的处理，构建更加健壮的人工智能，使人工智能系统对用户错误、目标偏差、错误模型以及未建模对象具有更 好的适应性。无人驾驶就是一种典型的非 完整信息处理问题。</p>
<p>由于我们不可能为所有的问题建模， “未知的未知”问题对构建稳健的人工智 能系统提出了挑战。为了设计更加健壮的 人工智能，需要采用稳健优化、<strong>学习因果模型</strong>和组合模型等方法来提高人工智能建 模问题的稳健性。(因果是克服 未知的未知 的一种手段，这可能就是人类为什么只要小样本就可以了的原因)</p>
<p>人的认知过程，在很多场合下是<strong>从全局到局部</strong>的，<strong>在大量先进知识的前提下</strong>，往往是一种<strong>自上而下的过程</strong>。</p>
<h3 id="9-脑科学与类脑研究概述"><a href="#9-脑科学与类脑研究概述" class="headerlink" title="[9] 脑科学与类脑研究概述"></a>[9] 脑科学与类脑研究概述</h3><p>Marr 不但是计算机视觉的开拓者，还奠定了神经元群之间存储、处理、传递信息的计算基础，特别是对学习与记忆、视觉相关环路的神经计算建模作出了重要贡献（我的 Marr 的工作一无所知）</p>
<ol>
<li>人工智能符号主义研究的出发点是<strong>对人类思维、行为的符号化</strong>高层抽象描述，20 世纪 70 年代兴起的专家系统是该类方法的代表</li>
<li>以人工神经网络为代表的<strong>联接主义</strong>的出发点正是<strong>对脑神经系统结构</strong>及其计算机制的<strong>初步模拟</strong>。</li>
</ol>
<p>理解大脑的结构与功能, 理解认知、思维、意识和语言的<strong>神经基础</strong>（这是 Science 的人干的）</p>
<h4 id="Minsky-干了神经网络两次"><a href="#Minsky-干了神经网络两次" class="headerlink" title="Minsky 干了神经网络两次"></a>Minsky 干了神经网络两次</h4><p>Minsky 干了神经网络两次，第一次是指出单层感知器无法表示异或函数，后来被用 BP 优化的多层感知器克服（MLP 早有，但起初没法训练，BP 解决了这个问题，才算克服这个问题）；第二次是 指出当时计算能力的提升不足以支持大规模神经网络训练，这个被深度学习的诞生和 GPU 的发展克服。</p>
<h4 id="图灵机计算的本质"><a href="#图灵机计算的本质" class="headerlink" title="图灵机计算的本质"></a>图灵机计算的本质</h4><p>图灵机计算的本质是<strong>需要人们对现实世界进行形式化的定义，模型能力取决于人对物理世界的认知程度</strong>， 因此人限定了机器描述问题、解决问题的程度。这使得 目前的智能系统在感知、认知、控制等多方面都存在巨 大瓶颈。例如还难以实现海量多模态信息的选择性感知 与注意、模式识别与语言理解在处理机制与效率等方面 与人脑相比还存在明显不足，需要针对某个专用问题非 常依赖人工输入知识或提供大规模标记训练样本</p>
<p>因此，人工智能要满足现实需求还缺乏足够的<strong>适应性</strong>，没法处理新问题，因为新问题没有事先在这个系统里被形式化定义好，这个的根源在于图灵机的本质（难道不是 数学 的本质？）</p>
<p>语音识别、 图像处理、自然语言处理、机器翻译等采用不同的模型 和不同的学习数据，两种不同的任务无法采用同一套系统进行求解，不同任务之间知识也无法共享。而人脑却 采用同一个信息处理系统进行自动感知、问题分析与求 解、决策控制等。这说的是 目前的人工智能技术缺乏通用性，而人脑是一个通用系统，知识可以共享。</p>
<h4 id="脑科学与类脑研究"><a href="#脑科学与类脑研究" class="headerlink" title="脑科学与类脑研究"></a>脑科学与类脑研究</h4><p>脑科学与类脑研究是两个目标完全不同的领域:脑科学的目标是要<strong>理解大脑</strong>的结构和功能、演化 来源和发育过程，以及神经信息处理的<strong>机制</strong>。类脑研究的目标是<strong>研发出新一代的智能技术</strong>，推动信息产业的发 展。</p>
<h3 id="10-人工智能中的联结主义和符号主义"><a href="#10-人工智能中的联结主义和符号主义" class="headerlink" title="[10] 人工智能中的联结主义和符号主义"></a>[10] 人工智能中的联结主义和符号主义</h3><p>人类的<strong>智能</strong>主要包括<strong>归纳总结</strong>和和<strong>符号逻辑演绎</strong>（这两个都算推理），对应着人工智能中的<strong>联结主主义</strong>和<strong>符号主义</strong>。（目前的机器学习里面没有 符号逻辑演绎的内容，这也就是 朱松纯 在 正本清源的文章里面提到的）</p>
<h4 id="符号主义"><a href="#符号主义" class="headerlink" title="符号主义"></a>符号主义</h4><p>符号主义的主要思想就是<strong>应用逻辑推理法则从公理出发推演整个理论体系</strong></p>
<p>人工智能中，符号主义的一个代表 就是机器定理证明</p>
<p>联结主义的缺陷：没有坚实的理论基础。通过仿生学和经验积累得到的突破，依然无法透 彻理解和预测</p>
<h4 id="海马体与梦"><a href="#海马体与梦" class="headerlink" title="海马体与梦"></a>海马体与梦</h4><p>大脑中有一对<strong>海马体(Hipocampus)</strong>，它们和人类的长期记忆有关。<strong>如果把大脑比喻成一个数据库， 那么海马体就像是索引</strong>。如果海马体有问题，那么许多 存入的记忆无法被 取出，同时也无法形成新的记忆。<strong>每天晚上，海马体将当天形成的短暂记忆加工成长期记忆，在这一过程中，就形成了梦</strong>。海马体和其他神经中枢相连，处理其他中枢已经处理好的数据，形成新的编码。<strong>海马体和视觉与听觉中枢直接相连</strong>，因此在梦中能够看到 并且听到;<strong>但海马体和嗅觉中枢不相连</strong>，因此在梦中无法闻到气味</p>
<h4 id="高级中枢向低级中枢反馈"><a href="#高级中枢向低级中枢反馈" class="headerlink" title="高级中枢向低级中枢反馈"></a>高级中枢向低级中枢反馈</h4><p>实际上，视觉处理的过程并不只是 从低级向高级传递的单向过程，高级中 枢可以向低级中枢发出反馈信息，最明 显的例子是<strong>高级中枢可以决定低级中枢的“注意力”和“焦点”</strong>。当看到模糊不清或一时无法辨认的图像时，高级中枢会产生各种概率上合理的解释，并且由这种猜测先入为主地影响低层中枢的判断，从而<strong>产生错觉</strong></p>
<h4 id="层次特征"><a href="#层次特征" class="headerlink" title="层次特征"></a>层次特征</h4><p>从视网膜到第一级视觉中枢的大脑皮层曲面的映射是保角映射，保角变换的最大特点是局部保持形状，但是忽略面积大小，这说明视觉处理对于局部形状非常敏感。</p>
<p>视觉高级中枢忽略色彩、纹理、光照等局部细节，侧重<strong>整体模式匹配</strong>和<strong>上下文关系</strong>，并可以<strong>主动补充大量缺失信息</strong>（这个主动补充就跟后面的错觉有关系，也就是高级中枢向低级中枢反馈）</p>
<h3 id="11-视觉选择性注意的模型化计算及其应用前景"><a href="#11-视觉选择性注意的模型化计算及其应用前景" class="headerlink" title="[11] 视觉选择性注意的模型化计算及其应用前景"></a>[11] 视觉选择性注意的模型化计算及其应用前景</h3><h4 id="什么是注意？"><a href="#什么是注意？" class="headerlink" title="什么是注意？"></a>什么是注意？</h4><p>三 大认知功能：记忆、注意、意识</p>
<p>“注意”<strong>在记忆与意识的引导下</strong>决定人类视觉感知的主动特性，其中最为核心的视觉选择性注意(Visual Selective Attention，VSA)机制，<strong>使人具备从复杂环境中搜索感 兴趣目标的能力</strong>。同时，这种主动行为特性也是<strong>当前机器视觉区别于人类视觉的沟壑所在</strong></p>
<p>人眼具有 VSA 特性主要<strong>源于大脑中可用资源的限制</strong>（现有的计算资源也是有限的，尤其是无人自主系统，比如 UAV，所以 VSA 也是有限资源下的必然选择，这是研究 VSA 的现实动机），尤其是视觉系统接受的信息量与大脑中神经细胞的数目相差无几。 其次，外界环境的<strong>所有信息对于观察者来说并不是同等重要</strong>（这是研究 VSA 符合实际情况的动机），因此大脑只需要对部分重要信息做出响应。</p>
<p>人类视觉系统能瞬息感知外部世界，其主要原因是<strong>人脑中关于物体知识的记忆以及意识的引导与环境刺激驱动相结合所引发的视觉选择性注意</strong>起着重要的指向与汇聚作用，极大地 提高了视觉感知的有效性（关于物体知识的记忆、意识引导，记忆还可以理解，究竟什么是意识？Top-Down 的 Task Driven?）</p>
<p>人类完美的视觉系统是智能化视觉信息处理系统的典范（这就是最简单的为什么要研究类脑计算的动机，这是联结主义的动机）</p>
<p>人类的主动视觉 (Active Vision，AV)行为是在大脑意识驱动下、记忆与注意共 同引导的视觉感知方式。 其中 VSA 作为 AV 的核心，最重要的功能在于<strong>协调外界信息量与人脑资源的不平衡</strong>，实现大脑资源的合理分配。（在大数据时代特别重要）</p>
<p>VSA <strong>强调有意识有目的</strong>的行为</p>
<h4 id="Marr-视觉理论"><a href="#Marr-视觉理论" class="headerlink" title="Marr 视觉理论"></a>Marr 视觉理论</h4><p>Marr 视觉理论的<strong>核心问题</strong>是设法从图像结构<strong>推导出外部世界的三维结构</strong>，视觉从图像开始，经过一系列的处理和转换，最后达到<strong>对外部现实世界的认识</strong>（对外部世界的认识由三维结构来表示？）。</p>
<p>视觉计算框架中的信息流通路是自下而上单一方向的，对高层知识的引导和反馈缺乏足够的重视，与人类视觉系统有目的的、主动的认知过程相距甚远。</p>
<p>知识引导下的<strong>主动视觉感知</strong>成为 Marr 理论较为完美的补充</p>
<p>主动选择行为的引入，使得传统的 Marr 视觉框架中信息表示的计算约束变得易于解决（Visual Selective Attention + 3D 重建）</p>
<h4 id="视觉信息处理通路"><a href="#视觉信息处理通路" class="headerlink" title="视觉信息处理通路"></a>视觉信息处理通路</h4><p>目前生理学普遍认为视觉信息处理通路分为三个阶段:最早的处理阶段包括 <strong>视网膜</strong>、<strong>侧膝体</strong>(Lateral Geniculate Nucleus，<strong>LGN</strong>)、<strong>主视皮层区</strong>(Primary Visual Cortex)，在灵长目动物中也称为 V1 区，都是通过提取简单的<strong>局部特征</strong>如中心-外周感受野以及具有朝向的线与边缘进行编码来表达图像。这种编码来源于去相关与冗余的计算准则或者采用稀疏编码对输入图像可靠的重建。早期处理过后，有效且适度的复杂特征在 V4 以及与其相邻的 TEO 区表达，最后，部分或完整的感兴趣物体视图在 IT 皮层的前区表达[23]。 </p>
<h4 id="VSA-模型化计算"><a href="#VSA-模型化计算" class="headerlink" title="VSA 模型化计算"></a>VSA 模型化计算</h4><p>VSA 的模型化计算的<strong>核心</strong>还是围绕<strong>如何构建由刺激引发的自底向上的数据驱动与任务导引下的自顶向下的目标驱动的视觉注意的数学模型</strong>所展开，以及由此而引发的信<br>息流通路的分叉与汇合问题的研究</p>
<p>Koch 和 Ullman 于 1985 年提出的视觉选择注意模型开 创了 VSA 模型化计算的新纪元。 <strong>显著图</strong>(Saliency Map)的概念也是由他们在那时提出。 Koch-Ullman 模型第一次提出了 <strong>先计算后选择的观点</strong>，并提出利用“赢者全取”(Winner Take All，WTA)与抑制返回(Inhibit of Return，IOR)机制，实现凝视 点的选取与转移，且转移必须遵循两条规则，一是距离优先， 二是特征相似优先。</p>
<p>将其提高到数理水平的定量计算主导了其后 VSA 模型化计算的研究方向。 Itti 和 Koch[28]于 1998 年在 IEEE Transactions on PAMI 上发表了一篇关于视觉场景快速分析的文章，其中最主要的贡献就是将 <strong>VSA 机制从本质上推向了模型化定量计算的新阶段</strong>，使 VSA 在从神经生 理学的机制理论研究到信息科学领域的可计算性分析，进而 走向定量计算的工程应用的发展过程中发挥了重要的里程 碑作用。</p>
<p>模型<strong>首次运用数学工具对已有生理机制的仿生建模</strong>，沟通了神经生理学、心理学与计算机科学、数理科学的桥梁</p>
<h5 id="VSA-模型化计算的关键环节"><a href="#VSA-模型化计算的关键环节" class="headerlink" title="VSA 模型化计算的关键环节"></a>VSA 模型化计算的关键环节</h5><ol>
<li>如何产生中央区自下而上兴趣图;</li>
<li>视觉任务和物体知识的表达;</li>
<li>自上而下与自下而上双向信息流的汇合</li>
<li>显性注意中凝视点的转移控制</li>
</ol>
<h5 id="自下而上兴趣图"><a href="#自下而上兴趣图" class="headerlink" title="自下而上兴趣图"></a>自下而上兴趣图</h5><p>中央区自下而上兴趣图的产生在 VSA 的研究中，其核心问题在于<strong>如何选择一种合适的显著性度量方法，使得选择的区域能够在其他区域中突显(pop-out)</strong>[31]，即找到输入图像中的感兴趣区域。</p>
<p>目前针对自下而上兴趣图的产生主要<strong>以目标先验信息是否存在</strong>为算法研究的依据。</p>
<p>当存在先验目标信息时，由于人脑执行<strong>基于内容语义的高级视觉注意</strong>，与视觉任务、物体和环境的知识有关，并且与模式识别和匹配密不可分</p>
<p>当目标先验信息不存在的时候，就像把人置于完全陌生的环境中，脑认知功能通过对于“pop-out”研究的实验结果认为<strong>此时刺激特征主要表现在当刺激物之间由低对比度转向高对比度时能引起感受野细胞的空间重组织</strong>，从而引起观察者的注 意[40-41]。 因此，<strong>采用对比度衡量显著性</strong>将是自底向上模型合理仿生的途径。</p>
<h5 id="物体知识的表达"><a href="#物体知识的表达" class="headerlink" title="物体知识的表达"></a>物体知识的表达</h5><p>语义网络模型的引入可以用来描述物体之间的关系</p>
<p>语义内容已经被证实是最佳搜索路径的预测线索，因此，建立大规模的真实世界语义模 型对选择性注意机制建模具有重要的意义</p>
<p>运用语义网络模型表达的调节控制注意选 择区域</p>
<p>对于双向信息流的汇合，常见的方法主要是通过得到目标先验信息， 再加以对底层特征的调制，即信息流在显著图汇合(激活)</p>
<p>至于两者之间，究竟谁执导谁的问题阐述， 较为有名的是 Wolfe 在特征整合理论的基础上提出的<strong>导向搜索理论</strong>(Guided Search)[34]。该理论假设存在两个加工阶段，首先是<strong>特征加工</strong>阶段，类似于特征整合理论的第一阶段; 在第二阶段上，Wolfe 认为在<strong>特征识别</strong>之后，<strong>来自自上而下和自下而上的信息可以联合起来对注意进行引导</strong>，在这个过程中那些和被试期望相匹配的奇异刺激最有可能获得注意，两种信息的结合可以使 对复合刺激的搜索快速完成。</p>
<p><strong>自上而下的线索对底层视觉特征的调制是精细粒度</strong>，即目标与分心物在单一的特征维内所在的区间也能影响搜索的速度</p>
<p>对底层特征各特 征维部署不同的特征权重得到的显著区域也会大相径庭。 同 时也有文献中的方法通过最优化策略得到不同特征维的自下而上显著图的相关权值，并<strong>通过学习得到的统计信息优化自底向上信息流中各底层特征信息的权重，提升目标显著性，抑制背景显著度</strong></p>
<blockquote>
<p>针对目前生 理学的研究成果，权值调制是一种可行的途径[57]。但从权值调制 中引发的权值设置也是信息科学领域中机器学习研究的热点 问题，目前已有基于 Bayes 网络的统计学方法[58]，基于场景内容 的方法[38]等用来解决 VSA 模型化计算中的该类问题</p>
</blockquote>
<p>Attention 很重要，但之前 Attention 权值都要设置；而现在已经是学出来的了，这就是进步以及 Attention Network 伟大的地方。</p>
<h5 id="在目标检测中的应用"><a href="#在目标检测中的应用" class="headerlink" title="在目标检测中的应用"></a>在目标检测中的应用</h5><p>VSA 通过引导高效可靠的视觉感知成为众多灵长目类 动物一项重要的心理调节机制，以过滤筛选的方式提取视觉 感知中的有效信息即目标相关信息，从而合理分配大脑视皮 层中的信息加工资源。因此，其应用的直观体现即是目标检测</p>
<h4 id="值得研究的问题"><a href="#值得研究的问题" class="headerlink" title="值得研究的问题"></a>值得研究的问题</h4><ol>
<li>如何合理地融入记忆与意识对自底向上视觉搜索的影响（这点是我感兴趣的）</li>
<li>如何找到衡量模型性能的标准</li>
<li>如何将显性 的眼动注意予以考虑，并将其与隐性注意相融合，找到它们 之间的切入点</li>
</ol>
<h3 id="12-刘成林-从模式识别到类脑研究"><a href="#12-刘成林-从模式识别到类脑研究" class="headerlink" title="[12] 刘成林:从模式识别到类脑研究"></a>[12] 刘成林:从模式识别到类脑研究</h3><p>“人工智能”(artificial intelligence)概念最早由 John McCarthy 等在1956年的达特矛斯会议(Dartmouth Conference)上提出:<strong>人工智能就是通过计算机编程使机器实现类人智能行为</strong></p>
<h4 id="模式识别"><a href="#模式识别" class="headerlink" title="模式识别"></a>模式识别</h4><p>模式识别有 2 个层面的含义:</p>
<ol>
<li>一是生物体 (主要是人脑)<strong>感知环境</strong>的模式识别能力与机理，属于心理学和认知科学范畴;</li>
<li>二是面向智能模拟和应用，<strong>研究计算机实现模式识别的理论和方法</strong>，属于信息科学和计算机科学领域 的范畴。</li>
</ol>
<p><strong>模式识别基础理论</strong>(模式表示与分类、机器学习等)、<strong>视觉信息处理</strong>(图像处理和计算机视觉)、<strong>语音语言信息处理</strong>(语音识别、自然语言处理、机器翻译等)是模式识别领域的三大主要研究方向。刘成林解释，模式识别是人工智能的一个分支领域。人工智能是通过计算使机器模拟人的智能行为，主要包括感知、思维(推理、决策)、动作、学习，而<strong>模式识别主要研究的就是感知行为</strong>。在人的 5 大感知行为(视 觉、听觉、嗅觉、味觉、触觉)中，视觉、听觉和触觉是人工智能领域研究较多的方向。模式识别领域主要研究的是视觉和 听觉，而触觉主要是跟机器人结合。</p>
<p>以深度学习为代表的主流方法有 3 个明显的不足:</p>
<ol>
<li>一是<strong>需要大量的标记样本进行监督学习</strong>，这势必增加模式识别系统开发中的人工成本;</li>
<li>二是模式识别系统的<strong>自适应能力差</strong>，不像人的知识和识别能力是<strong>随着环境不断进化的</strong>;</li>
<li>三是模式识别一般只进行分类，没有<strong>对模式对象的结构解释</strong>。</li>
</ol>
<h4 id="类脑智能"><a href="#类脑智能" class="headerlink" title="类脑智能"></a>类脑智能</h4><p>类脑智能就是以计算建模为手段，受脑神经机理和认知行为机理启发，并通过软硬件协同实现的机器智能。</p>
<p>类脑智能研究的软件方向：</p>
<ol>
<li>一是使智能计算模型在结构上更加类脑（联结主义）</li>
<li>另外一方面是在认知和学习行为上更加 类人（行为主义）</li>
</ol>
<h3 id="13-深度学习-多层神经网络的复兴与变革"><a href="#13-深度学习-多层神经网络的复兴与变革" class="headerlink" title="[13] 深度学习:多层神经网络的复兴与变革"></a>[13] 深度学习:多层神经网络的复兴与变革</h3><h4 id="深度学习成功的启示"><a href="#深度学习成功的启示" class="headerlink" title="深度学习成功的启示"></a>深度学习成功的启示</h4><h5 id="优化方法的变革是开启深度学习复兴之门的钥匙"><a href="#优化方法的变革是开启深度学习复兴之门的钥匙" class="headerlink" title="优化方法的变革是开启深度学习复兴之门的钥匙"></a>优化方法的变革是开启深度学习复兴之门的钥匙</h5><p>Hinton 等 2006 年的主要 贡献是开创了无监督的、分层预训练多 层神经网络的先河</p>
<p>但实 际上最近 3 年来 DCNN 的繁荣与无监 督、分层预训练并无多大关系，而更多 的与优化方法或者有利于优化的模块 有关，如 Mini-Batch SGD、ReLU 激活函数、Batch Normalization 等，特别是其中 处理梯度消失问题的手段</p>
<h5 id="从经验驱动的人造特征范式到数据驱动的表示学习范式"><a href="#从经验驱动的人造特征范式到数据驱动的表示学习范式" class="headerlink" title="从经验驱动的人造特征范式到数据驱动的表示学习范式"></a>从经验驱动的人造特征范式到数据驱动的表示学习范式</h5><p>在信息表示和特征方面， 过去大量依赖人工的设计，严重影响了 智能处理技术的有效性和通用性。深 度学习彻底颠覆了这种“人造特征”的 范式，开启了数据驱动的表示学习范 式。具体体现在: 1)<strong>所谓的经验和知识也在数据中</strong>，在数据量足够大时无需显式的经验或知识的嵌入，直接从数据 中可以学到;2)可以直接从原始信号开始学习表示，而无需人为转换到别的空间再进行学习。</p>
<h5 id="从“分步分治”到“端到端的学习”"><a href="#从“分步分治”到“端到端的学习”" class="headerlink" title="从“分步分治”到“端到端的学习”"></a>从“分步分治”到“端到端的学习”</h5><p>分治或分步法，即将复杂问题分解 为若干简单子问题或子步骤，曾经是解 决复杂问题的常用思路</p>
<p>但从深度学习的视角来 看，其劣势也同样明显:子问题最优未 必意味着全局的最优，每个子步骤是最 优的也不意味着全过程是最优的</p>
<p>相 反，深度学习更强调端到端的学习 (end-to-end learning)，即:不去人为的 分步骤或者划分子问题，而是完全交给 神经网络直接学习从原始输入到期望 输出的映射。相比分治策略，端到端的 学习具有协同增效(synergy)的优势，有 更大的可能获得全局上更优的解。当 然，如果一定要把分层看成是“子步骤 或子问题”也是可以的，但它们各自完成什么功能并不是预先设定好的，而是 通过基于数据的全局优化来自动学 习的</p>
<h5 id="脑神经科学启发的思路值得更多的重视"><a href="#脑神经科学启发的思路值得更多的重视" class="headerlink" title="脑神经科学启发的思路值得更多的重视"></a>脑神经科学启发的思路值得更多的重视</h5><p>Fukushima 在 1980 年底提出的认知机 模型，而该模型的提出动机就是模拟感 受野逐渐变大、逐层提取由简及繁的特 征、语义逐级抽象的视觉神经通路</p>
<p>生物神经系统的连接极 为复杂不仅仅有自下而上的前馈和<strong>同层递归</strong>更有大量的自上而下的反馈以及来自其他神经子系统的外部连接这些都是目前的深度模型尚未建模的</p>
<h4 id="如何赋予机器演绎推理能力"><a href="#如何赋予机器演绎推理能力" class="headerlink" title="如何赋予机器演绎推理能力"></a>如何赋予机器演绎推理能力</h4><p>基于大数据的深度学习可以认为 是<strong>归纳法</strong>，而<strong>从一般原理出发进行演绎是人类的另一重要能力</strong>，特别是在<strong>认知和决策</strong>过程中，我们大量依赖演绎推理。演绎推理在很多时候似乎<strong>与数据无关</strong>。（这是人能够实现小样本、弱监督学习的关键）</p>
<h3 id="14-从脑网络到人工智能-——-类脑计算的机遇与挑战"><a href="#14-从脑网络到人工智能-——-类脑计算的机遇与挑战" class="headerlink" title="[14] 从脑网络到人工智能 —— 类脑计算的机遇与挑战"></a>[14] 从脑网络到人工智能 —— 类脑计算的机遇与挑战</h3><p>如何实现小样本的学习和有效推广?</p>
<p>目前取得巨大成功的深度学习依赖于庞大的样本数量，这与大脑卓越的<strong>“举一反三”，即小样本学习的能力</strong>形成鲜明对比[11]。原理上看，这意味着生物脑的学习过程并非从零开始，而是从学习之初，就拥有并运用了重要的先验知识，这包含了物种在进化过程中学到的(生物学称之为系统发生)，以及个体在生活过程中学到的有关真实世界的关键知识[12]。读取这些知识，以及借鉴如何<strong>将这些知识作为先验信息注入神经网络结构从而实现小样本学习</strong>，可能会是神经科学以及类 脑算法设计中一个富于成果的领域。(我觉得不仅仅有先验知识的积累，还有推理能力，那么问题来了，推理能力强可以是因为先验知识多么，什么都见过，自然一下就能知道事情发生的后果)</p>
<h3 id="15-人工智能在军事领域的渗透与应用思考"><a href="#15-人工智能在军事领域的渗透与应用思考" class="headerlink" title="[15] 人工智能在军事领域的渗透与应用思考"></a>[15] 人工智能在军事领域的渗透与应用思考</h3><h4 id="人工智能的三个层次"><a href="#人工智能的三个层次" class="headerlink" title="人工智能的三个层次"></a>人工智能的三个层次</h4><ol>
<li><strong>运算智能</strong>即快速计算和记忆存储能力。旨在协助存储和快速处理海量数据，是感知和认知的基础，以科学运算、逻辑处理、统计查询等形式化、规则化运算为核心。在此方面，计算机早已超过人类，但如集合证明、数学符号证明一类的复杂逻辑推理，仍需要人类直觉的辅助。</li>
<li><strong>感知智能</strong>即视觉、听觉、触觉等感知能力。旨在让机器“看”懂与“听”懂，并据此辅助人类高效地完成“看”与“听”的相关工作，以图像理解、语音识别、语言翻译为代表。由于深度学习方法的突破和重大进展，感知智能开始逐步趋于实用水平，目前已接近人类。</li>
<li><strong>认知智能</strong>即“能理解、会思考”。旨在让机器学会主动思考及行动，以实现全面辅助或替代人类工作，以理解、推理和决策为代表，强调会思考、能决策等。因其综合性更强，更接近人类智能，认知智能研究难度更大，长期以来进展一直比较缓慢。</li>
</ol>
<h3 id="16-感知智能到认知智能中对知识的思考"><a href="#16-感知智能到认知智能中对知识的思考" class="headerlink" title="[16] 感知智能到认知智能中对知识的思考"></a>[16] 感知智能到认知智能中对知识的思考</h3><p><a href="http://www.52nlp.cn/%E8%AE%A4%E7%9F%A5%E6%99%BA%E8%83%BD%E5%88%B0%E6%84%9F%E7%9F%A5%E6%99%BA%E8%83%BD%E4%B8%AD%E5%AF%B9%E7%9F%A5%E8%AF%86%E7%9A%84%E6%80%9D%E8%80%83" target="_blank" rel="noopener">网页链接</a></p>
<p>认知心理学将人脑认知世界的过程可以总结为：<strong>感知到认知，从认知到理解</strong></p>
<p><img src="http://www.52nlp.cn/wp-content/uploads/2018/12/%E5%9B%BE%E7%89%87-1-11.png" alt=""></p>
<p><strong>认知是大脑把感知得到的信息与已有信息产生联系，得到信息结果是什么的这一过程</strong>。为什么是联系，因为人脑对某一个状态的判断并不是独立的，比如看见一辆汽车，你是怎么判断它是汽车，定是你把这辆汽车与头脑中已有的汽车信息进行关联，你才知道。如果你从没有见过汽车，你也不知道这个东西是什么。</p>
<p><strong>理解就是你对认知得到的信息有了一些相对固化认知</strong>，比如见到很多辆车的样子，就车的形状有了一个固化的认知，你理解了什么是真正的车。</p>
<p>感知得到信息 -&gt; 认知把感知得到的信息与已有信息联系 -&gt; 理解就是对这类信息形成固化认识。</p>
<blockquote>
<p>知识是人为理解和经验充实的信息，是被证明在一段时间范围内正确的信息。因当前的人工智能算法通常只能针对特定数据集执行特定任务，一旦任务条件超出了限制，人工智能就会瞬间沦为 “人工智障”。所以对于人工智能来说，掌握人类知识和常识是下一阶段的重要目标。而如何获得人类所具有的知识和常识？大规模知识工程构建的知识图谱就成为了人工智能的一个重要要点。</p>
<p><strong>智慧是在大量知识积累基础上，并对知识有了深入的理解，能举一反三产生新的洞察。</strong>因其具有了洞察力、迁移性和创造性，就克服了当前弱人工智能仅能针对单项任务的缺陷，从而实现了强人工智能。（人之所以能够小样本学习，是因为人是有智慧的，不仅有知识还有推理能力）</p>
</blockquote>
<h3 id="17-机器学习-现在与未来"><a href="#17-机器学习-现在与未来" class="headerlink" title="[17] 机器学习: 现在与未来"></a>[17] 机器学习: 现在与未来</h3><p>机器学习是人工智能的一个分支学科，主要研究的是<strong>让机器从过去的经历中学习经验，对数据的不确定性进行建模，在未来进行预测</strong>。(学习经验、对不确定性建模、对未来预测)</p>
<p>谷歌 AlphaGo 的成功，告诉我们<strong>结合机器学习与传统符号搜索方法可以解决人工智能里相对复杂的推理问题</strong>（解决复杂推理问题的途径）</p>
<p>在计算机视觉领域，即使我们在人脸识别和图片分类上取得了不小的成就，但是对于关系理解和完整的场景认知，现在系统能做到的还很有限（后面 CV 的研究重点应该放在 <strong>关系理解和完整的场景认知</strong> 上）</p>
<h3 id="18-小样本的类人概念学习与大数据的深度强化学习"><a href="#18-小样本的类人概念学习与大数据的深度强化学习" class="headerlink" title="[18] 小样本的类人概念学习与大数据的深度强化学习"></a>[18] 小样本的类人概念学习与大数据的深度强化学习</h3><p>这篇文章就是介绍 Human-level concept learning through probabilistic program induction，贝叶斯规划学习的，做的是小样本学习问题</p>
<p>深度学习是一种机器学习中<strong>建模数据的隐含分布的多层表达</strong>的算法</p>
<p>强化学习，其实就是一个连续决策的过程，其特点是不给任何数据做标注，仅仅提供一个回报函数，这个回报函数决定当前状态得到什么样的结果(比如“好”还是“坏”)，从数学本质上来看，还是一个马尔科夫决策过程。强化学习最终目的是让决策过程中整体的回报函数期望最优。</p>
<h3 id="19-人工智能-“热闹”背后的“门道”"><a href="#19-人工智能-“热闹”背后的“门道”" class="headerlink" title="[19] 人工智能: “热闹”背后的“门道”"></a>[19] 人工智能: “热闹”背后的“门道”</h3><h4 id="人类智慧、人类智能、人工智能"><a href="#人类智慧、人类智能、人工智能" class="headerlink" title="人类智慧、人类智能、人工智能"></a>人类智慧、人类智能、人工智能</h4><p><strong>人类智慧</strong>最为充分的表现就是为了实现自己的目标而不断地<strong>发现问题、定义问题和解决问题的能力</strong></p>
<p><strong>发现问题和定义问题的能力</strong>是人类智慧之中最具创造性的能力，这是因为，发现问题和定义问题是人类寻求进步的第一步，也是最重要的一步;<strong>这种能力有赖于人类的目的、知识、直觉、想象、审美、灵感和顿悟这样一些抽象的隐性能力</strong>，因此通常被称为“<strong>隐性智慧能力</strong>”</p>
<p><strong>解决问题的能力</strong>则是人类智慧之中最具操作性的能力，主要有赖于获取信息、提炼知识、演绎智能策略和执行智能策略这样一些显性的操作能力，因此通常被称为“<strong>显性智慧能力</strong>”</p>
<p>人们把隐性智慧和显性智慧的整体称为“人类智慧”，<strong>把具有操作性特色的显性智慧称为“人类智能”</strong>。于是，可以把探索、理解和模拟“人类智能(显性智慧能力)”的研究称为“人工智能”研究。</p>
<p>弱人工智能：对“<strong>人类智能</strong>(显性智慧能力)”的探索、理解、模拟和扩展</p>
<p>强人工智能：对“<strong>人类智慧</strong>(包括隐性智慧和 显性智慧两者)”的探索、理解、模拟和扩展</p>
<h4 id="AI-时代的工作方式"><a href="#AI-时代的工作方式" class="headerlink" title="AI 时代的工作方式"></a>AI 时代的工作方式</h4><p>AI可以远远高于人类的工作速度、远远优于人类的工作精度、远远胜过人类的工作耐力，协助人类解决各种各样的问题，甚至在危险环境和极端环境等场合代替人类去完成各种任务</p>
<p>人类发挥驾驭和引领的作用主要负责发现问题和定义问题，即负责</p>
<ol>
<li>明确描述所要解决的问题</li>
<li>预设问题解决所应当达到的目标</li>
<li>提供解决问题所需要的知识</li>
</ol>
<p>这三者的联合就构成了解决问题的工作框架</p>
<p>这就是人们所说的“人机 共生”的工作方式，更确切地说是“人为 主、机为辅的人机合作”工作方式。</p>
<p>基于结构模拟的人工智能(<strong>人工神经网络</strong>)的原理是:在投入工作之前需 要对人工神经网络进行训练，以便<strong>学习到求解问题所需要的经验知识(表现为各个神经元之间的连接权重)</strong>，从而解决相应的问题。</p>
<h3 id="20-人工智能研究的三大流派-比较与启示"><a href="#20-人工智能研究的三大流派-比较与启示" class="headerlink" title="[20] 人工智能研究的三大流派: 比较与启示"></a>[20] 人工智能研究的三大流派: 比较与启示</h3><ol>
<li><strong>符号主义</strong>认为智能是基于逻辑规则的符号操作;</li>
<li><strong>联结主义</strong>认为智能是脑神经元构成的信息处理系统;</li>
<li><strong>行为主义</strong>认为智能是通过感知外界环境做出相应的行为</li>
</ol>
<h4 id="符号主义-1"><a href="#符号主义-1" class="headerlink" title="符号主义"></a>符号主义</h4><p>符号主义，也被称之为<strong>逻辑主义</strong>，它是<strong>基于还原论</strong>的理性主义方法。该学派<strong>认为智能的基本元素是“符号”</strong>，人的<strong>认知过程是一个信息加工过程</strong>，通过<strong>对符号的逻辑演绎与推理等方式可以将智能活动表达出来</strong>。他们将信息加工系统也就是通常所理解的<strong>“认知系统”看作是一个巨大的符号加工系统</strong>，将智能系统理解为物理符号系统。设计好的信息处理程序将智能活动形式化为某种算法，通过算法对搜集到的信息进行逻辑处理，使得机器最终输出智能结果。</p>
<p>符号主义的缺点：还原论的理性主义方法<strong>无法对复杂系统的问题进行有效处理</strong>，简单的线性分解会使得系统复杂性遭到破坏，并且形式化的处理方式<strong>对常识问题采取了回避态度</strong>；复杂的现实世界，使得符号系统无法周全地完成万能逻辑推理体系，从而开始走向衰落。逻辑演绎并非人类智能的全部，对符号主义而言，<strong>非逻辑思维是其无法实现的障碍</strong>;对<strong>人类直觉思维、情感思维以及联想思维</strong>等的模拟，是符号主义的“短板”</p>
<h4 id="联结主义"><a href="#联结主义" class="headerlink" title="联结主义"></a>联结主义</h4><p>联结主义也被称之为<strong>仿生学派</strong>。通过<strong>模拟人类神经系统的结构与功能</strong>，联结主义试图使机器拥有智能</p>
<p>人工神经网络科学研究是联结主义研究的重要部分</p>
<h4 id="行为主义"><a href="#行为主义" class="headerlink" title="行为主义"></a>行为主义</h4><p>行为主义，也被称之为<strong>进化主义</strong>。行为主义认为<strong>智能行为就是通过与环境交互对感知结果做出相应行为</strong>。基于控制论的“感知—动作”模式，行为主义希望能够通过模拟生物的进化机制，使机器获得自适应能力</p>
<p>行为主义认为智能取决于感知与行为，以及智能取决于对外界环境的自适应能力的观点</p>
<p>由于行为主义的经验主义表现，在智能的实现过程中，不存在符号主义里无限的形式系统的尴尬，也不像联结主义那样需要对人体结构极度透彻的了解。它只需要智能体通过“感知—动作”型控制系统，以<strong>进化计算或强化学习</strong>的方法，<strong>通过对外部感知而做出的反应进行进化和学习</strong>，同时找寻合理的协调机制对智能体内部进行自我协调与主体间协调</p>
<p>与符号主义及联结主义相比，行为主义不再执着于 “内省式”的沉思，而是在与外界交互过程中用具体行为去拥抱真实世界</p>
<p>符号主义和联结主义还算是 “内省式”的沉思，向内求；</p>
<h4 id="三大流派的比较"><a href="#三大流派的比较" class="headerlink" title="三大流派的比较"></a>三大流派的比较</h4><ol>
<li>符号主义认为<strong>智能</strong>是基于逻辑规则的符号操作，人的<strong>认知</strong>是符号计算的过程 </li>
<li>联结主义认为<strong>智能</strong>是脑神经元构成的信息处理系统，人类的<strong>认知</strong>是脑神经元运动的经验结果</li>
<li>行为主义认为<strong>智能</strong>就是通过感知外界环境做出相应的行为，<strong>认知</strong>活动是对外界环境“感知—动作”的反应模式。</li>
</ol>
<p>基于<strong>对智能的理解不同</strong>，三大流派对“<strong>认知</strong>”都提出了自己的观点。</p>
<ol>
<li>符号主义是<strong>对人类逻辑演绎思维的模拟</strong>（认知就是计算）</li>
<li><p>联结主义对应的则是<strong>人类归纳推理思维的再现</strong>（认知是脑神经元运动的经验结果）</p>
</li>
<li><p>符号主义是直接复制人类的<strong>演绎推理能力</strong>来实现人工智能。</p>
</li>
<li>联结主义是从通过对生物<strong>内在组织的模仿</strong>来实现人工智能；</li>
<li>行为主义是从通过对生物<strong>外在行为的模拟</strong>来实现人工智能；</li>
</ol>
<p>符号主义认为智能是基于逻辑规则的符号操作，他们从“符号是智能行动的根基”出发，<strong>认为有机体是带程序的“活生生的机器</strong>，<strong>智能的核心就是根据某套规则作出理性决策”</strong>[10]，符号主义对于智能的表达方式，我们可以理解为是这样的一个过程:<strong>模拟人类逻辑思维去设计信息处理程序，对搜集到的信息符号化并按照程序进行逻辑处理，最后输出知识或行为完成智能的表达</strong>。符号主义主张功能模拟方法，坚信只要建立出一个通用的、万能的逻辑运算体系，就可以使计算机模拟人类的思维。诚然，知识是对信息的积累以及重新组合，智能的基本元素是符号的观点同人类的逻辑演算能力相契合。；符号主义研究者是企图绕过大脑和躯体，利用<strong>对行为的形式化去认知世界的本质</strong>的“<strong>最后的形而上学家</strong>”；由于注重智能对信息的逻辑运算结果，而不注重对信息的归纳总结，从而引起了联结主义的强烈排斥，与联结主义形成了很大的理论分歧</p>
<p>人工智能三种学派的研究方式各有优劣，“<strong>精确性</strong>”可以通过运用规则、符号进行表征而获取（符号主义），但“灵活性”却需要通过统计性描述才能得到（联结主义）</p>
<p>符号主义擅长<strong>知识推理</strong>，联结主义擅长<strong>技能建模</strong>，行为主义擅长<strong>感知行动</strong>（所以应该将三大流派尽量融合才可以通向更好的 AI）</p>
<p>符号主义对应 <strong>逻辑演绎</strong>，联结主义对应 <strong>归纳推理</strong>，逻辑演绎与归纳推理是人类智能中的两种主要思维能力。</p>
<h4 id="归纳和演绎"><a href="#归纳和演绎" class="headerlink" title="归纳和演绎"></a>归纳和演绎</h4><p>人类认识活动，总是先接触到个别事物，而后推及一般（归纳，Induction），又从一般推及个别（演绎，Deduction），<strong>如此循环往复，使认识不断深化</strong>。</p>
<p>归纳（Induction）就是从个别到一般，演绎（deduction）则是从一般到个别。</p>
<p>归纳和演绎这一组，描述的是<strong>特殊和一般的关系</strong></p>
<p>归纳是从个别的或者特殊的现象中概括出一般性的原理。而演绎是从一般性的原理推演出个别性的结论。</p>
<p><strong>综合和分析</strong>这一组是针对<strong>整体与要素</strong>的分析方法。一般先分析后综合，<strong>分析就是把整体拆解也各个部分，把复杂的事物分解为简单的要素而各个击破，单独分析每一个要素的特点，获得每个要素的属性</strong>。而<strong>综合是把对象各个部分、属性、要素有机的结合到一起看，从而对这个整体的属性有个认识</strong>。</p>
<h3 id="21-类脑智能研究的回顾与展望"><a href="#21-类脑智能研究的回顾与展望" class="headerlink" title="[21] 类脑智能研究的回顾与展望"></a>[21] 类脑智能研究的回顾与展望</h3><h4 id="图灵机模型和冯·诺依曼计算机体系结构"><a href="#图灵机模型和冯·诺依曼计算机体系结构" class="headerlink" title="图灵机模型和冯·诺依曼计算机体系结构"></a>图灵机模型和冯·诺依曼计算机体系结构</h4><p>图灵机模型和冯·诺依曼计算机体系结构的提出，从<strong>计算本质</strong>和<strong>计算结构</strong>方面分别奠定了现代信息处理和计算技术的两大基石，然而两者共同的问题是<strong>缺乏自适应性</strong>（一个是自适应性很难实现，另外是因为当时只要计算原子弹等物理问题，不需要自适应性）</p>
<p><strong>图灵计算的本质</strong>是<strong>使用预定义的规则对一组输入符号进行处理</strong>，规则是限定的、输入也受限于预定义的形式. <strong>图灵机模型取决于人对物理世界的认知程度，因此人限定了机器描述问题、解决问题的程度</strong>. </p>
<p>冯·诺依曼体系结构是<strong>存储程序式计算</strong>，<strong>程序也是预先设定好的，无法根据外界的变化和需求的变化进行自我演化</strong></p>
<p>（这么看来，只要图灵机模型和冯·诺依曼计算机体系结构，首先就不具有主动描述、定义问题的能力，由此看来是没有办法在图灵机上实现强人工智能的，因为强人工智能在解决问题的基础上还需要能够发现问题、定义问题）</p>
<p>难以实现<strong>海量多模态信息的选择性感知与注意</strong></p>
<p>目前几乎所有的人工智能系统都需要<strong>首先进行人工形式化建模</strong>，<strong>转化为一类特定的计算问题(如搜索、自动推理、机器学习等)进行处理</strong>（没想到除了机器学习以外，还有搜索和自动推理这两个的存在）</p>
<p>强人工智能的核心问题之一便是<strong>问题的自动形式化建模</strong></p>
<p>深度学习的缺点：深度学习的优越性能仍然限于特定领域（不是通用智能平台），其实现依赖大量标记样本（缺少弱监督、小样本学习能力，缺少知识和智慧），而且主要是离线学习，它的环境迁移和自适应能力较差（自适应能力弱、鲁棒性差）.</p>
<p>大数据大部分为<strong>非结构化数据</strong>，如<strong>图像、视频、语音、自然语言</strong>等.机器对这些数据的理解能力与人类相比还有明显的差距，正是这种能力的不足阻碍了大数据的充分和有效利用</p>
<p>人脑是一个<strong>通用</strong>智能系统，能举一反三、融会贯通，可处理视觉、听觉、语言、学习、推理、决策、规划等各类问题，可谓“一脑万用”.并且，人类的<strong>智能感知和思维能力</strong>是在成长和学习中自然形成和不断进化的，其<strong>自主学习和适应能力</strong>是当前计算机难以企及的.因此，<strong>人工智能的发展目标</strong>是构建像人脑一样能够自主学习和进化、具有类人通用智能水平的智能系统</p>
<h4 id="类脑智能-1"><a href="#类脑智能-1" class="headerlink" title="类脑智能"></a>类脑智能</h4><h5 id="什么是类脑智能？"><a href="#什么是类脑智能？" class="headerlink" title="什么是类脑智能？"></a>什么是类脑智能？</h5><p><strong>类脑智能</strong>是<strong>以计算建模为手段</strong>，<strong>受脑神经机制和认知行为机制启发</strong>，并通过软硬件协同实现的机器智能</p>
<p>由于类脑智能的手段主要是从机制上借鉴脑，而不是完全模仿脑，其对应的英文术语为“Brain inspired Inteligence”更为合适（这就是 脑启发啊）</p>
<p>类脑智能是脑与神经科学、认知科学、人工智能这三个科学的交叉学科</p>
<h5 id="为什么是借鉴而非模仿脑？"><a href="#为什么是借鉴而非模仿脑？" class="headerlink" title="为什么是借鉴而非模仿脑？"></a>为什么是借鉴而非模仿脑？</h5><p>人脑是进化的产物，在进化过程中存在各种设计妥协，因此从脑信息处理机制出发推动人工智能研究最优的途径应当是受脑启发、借鉴其工作机制，而不是完全地模仿（这一点很像商业里面，大鳄们分享自己的成功经验、原因，但完全 follow 经验肯定会失败，那只是幸存者偏差，不一定是导致对方成功的原因）</p>
<h5 id="认知科学中的类脑智能研究"><a href="#认知科学中的类脑智能研究" class="headerlink" title="认知科学中的类脑智能研究"></a>认知科学中的类脑智能研究</h5><p>回答的科学问题:“人类的心智如何能够在物理世界重现；其具体的探索即是<strong>人类思维如何在计算机系统上重现</strong></p>
<h5 id="计算神经学中的类脑智能研究"><a href="#计算神经学中的类脑智能研究" class="headerlink" title="计算神经学中的类脑智能研究"></a>计算神经学中的类脑智能研究</h5><p><strong>计算神经科学</strong>是以计算建模为手段，研究脑神经信息处理原理的学科（和人工智能一样都是以计算建模为手段，区别在于计算神经科学的目的是研究（我觉得是验证更合适）脑神经信息处理原理，人工智能的目的在于是实现我们想要的结果，所以一个是 Science，是发现科学，一个是 Engineering，是工程科学）</p>
<p>计算神经科学研究的重点是通过多尺度计算建模的方法<strong>验证</strong>各种认知功能的脑信息处理模型</p>
<h6 id="传统计算神经学与类脑智能的区别"><a href="#传统计算神经学与类脑智能的区别" class="headerlink" title="传统计算神经学与类脑智能的区别"></a>传统计算神经学与类脑智能的区别</h6><p>传统的计算神经科学仍然更为关注神经系统表现出来的物理现象(如振荡、相变等) 和<strong>微观尺度的建模</strong>（如神经元尺度精细的连接结构、脑区尺度反馈的机制），<strong>对于整体的脑认知系统相对缺乏框架级别的计算模型</strong></p>
<p>专注于极为精细的微观神经元及其微环路建模，目前较为完整地完成了特定脑区内<strong>皮质柱</strong>的计算模拟（可以看一下递归皮质网络）</p>
<h5 id="人工智能中的类脑智能研究"><a href="#人工智能中的类脑智能研究" class="headerlink" title="人工智能中的类脑智能研究"></a>人工智能中的类脑智能研究</h5><p>人工智能的<strong>符号主义</strong>研究出发点是<strong>对人类思维、行为的符号化高层抽象描述</strong>，而以人工神经网络为代表的<strong>连接主义</strong>的出发点正是对脑神经系统结构及其计算机制的<strong>初步模拟</strong>.</p>
<p>人工智能研究集中在<strong>类人行为建模</strong>上，目标一般为行为尺度接近人类水平；</p>
<p>类脑智能研究分为 <strong>类脑模型与类脑信息处理</strong>、<strong>类脑芯片与计算平台</strong> 这两个方向性，所以就像一级学科、二级学科一样，更具体的感兴趣方向是 <strong>类脑信息处理</strong></p>
<h4 id="认知脑计算模型的构建"><a href="#认知脑计算模型的构建" class="headerlink" title="认知脑计算模型的构建"></a>认知脑计算模型的构建</h4><p>传统人工智能系统的设计与实现思路是: 从待解决问题相关数据的特点与问题目标的角度出发，从计算的视角设计算法. 这使得所实现的智能系统只适用于解决某一类问题.</p>
<p><strong>类脑智能研究长期的目标是实现通用智能系统</strong>，这就需要首先研究人脑如何通过同一系统实现不同的认知能力，从中得到启发并设计下一代智能系统. 因此，类脑智能研究的首要任务是集成两百年来科学界对于人脑多尺度结构及其信息处理机制的重要认识，受其启发构建模拟脑认知功能的认知脑计算模型，特别需要关注<strong>人脑如何协同不同尺度的计算</strong>组件，进行动态认知环路的组织，完成不同的认知任务.</p>
<h5 id="认知脑计算模型研究内容"><a href="#认知脑计算模型研究内容" class="headerlink" title="认知脑计算模型研究内容"></a>认知脑计算模型研究内容</h5><ol>
<li>多尺度、多脑区协同的认知脑计算模型: 根据脑与神经科学实验数据与运作原理，构建认知脑计算模型的多尺度(神经元、突触、神经微环路、皮质柱、脑区)计算组件和多脑区协同模型，其中包括类脑的多尺度前馈、反馈、模块化、协同计算模型等;</li>
<li>认知/智能行为的类脑学习机制: 多模态协同与联想的自主学习机制，概念形成、交互式学习、环境自适应的机制等;</li>
<li>基于不同认知功能协同实现复杂智能行为的类脑计算模型: 通过计算建模实现哺乳动物脑模拟系统，实现具备<strong>感知</strong>、<strong>学习与记忆</strong>、<strong>知识表示</strong>、<strong>注意</strong>、<strong>推理</strong>、<strong>决策与判断</strong>、<strong>联想</strong>、<strong>语言</strong>等认知功能及其协同的类脑计算模型</li>
</ol>
<p>最核心的是研究 <strong>学习与记忆的计算模型</strong></p>
<p>所有认知任务相关的脑区中，学习与记忆遵循相同的法则:即<strong>赫布学习法则</strong>(Hebb’sLaw)与<strong>脉冲时序依赖的突触可塑性</strong>(缩写为STDP)</p>
<h5 id="认知脑计算模型-和-类脑信息处理-的区别？"><a href="#认知脑计算模型-和-类脑信息处理-的区别？" class="headerlink" title="认知脑计算模型 和 类脑信息处理 的区别？"></a>认知脑计算模型 和 类脑信息处理 的区别？</h5><p>我感觉 认知脑计算模型 就是计算神经学做的，所以研究认知脑计算模型是为了搞清楚机制；类脑信息处理 是 人工智能 做的，是为了对类人行为建模。</p>
<p><strong>模型</strong> Modeling 和 <strong>处理</strong>（依据 Cost Function 优化出想要的结果）</p>
<h4 id="类脑信息处理"><a href="#类脑信息处理" class="headerlink" title="类脑信息处理"></a>类脑信息处理</h4><p>需要在认知脑计算模型的基础上进一步抽象，选取最优化的策略与信息处理机制，建立类脑信息处理理论与算法，并应用于多模态信息处理中（由此可见，模型 和 信息处理机制是不一样的，模型应该是一个表示方式，是没有应用目的导向的，而怎么处理是含有目的导向的 ）</p>
<h5 id="研究内容："><a href="#研究内容：" class="headerlink" title="研究内容："></a>研究内容：</h5><ol>
<li><strong>感知信息特征表达与语义识别模型</strong>: 针对视觉(图像和视频)、听觉(语音和语言)、触觉等感知数据的分析与理解，借鉴脑神经机理和认知机理研究结果，研究感知信息的基本特征单元表示与提取方法、基于多层次特征单元的感知信息语义(如视觉中的场景、文字、物体、行为等)识别模型与学习方法、<strong>感知中的注意机制计算模型以及结合特征驱动和模型驱动的感知信息语义识别</strong>方法等;</li>
<li>多模态协同自主学习理论与方法: 人脑的环境感知是多模态交互协同的过程，同时感知特征表示和语义识别模型在环境感知过程中不断地在线学习和进化</li>
<li>多模态感知大数据处理与理解的高效计算方法:面向大数据理解的应用需求，基于类脑感知信息表达和识别模型，研究面向感知大数据处理的新型计算模式与方法，如<strong>多层次特征抽取和识别</strong>方法，<strong>结合特征和先验知识、注意机制的多层次高效学习、识别与理解等</strong>;</li>
<li>类脑语言处理模型与算法: 借鉴人脑语言处理环路的结构与计算特点，实现具备语音识别、实体识别、句法分析、<strong>语义组织与理解</strong>、<strong>知识表示与推理</strong>、情感分析等能力的统一类脑语言处理神经网络模型与算法.</li>
</ol>
<blockquote>
<p>由 Hawkins 等人提出的分层时序记忆(Hierarchical Temporal Memory)模型更为深度借鉴了脑信息处理机制，主要体现在该模型借鉴了脑皮层的6层组织结构及不同层次神经元之间的信息传递机制、皮质柱的信息处理原理等.该模型非常适用于处理带有时序信息的问题，并被广泛地应用于物体识别与跟踪、交通流量预测、人类异常行为检测等领域.</p>
</blockquote>
<p>这里的 Hawkins 就是 Jeff Hawkins，写 《On Intelligence》也就是 人工智能的未来的那个人，后来他徒弟出走创建的公司 vicarious 用的技术也是这个 HTM 的衍生，Vicarious 后来的 Paper 也上了 Science，就是 递归皮质网络</p>
<p>大部分数据是图像视频、语音、自然语言等<strong>非结构化数据</strong>，需要类脑智能的理论与技术来提升机器的数据分析与理解能力.</p>
<h5 id="我感兴趣的内容"><a href="#我感兴趣的内容" class="headerlink" title="我感兴趣的内容"></a>我感兴趣的内容</h5><p><strong>多层次特征抽取，结合特征和先验知识、注意机制，语义组织与理解，知识表示与推理，感知中的注意机制计算模型</strong></p>
<h3 id="22-类脑计算芯片与类脑智能机器人发展现状与思考"><a href="#22-类脑计算芯片与类脑智能机器人发展现状与思考" class="headerlink" title="[22] 类脑计算芯片与类脑智能机器人发展现状与思考"></a>[22] 类脑计算芯片与类脑智能机器人发展现状与思考</h3><p>“类脑芯片”是指参考人脑神经元结构和人脑感知认知方式来设计的芯片。</p>
<h4 id="类脑芯片研究的两大方向"><a href="#类脑芯片研究的两大方向" class="headerlink" title="类脑芯片研究的两大方向"></a>类脑芯片研究的两大方向</h4><h5 id="方向一：神经形态芯片"><a href="#方向一：神经形态芯片" class="headerlink" title="方向一：神经形态芯片"></a>方向一：神经形态芯片</h5><p>“神经形态芯片”就是一种类脑芯片，顾名思义，它侧重于<strong>参照人脑神经元模型及其组织结构来设计芯片结构</strong>。</p>
<p>代表是 IBM 的 TrueNorth，高通的 Zeroth。高通的 Zeroth 在业界引起了巨大的震动。原因就在于它可以融入到高通公司量产的 Snapdragon 处理器芯片中，以协处理的方式提升系统的认知计算性能，并可实际应用于手机和平板电脑等设备中，支持诸如语音识别、图像识别、场景实时标注等实际应用并且表现卓越（在产业界能应用就会有很大的意义）</p>
<p>TrueNorth 芯片采用了神经形态的组织结构和新兴的“脉冲神经网络”算法</p>
<h5 id="方向二："><a href="#方向二：" class="headerlink" title="方向二："></a>方向二：</h5><p>参考人脑感知认知的<strong>计算模型而非神经元组织结构</strong>（由此看出，计算模型 和 神经元组织结构 是不一样的，计算层面 应该是高于 （计算机）组织结构的，一个是应用抽象层，一个是物理层）（专门针对深度学习算法设计，而不像 CPU 那样通用，就和算法一样，先验越强越符合，算法性能更好；这里也是，越是针对某类算法设计，效能越好）</p>
<p>代表就是 中科院的 DianNao 和 DaDianNao，<strong>寒武纪</strong>（全球首款深度学习处理器芯片）</p>
<p>类脑芯片完全可以<strong>同时参考神经元组织结构并支持成熟的认知计算算法</strong>，这并不矛盾。</p>
<h4 id="冯诺依曼架构-vs-大脑架构"><a href="#冯诺依曼架构-vs-大脑架构" class="headerlink" title="冯诺依曼架构 vs 大脑架构"></a>冯诺依曼架构 vs 大脑架构</h4><p>目前，传统计算机芯片主要基于冯诺依曼架构，<strong>处理单元和存储单元分开，通过数据传输总线相连</strong>。芯片总信息处理能力受总线容量的限制，构成所谓 “<strong>冯诺依曼瓶颈</strong>”。而且传统计算机的处理单元一直处于工作状态，导致能耗巨大。同时，由于需要精确的预编程，传统计算机无法应对编程以外的情况和数据。</p>
<p>大脑结构则完全不同：<strong>神经元 (处理单元) 和突触 (存储单元) 位于一体，不需要高能耗的总线连接</strong>，突触是神经元之间的连接，具有可塑性，能够随所传递的神经元信号强弱和极性调整传递效率，并在信号消失后保持传递效率。</p>
<h3 id="23-国务院关于印发新一代人工智能发展规划的通知"><a href="#23-国务院关于印发新一代人工智能发展规划的通知" class="headerlink" title="[23] 国务院关于印发新一代人工智能发展规划的通知"></a>[23] 国务院关于印发新一代人工智能发展规划的通知</h3><p>链接：<a href="http://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm" target="_blank" rel="noopener">http://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm</a></p>
<h4 id="适合我的人工智能关键共性技术"><a href="#适合我的人工智能关键共性技术" class="headerlink" title="适合我的人工智能关键共性技术"></a>适合我的人工智能关键共性技术</h4><p><strong>自主无人系统</strong>的智能技术。重点突破自主无人系统计算架构、<strong>复杂动态场景感知与理解</strong>、实时精准定位、面向复杂环境的适应性智能导航等共性技术，无人机自主控制以及汽车、船舶和轨道交通自动驾驶等智能技术，服务机器人、特种机器人等核心技术，支撑无人系统应用和产业发展。研究复杂环境下基于计算机视觉的定位、导航、识别等机器人及机械手臂自主控制技术。</p>
<p>关键词就是 自主无人系统 和 复杂动态场景感知与理解</p>
<h3 id="24-谈方法论：归纳与统计"><a href="#24-谈方法论：归纳与统计" class="headerlink" title="[24] 谈方法论：归纳与统计"></a>[24] 谈方法论：归纳与统计</h3><p>链接：<a href="https://zhuanlan.zhihu.com/p/34190289" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34190289</a></p>
<p>我找到这篇文章是因为我想知道 归纳 与 统计有什么区别？都说统计机器学习，机器学习有不用统计的吗？</p>
<p>认识是从个别到一般, 又由一般再到个别的过程。通过个别认识一般的主要思维方法是归纳, 由一般认识个别的主要思维方法是演绎。</p>
<h4 id="归纳"><a href="#归纳" class="headerlink" title="归纳"></a>归纳</h4><p>归纳方法是<strong>从个别或特殊事物概括出共同本质或一般原理的逻辑思维方法</strong>, 在逻辑上叫做<strong>归纳推理</strong>。</p>
<p><strong>不完全归纳法</strong>是在考察了某类事物的部分个别对象后，就得出关于这类事物的一般性原理的方法。实际上就是对事物全集的子集进行抽象，得到部分子集的共性，而这个共性不一定就是全集的共性。（这点和机器学习很像）</p>
<p><strong>结论的跨层级性</strong>。这个用集合去理解，<strong>通过对子集的归纳，得到了对母集属性的刻画，认识就从个性上升为共性，部分上升为整体</strong>，从偶然性上升为必然性。</p>
<h4 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h4><p><strong>统计</strong>是<strong>不完全归纳</strong>常用的<strong>数学工具</strong>之一，统计方法<strong>实质上是归纳的低层次具现化</strong>。</p>
<p>统计，就是对样本性质做调查分析，去估计总体的性质。掌握统计思想，可以使得我们花费较小的成本和精力，就可以得到对广泛事物的高层级认识，达到<strong>见微知著、见微知萌</strong>的作用。（这么感觉，统计、压缩感知、机器学习 都是 见微知著、见微知萌 的味道）</p>
<p><strong>统计的局限性</strong>：统计得到的数据和结论，仅仅只是现实的直观的表象的，得到的是<strong>相关性，而不直接是事物之间的本质联系</strong>，需要掌握好其他思维方法才能够利用好统计工具。比如说，我们统计一批老鼠，所有的正常老鼠碰见人都会逃跑，然后所有的被剁掉四条腿的老鼠看见人都不会跑，于是得出结论：老鼠的耳朵在腿上。</p>
<h3 id="25-人工智能-天使还是魔鬼"><a href="#25-人工智能-天使还是魔鬼" class="headerlink" title="[25] 人工智能: 天使还是魔鬼?"></a>[25] 人工智能: 天使还是魔鬼?</h3><p>人工智能系统的能力维度可分为<strong>信息感知</strong> (perceiving)、<strong>机器学习</strong> (learning)、<strong>概念抽象</strong> (abstracting) 和<strong>规划决策</strong> (reasoning). 目前人工智能系统在信息感知和机器学习方面进展显著, 但是在概念抽 象和规划决策方面能力还很薄弱. 总体上看, 目前的人工智能系统可谓有智能没智慧、有智商没情商、 会计算不会 “算计”、有专才无通才. 人工智能还有很多不能.</p>
<p><strong>统计学习</strong>成为人工智能走向实用的<strong>理论基础</strong>。</p>
<h3 id="26-类脑智能研究现状与发展思考"><a href="#26-类脑智能研究现状与发展思考" class="headerlink" title="[26] 类脑智能研究现状与发展思考"></a>[26] 类脑智能研究现状与发展思考</h3><h4 id="以往的人工智能"><a href="#以往的人工智能" class="headerlink" title="以往的人工智能"></a>以往的人工智能</h4><p>以往人工智能的研究成果属于<strong>行为尺度模拟部分智能的计算模型</strong>（可见基于类脑智能的人工智能研究是希望能够<strong>下沉到 脑信息处理机理、脑组织结构的尺度上</strong>模拟智能的计算模型，尺度上的下移）（这个下移有两个促因，一个是对脑机制的了解更加深入了，机理上到了某个相对成熟的点，另一个是现在的人工智能太消耗计算资源和数据，而人脑才 20 瓦可以小样本学习，有充分的动机）</p>
<h4 id="脑与神经科学对人工智能的潜在启发"><a href="#脑与神经科学对人工智能的潜在启发" class="headerlink" title="脑与神经科学对人工智能的潜在启发"></a>脑与神经科学对人工智能的潜在启发</h4><p><strong>类脑智能</strong>研究的<strong>核心</strong>是受脑启发构建<strong>机制类脑、行为类人</strong>的类脑智能计算模型（计算神经学是要搞懂人脑认知机制、建立认知计算模型，这是机制类脑；人工智能则是实现 行为类人，行为类人 可以通过组织架构类脑来实现（脑启发），也可以不通过借鉴来实现）</p>
<h5 id="脑的多尺度结构"><a href="#脑的多尺度结构" class="headerlink" title="脑的多尺度结构"></a>脑的多尺度结构</h5><ol>
<li>微观：神经元、突触工作机制及其特性</li>
<li>介观：网络连接模式</li>
<li>宏观：脑区间的链路及其协同特性</li>
</ol>
<p>在<strong>微观层面</strong>，突触方面， 如时序依赖的突触可塑性(Spike-Timing Dependent Plas- ticity, STDP)是一类<strong>时序依赖的连接权重学习规则</strong>，突触权值的变化主要依赖于细胞放电发生于突触前神经元和突触后神经元的先后时刻，通过对放电时间差与权重 更新建立数学映射关系，来描述网络中的神经连接强度的变化情况。（目前看来，我的兴趣点不会在围观层面去借鉴神经元、突触的工作机制）</p>
<p>在<strong>介观层面</strong>，特异性的脑区内部的连接模式和随机性的网络背景噪声的有效融合，例如生物神经网络中的泊松背景噪声对生物神经网络的学习和训练过程起到极大的促进作用（介观也不是我的兴趣点）</p>
<p>在<strong>宏观层面</strong>，脑区之间的连接不仅决定信号的传递，而且反映了<strong>信息处理的机制</strong>。如脑区之间的<strong>前馈连接可能反映了信息的逐层抽象机制</strong>，而<strong>反馈连接则反映了相对抽象的高层信号对低层信号的指导或影响</strong>。此外，有些脑区负责融合来自不同脑区的信号，从而使对客观对象的认识更为全面(如颞极<strong>对多模态感知信号的融合</strong>)，而有些脑区在接收到若干脑区的输入后则负责在问题求解的过程中<strong>屏蔽来自问题无关脑区的信号</strong>（宏观层面是我的兴趣点，不管是 前馈对应的 信息的逐层抽象机制，反馈对应的 高层信号对低层信号的指导，还是 颞极对多模态感知信号的融合，还是屏蔽来自问题无关脑区的信号（注意机制）都是我感兴趣的，因此，我对于宏观尺度脑的工作机制、也就是<strong>信息处理机制比较感兴趣，而非脑的信号传递机制</strong>  ）</p>
<p>要实现人类水平的智能，需要计算模型能够融合来自微观、介观、宏观多尺度脑结构和信息处理机制的启发。实现跨尺度机制的融合（但我对微观、介观并不感兴趣）</p>
<h4 id="新一代人工神经网络模型"><a href="#新一代人工神经网络模型" class="headerlink" title="新一代人工神经网络模型"></a>新一代人工神经网络模型</h4><p>究竟什么才算新一代人工神经网络模型，从感知机-多层感知机-脉冲神经网络 这么去看，似乎指的是借鉴了更多生物机制的脉冲神经网络算新一代。但我觉得还是不要这样去想好，我感觉还是任意在结构上做了创新的都算新的神经网络模型，所以就不用去纠结是不是 新一代，只要是 新的就可以了。由此来说，这也就可以是我可以做的问题了。</p>
<p>目前在神经元的类型、突触的类型及其工作机理、 网络权重更新、网络背景噪声等方面，神经生物学的研究都取得了可以被计算模型应用的进展。（意思是说这些领域的成果形式化成数学模型可以加到现有结构来）</p>
<p><strong>自顶向下的视觉注意就是来自于从高级认知脑区(如PFC、LIP 等)的脑活动到初级视觉脑区的反馈信号</strong>（这个脑的反馈，和 BP 优化根据 Loss 更新前面的权重一样吗？后者能算反馈吗？）</p>
<h4 id="基于记忆、推理和注意的认知"><a href="#基于记忆、推理和注意的认知" class="headerlink" title="基于记忆、推理和注意的认知"></a>基于记忆、推理和注意的认知</h4><p>记忆 (Memory)、推理(Reasoning)和注意(Attention)等 机制逐渐成为神经网络领域的新研究热点（Attention 多火就不说了，Reasoning 应该是 朱松纯那边的工作吧，记忆，不太清楚，也许是 LSTM 那些？ 但这些的确是可以做的方向。）</p>
<p>合理采用记忆、推理和注意机制，可以有效地解决人工智能的很多核心问题</p>
<p>目前主要注意的问题：</p>
<ol>
<li>记忆单元中存储哪些内容?</li>
<li>神经记忆单元中记忆的表示形式?</li>
<li>记忆单元规模较大时如何进行快速语义激活?</li>
<li>如何构建层次化记忆结构? </li>
<li>如何进行层次化信息推理?</li>
<li>如何对冗余信息进行遗忘或压缩处理?</li>
<li>如何评价系统的推理和理解能力?</li>
<li>如何从人类或动物记忆机制中获得启发?</li>
</ol>
<p>“记忆” 具有自适应性，具有记忆单元的智能体可以从经验中学习，概括能力更佳，可以利用先验信息在不完整数据中进行更好地推理和预测。</p>
<p>从智能应用角度看，短时记忆由<strong>当前环境数据</strong>产生的状态编码更新和存储，而长时记忆是<strong>对历史信息进行高度经验性概括</strong>的编码，如概念、实体和结构化知识的表示。</p>
<p>早期利用神经网络对信息进行编码记忆的模型注重于<strong>神经记忆单元的结构化设计</strong>（早期放在怎么设计带有记忆功能的神经元单元结构，代表就是 LSTM）</p>
<p>注意机制是由外部刺激引发注意转变，从环境和非定长记忆编码单元中选择重 要信息进行融合，得到当前时刻刺激下有限长度的语义向 量。</p>
<p>人类视觉系统能够<strong>在复杂场景中</strong>迅速地将注意力集中在显著的视觉对象上，这个过程称之为<strong>视觉选择性注意</strong>（VSA）。人类的视觉注意过程包括两个方面:<strong>由刺激驱动的自下而上的视觉注意过程</strong>和<strong>由任务驱动的自上而下的视觉注意过程</strong>。（在 CV 中，复杂场景可以成为为什么要应用注意机制的一个动机）</p>
<p>DNN 所忽略的很多生物规则可能恰恰是实现类脑智能的关键(如对于时间的编码、抑制性神经元在网络中的特殊作用等)</p>
<h4 id="关于类脑智能模型的进一步思考"><a href="#关于类脑智能模型的进一步思考" class="headerlink" title="关于类脑智能模型的进一步思考"></a>关于类脑智能模型的进一步思考</h4><p> 脑是自然界中最复杂的系统之一，由上千亿(10的11次方) 神经细胞(神经元)通过百万亿(10的14次方)突触组成巨大网络（我知道神经元数量是上千亿，没想到突触是百万亿）</p>
<p> 大量简单个体行为产生出复杂、不断变化且难以预测的行为模式(这种宏观行为有时叫做<strong>涌现</strong>)，并通过学习和进化 过程产生适应，即改变自身行为以增加生存或成功的机 会。（智能是通过涌现 产生的么？如果可以，那就联结主义的目的就达到了）</p>
<h5 id="目前脉冲神经网络的缺陷"><a href="#目前脉冲神经网络的缺陷" class="headerlink" title="目前脉冲神经网络的缺陷"></a>目前脉冲神经网络的缺陷</h5><p>更多地 借鉴了神经元、突触等微观尺度的机制，其在学习方式 上更加接近于无监督学习，计算效能也比深度网络高出 一个量级，但由于网络训练只考虑了两个神经元之间的局部可塑性机制，<strong>对介观(如神经元网络连接、皮层结 构)、宏观尺度(如脑区之间的网络连接)的借鉴非常缺乏 </strong>，因此在性能上与 DNN 等模型还存在一定差距。</p>
<p>让机器像人一样不断地从周围环境对知识、模型结构和参数进行学习和自适应进化，是机器学习的最高目标，这种学习方式被称为<strong>终生学习</strong>(Life-Long Learning)或永不停止的学习(Never-Ending Learning)[53,54]，里面混合监督学习、无监督学习、半监督学习、增量学习、迁移学习、多任务学习、交互学习等多种灵活方式。</p>
<p>认知科学认为，<strong>一个概念的形成具有组合性和因果性</strong>，因此认知一个新概念时用到了已有的经验积累，从而具有个例的举一反三能力。</p>
<h3 id="27-类脑-受脑启发的-计算的问题与视觉认知"><a href="#27-类脑-受脑启发的-计算的问题与视觉认知" class="headerlink" title="[27] 类脑(受脑启发的)计算的问题与视觉认知"></a>[27] 类脑(受脑启发的)计算的问题与视觉认知</h3><p>是 郑南宁 院士做的报告。</p>
<p>我之后的工作应该也是试图从<strong>脑网络连接机制</strong>及<strong>视觉认知</strong>的角度探讨类脑计算可能的实现途径和方法</p>
<h5 id="目前深度学习模型的缺点："><a href="#目前深度学习模型的缺点：" class="headerlink" title="目前深度学习模型的缺点："></a>目前深度学习模型的缺点：</h5><p>对训练数据过度依赖，大多采用前馈连接，<strong>缺乏逻辑推理和对因果关系的表达能力</strong>、<strong>缺乏短时记忆</strong>和高效的<strong>无监督学习能力</strong>，<strong>很难处理具有复杂时空关联性的任务</strong>。 这些问题促使我们去寻求新的计算模式。</p>
<p>寻求类脑计算的物理实现形式，我们需要<strong>在物理的、符号的、语义的三个层面</strong>上弄清楚如下两者之间的 关系，即:计算装置与计算过程之间的关系，大脑与认知之间的关系。</p>
<h5 id="图灵机的局限（判符号主义死刑）"><a href="#图灵机的局限（判符号主义死刑）" class="headerlink" title="图灵机的局限（判符号主义死刑）"></a>图灵机的局限（判符号主义死刑）</h5><p>图灵机模型表明，存在一种普适的计算机制，它<strong>可以完成任何可用形式化方式描述的计算任务</strong>（只要这个问题可以用形式化表达，那么这个任务就可以用图灵机完成，因此问题的关键在于脑认知机制能否被形式化），而且图灵测试的可能性是建立在符号系统所具有的可塑性的基础之上。计算形式的普适性使得冯诺依曼结构的现代计算机可以完成图灵机表征 的任何过程，但<strong>前提是能将人类或其他生物的认知行为抽象出诸如:规则、推理、推论、归纳等这样的语义规律性，并把它们看作是关于符号的计算。</strong>（这就是符号学派背后的逻辑）然而，<strong>人类的大脑具有感知、识别、学习、联想、记忆 和推理等功能，并不能全部用符号计算的形式来实现</strong>（这不就从根本上判了符号主义死刑了么）。这些功能与大脑的结构存在着对应关系，并且大脑的神经网络系统具有多层的反馈机制，如<strong>来自于高级 “控制” 脑区到初级视觉脑区的反馈信号，形成了基于内容和语义的视觉 “选择性注意” 机制</strong>。</p>
<p><strong>类脑计算就是受上述脑功能和脑神经网络连接机制启发的一种计算架构</strong>，它以神经形态计算的模式来部分模拟大脑功能与其结构的对应关系和反馈连接， 增强人工智能及其计算效率，不完全依赖现有冯诺依曼计算结构，也不是复制人类的大脑或简单地建造一种模拟神经元功能的芯片，更不是去完全替代冯诺依曼计算结构。</p>
<h5 id="大脑认知的层次"><a href="#大脑认知的层次" class="headerlink" title="大脑认知的层次"></a>大脑认知的层次</h5><p>人类大脑认知活动分为三个不同层次:</p>
<ol>
<li>直觉</li>
<li>形象思维和逻辑思维</li>
<li>灵感与顿悟 </li>
</ol>
<p>其中<strong>形象思维和逻辑思维是在人的意识控制之下进行的</strong>，而<strong>直觉、灵感与顿悟则是一种潜意识活动，是大脑的自主信息处理功能的具体表现</strong>。直觉、灵感与顿悟是人类在发明创造的过程中经常表现出来的认知活动。</p>
<p><strong>直觉是以知识经验为基础，跳跃地、直接抽象地识别事物的本质</strong>，直觉判断往往是为了迅速解决当前的问题，而灵感则是在某种偶然因素的启发下使问题得以顿悟。然而，<strong>人工智能的很多研究工作主要集中在完整信息(结构化或半结构化)的处理，用特征学习和定量计算的模式来实现大脑认知的 “形象思维和逻辑思维</strong>”（现在的人工智能研究其实都是为了实现大脑认知的形象思维和逻辑思维），将深度学习与概率网络结合，也可在一定程度上对完整信息进行直觉判断，而对于实现非完整信息的直觉判断还无能为力。</p>
<h5 id="传统人工智能的局限性"><a href="#传统人工智能的局限性" class="headerlink" title="传统人工智能的局限性"></a>传统人工智能的局限性</h5><ol>
<li>需要<strong>对问题给出形式化描述</strong>(即抽象出一个可解析的数学模型，如果抽象不出，即归纳为不可解问题);</li>
<li>需要对形式化描述设计确定的算法(容易产生 NPC 类问题)</li>
<li>处理的结果无法表示现实世界问题所存在的测不准性和不完备性</li>
<li>图灵意义下的可计算问题都是可递归的(“可递归的” 都是有序的)</li>
<li>用 “度量” 来区分模式、只能处理可向量化的数据</li>
</ol>
<p>传统人工智能的<strong>基本理论框架</strong>建立在 <strong>“思维即计算” 的理论基点</strong>上，以 “<strong>演绎逻辑和语义描述</strong>” 和 “<strong>形式化方法</strong>” <strong>实现计算</strong>。<strong>将 “思维” 抽象为 “符号计算”</strong>（像low-rank、机器学习的模型这些可以认为是将思维抽象成数学符号计算吗？）对人工智能的发展产生了重大的推动作用，但为所有的对象建立模型是不可能的，也未必是完备的。这里存在条件问题(Qualification Problem)和分支问题 (Ramification Problem)，即不可能枚举出一个行为的所有先决条件，也不可能枚举出一个行为的所有分支。而大脑的认知具有多种方式，如对环境的理解、非完整信息的处理、复杂时空关联的任务，还有最基本的形象思维，特别是人脑在非认知因素和认知功能之间的相互作用，它们是形式系统难以，甚至不能描述的。</p>
<p>人类能够为未来做出计划、可以灵活处理问题并且向他人学习，这些是人类智能的基本属性。而<strong>传统人工智能方法，无法实现类似人一样思考推理的机器，去深度解决自然场景描述和环境理解等知识推理问题，也难以完成许多对于人类大脑来讲轻而易举的一些任务</strong>。因此，人们期望借鉴大脑的工作原理发展出一种新的智能机器的架构或称之为强人工智能的计算理论和方法。（自然场景描述和环境理解 属于 知识推理问题，这里面既要有知识，又要有推理的成分）</p>
<h5 id="冯诺依曼计算架构的面临的困境"><a href="#冯诺依曼计算架构的面临的困境" class="headerlink" title="冯诺依曼计算架构的面临的困境"></a>冯诺依曼计算架构的面临的困境</h5><p>冯 ̇诺依曼架构的计算机可以实现任何可用形式化方法描述的计算任务。（关键在于问题能否被形式化定义），但我们面临的计算任务并不都是可用形式化方法来描述的。（那不用形式化定义，用什么来定义呢？）</p>
<p>冯 ̇诺依曼计算架构主要是由于<strong>分离的运算和存储结构</strong>（总线传输瓶颈）、以及有限的并行度(指令级、数据级、线程和任务级)（人脑是高度并行的）、有限的容错和鲁棒性，特别是功耗问题。</p>
<h4 id="大脑网络连接与认知的关系"><a href="#大脑网络连接与认知的关系" class="headerlink" title="大脑网络连接与认知的关系"></a>大脑网络连接与认知的关系</h4><p>类脑计算的最根本的挑战是人类<strong>大脑信息处理和认知功能的复杂性</strong>（所以机制上是很复杂的咯？那这个符合“涌现”的定义吗？）。</p>
<p>大脑是由多个不同区域的脑组织连接而成的网络达成共识，其中各个脑组织区域负责不同的认知任务。层次化、多尺度、高度连通、多中央枢纽的网络拓扑结构，决定着大脑任务相关以及自发的活动。</p>
<p>通过发掘大脑<strong>结构连接</strong>(structural connectivity)、<strong>功能连接</strong>(functional connectivity)和<strong>有效连接</strong>(effective connectivity)的聚合和分离(敛散性)来洞察大脑的认知机理 (图 4)。其中，大脑的<strong>结构连接是相对静态的</strong>，而<strong>功能连接和有效连接具有时、空动态演化的特性</strong>， 具体表现在<strong>连接强度变化以及神经脉冲信号的时序关系变化</strong>上。（神经网络的连接强度是个什么东西？就是神经网络的权重，所以在脑中突触连接的权重是在动态变化的，但目前的神经网络一旦学好，权重都是固定的）</p>
<h5 id="大脑的结构连接"><a href="#大脑的结构连接" class="headerlink" title="大脑的结构连接"></a>大脑的结构连接</h5><p>通过对猫科动物和猕猴的大脑皮层解剖发现，大脑的结构网络具有 “Small world” 的特性。大脑连接的形成方式和连接长度受限于生物材料和能量代谢的约束，形成了占大量比重的<strong>短距离连接</strong>(低成本)以及丰富的中央枢纽结构(适应性)。（这个短距离连接就是 CNN 用卷积来建模背后的机理吧，这个中央枢纽结构是啥？）</p>
<h5 id="大脑的功能连接"><a href="#大脑的功能连接" class="headerlink" title="大脑的功能连接"></a>大脑的功能连接</h5><p>大脑皮层的功能连接常用来分析识别大脑特定的任务和功能(Task-Specific)</p>
<p>功能连接是和特定的任务相关联的，例如:通过对脸盲症患者的实验发现，人眼看到运动的人物时，<strong>大脑是通过两条不同的神经传输路径分别来提取人物身份和判断运动位姿</strong>(功能连接)（两条不同的神经传输路径，这个已经在很多论文里提到了）</p>
<p>高级的认知任务中则表现出了较多的模块间的互连度</p>
<p>在大脑处理新任务时，位于额顶叶中的<strong>中央枢纽灵活地在各个专门任务处理区域间进行多项快速的连接切换</strong>，中央枢纽网络的存在使得人可以处理新的认知任务，并增强人的学习能力和适应性</p>
<p><strong>大脑的功能与其结构存在着对应关系</strong>。这种关系有别于基于符号和概率的知识表达，<strong>大脑通过复杂的时、空动态演化的网络系统来完成信息的判断和推理</strong>。对于这样一种可塑的、<strong>动态的非线性</strong>关系网络， 目前，我们无法使用形式化的方法进行完整描述，更无法简单地利用传统的基于数值的计算模型来实现。（动态的非线性，难怪混沌、复杂网络会跟神经网络扯到一起去）</p>
<h4 id="大脑的记忆"><a href="#大脑的记忆" class="headerlink" title="大脑的记忆"></a>大脑的记忆</h4><p>大脑首先从感知觉系统的外部或者内部感受器中收集内外部的信息，然后<strong>利用神经系统中记忆的知识对收集的信息进行解释和判断</strong>（解释和判断必须要依赖记忆，更好地建模记忆才能够更好的构建场景理解的模型）。</p>
<p>由于信号不可避免的带有噪声，而且通常观察也是不完全的，因此，在神经系统的各个水平上都必须借助记忆完成对接受的信号的修正和完整化（记忆能够帮助修正和补全不完全观测，这对于目标检测、识别、跟踪里面的遮挡、噪声等问题可以很好的解决途径）。</p>
<p>同样的，为了形成适应性的行为决策，神经系统必须能够对环境变化的 “历史” 形成内部模型，这个作为决策依据的模型也是由记忆提供的。</p>
<p>机械记忆和生物记忆是两类主要的记忆形式，分别以计算机中对于数据的存储和高等动物脑中的记忆为代表，不同于机械记忆，生物记忆有如下几个特点:</p>
<p>首先，生物记忆的介质是生物神经系统，神经元是神经系统的基本组成单位。神经生物学实验表明，<strong>神经系统主要通过改变多个神经元之间的突触联接强度而记忆信息</strong>，并通过多个相关神经元状态的集体变 化表示不同的信息。因此，生物记忆的第一个特点是分布式记忆，这与现代计算机利用一个或几个相邻 字节表示一个单位信息的所谓局部性方式有很大不同。（意思是神经系统的记忆也是在突触连接强度上，那神经网络学到的知识也是在权重上，不就一样了么。说神经网络是黑箱没有可解释性，那问题来了，人脑有可解释性吗？）</p>
<p>其次，在生物记忆的回忆过程中，<strong>输入的信息与回忆出来的信息必定有某种关联</strong>，或者前者是后者的一部分，或者两者在内容上相似或有联系(如正好相反)，或者两者在环境中同时出现(即空间相关)或 相继出现(即时间相关)。早在两千多年前，亚里士多德就提出记忆的输入信息和回忆出的信息之间具有关联性，他把这种现象总结为联想律(Principle of Association)。因此，人们通常把人类或高等动物 的记忆称为联想记忆。输入信息与读取信息的关联性是生物记忆的重要特点，而在计算机中，信息在介质中存储具有确定的地址。（一个是某种关联，计算机是就是等同，这点对我们构建算法会有什么启示？想不到诶）</p>
<p>生物记忆的第三个特点是<strong>动态性</strong>，在人类的联想记忆中，不只是由一个输入项联想出一个相关联的记忆项，人们能够<strong>记忆和回忆一个结构化的序列</strong>，人的回忆是一个具有丰富动态特点的过程。形成鲜明对比是，计算机利用一个地址读取一个信息，是一种机械单调的过程。（回忆其实就是一个<strong>重建</strong>的过程，而不是简单的读取）</p>
<p>在生物神经系统中，记忆与信息的处理过程是缠绕在一起的，不像计算机系统那样，信息存取的过程与计算过程是相对分离的</p>
<h5 id="神经记忆的特征"><a href="#神经记忆的特征" class="headerlink" title="神经记忆的特征"></a>神经记忆的特征</h5><ol>
<li>分布式表达和存储</li>
<li>输入信息与检索记忆在内容上具有关联性</li>
<li>存储和记忆检索具有动态性</li>
<li>记忆与信息处理过程紧密结合</li>
</ol>
<p>我对类脑智能感兴趣，主要是我想通过借鉴 <strong>在复杂的认知行为中，大脑功能网络如何有效的合作、竞争以及协调工作的</strong>，从而来指导我对于场景理解算法的构建。</p>
<h5 id="人脑强悍的计算能力"><a href="#人脑强悍的计算能力" class="headerlink" title="人脑强悍的计算能力"></a>人脑强悍的计算能力</h5><p>类脑计算需要完成高性能计算到高智能计算的进阶，计算能力的度量由每秒完成的浮点数操作 (Floating-point Operations Per Second，FLOPS)变化为每秒完成的突触操作(Synaptic Operations Per Second，SOPS)。人类大脑约有 10^11 的神经元，其中每个神经元有约 10^4 的突触连接，如果以 10Hz 的速度释放神经脉冲，其计算量约为 10^16 次突触操作(SOPS)，假设每次神经脉冲操作需要 10^2 次数值计算，则共需要具有 10^18 （Quintillion，百亿亿）次运算能力的高性能计算机(High Performance Computer，HPC)才能匹配整个大脑突触操作的次数。目前最快的高性能计算机天河 - 2 的计算能力为 33.86~54.90 PFLOPS（one quadrillion floating point operations per second，每秒千万亿，10^15）。而具有 10^18 浮点计算能力的机器预期在 2019-2023 年才能出现。（也就是说 10^15 到 10^18 还差三个数量级。）</p>
<p>10^3，Thousand，千<br>10^6，Million，百万<br>10^9，Billion，十亿<br>10^12，Trillion，万亿<br>10^15，Quadrillion，千万亿<br>10^18，Quintillion，百亿亿</p>
<h5 id="三种类脑-受脑启发的-认知计算模型"><a href="#三种类脑-受脑启发的-认知计算模型" class="headerlink" title="三种类脑(受脑启发的)认知计算模型"></a>三种类脑(受脑启发的)认知计算模型</h5><p>(1)基于生物学的脑认知网络计算模型(图 5a)，代表性的工作有瑞士联邦理工的马克哈姆教授发起 的欧盟 HBP 项目;<br>(2)基于数据驱动的脑认知计算模型(图 5b)，设计各种巧妙的激励测试实验，通过如核磁共振、脑 电图等神经成像技术获得有限的实验数据，并对测量数据加以分析归纳;<br>(3)基于数学和人工神经网络的脑认知计算模型(5c)，使用数学分析和计算机模拟的方法对生物实 验观察数据和测试结果进行研究，提出大脑信息加工的生物学假设、提炼出相应的数学和计算模型，发 展出了相应的计算神经理论和计算方法。</p>
<p>这块我没看懂，不知道 数据驱动 与 数学和人工神经网络 有什么区别？</p>
<p>已有类脑计算架构设计者大多是来自计算机相关专业的专家和学者，往往受人工智能神经网络设计思路的影响，<strong>集中在寻找合适的特征来描述外部世界的复杂性和不变性，而忽略了从神经网络内部信息表达模态不变性</strong>的角度分析和设计类脑计算系统的研究方法。</p>
<h4 id="视觉计算"><a href="#视觉计算" class="headerlink" title="视觉计算"></a>视觉计算</h4><p>视觉信息中存在着大量的无关甚至使人误解的偏差，并且视觉信息数据本身不会显现出相应的相关性和不变性</p>
<p>用机器来求解视觉场景理解的问题时，需要回答:在物理学和光学的基础上，对感知的景物图像必须完成哪些处理?<strong>如何表示和利用客观世界模型、知识以及选择性注意机制?</strong></p>
<p><strong>选择人类视觉处理机制的典型应用为出发点</strong>和突破口，<strong>尝试构建类似大脑的视觉信息处理模型</strong>及架构（我想做的点），对促进类脑计算的深入研究具有重要的指导意义。</p>
<p>作者在从事计算机视觉的研究工作中，始终思考着这样一个问题: <strong>怎样利用知识，将大脑的某些视觉感知功能赋予机器</strong>，即:</p>
<ol>
<li>如何实现初级视觉中不同层次和水平的自然衔接，使视觉系统自动将信息组织成具有连续性的结构?</li>
<li><strong>认知的基本单元是什么?是否存在统一的方式处理不同视觉模块灰度、纹理、形状、颜色、表面深度和运动的组织信息?</strong></li>
<li><strong>选择性注意力机制是怎样在大脑的初级视觉信息处理中产生作用的?</strong></li>
<li>如何将这个组织原则映射到物理可实现的高度并行的 “类脑” 计算结构中?</li>
</ol>
<p>视觉认知计算可以作为类脑计算的一个突破点（由此可以看到，视觉认知计算是 类脑计算下面的一个子领域，目的在于解决视觉问题）</p>
<p>信号经由感受器(视杆和视锥细胞)-&gt; 双极细胞(第一级神经元)-&gt; 节细胞(第二级神经元)-&gt; 视神经 -&gt; 视交叉 -&gt; 视束 -&gt; 外侧膝状体 (第三级神经元)-&gt; 视辐射 -&gt; 内囊枕部 -&gt; 枕叶视区的传导途径到达大脑皮层，形成视觉</p>
<p>研究计算视觉，我们必须知晓: 视觉不是孤立地起作用，而是复杂的行为系统的一部分;其次，<strong>视觉计算是动 态的，通常并不需要一次将所有的问题都计算清楚，而是对所需要的信息加以计算</strong>（Attention）;第三，视觉计算应该是自适应的，视觉系统的特性应该随着与外界的交互而变化。同时，<strong>初级视觉中的全局和局部感知同样存在着交互行为，小尺度和大尺度感知是并行的、相互作用的</strong>（局部和全局是有交互且相互作用的，小尺度和大尺度也是并行且相互作用的，关键就在于怎么建模这种相互作用）。生物视觉具有小范围竞争、大范围协作的特点</p>
<h5 id="视觉交互行为与注意力集中"><a href="#视觉交互行为与注意力集中" class="headerlink" title="视觉交互行为与注意力集中"></a>视觉交互行为与注意力集中</h5><p>视觉认知过程不只是被动地对环境的响应，同时也是一种主动行为: 人们在环境信息的刺激下，通过眼动、走动，改变观察点，从动态的信息流中抽取不变性，在交互作用下产生知觉 (主动视觉系统)。人脑在视觉认知过程中存在自下而上和自上而下的双向信息处理通道。生物视觉通道使用自下而上的传递过程(200ms-300ms)对视觉对象形成初步认知结果(100 步法则)。通过自上而下的反向传递控制眼球的注意力，完成<strong>预测 - 验证的认知过程</strong>。<strong>人具有从复杂环境中搜索特定目标，并对目标信息进行选择处理的能力</strong>。这种搜索与选择的过程被称为注意力集中(Focus attention)。比如，大脑通过控制 眼球的肌肉，完成注意区域的聚焦，在眼动过程中的信息则是被忽略的。人们对于注视点周围的物体可 以精确地反应出其颜色、形状、深度信息，而对于处于视野边缘的物体，则很难分辨清楚它的颜色、形状和距离。这就是信息表达的不完整性。选择注意机制可分为<strong>独立于内容和语义的初级(Low-level)注意系统</strong>和<strong>基于内容和语义的高级(High-level)注意系统</strong>两个层次。</p>
<h5 id="深度学习的问题"><a href="#深度学习的问题" class="headerlink" title="深度学习的问题"></a>深度学习的问题</h5><ol>
<li>缺乏理论支持(如:面向不同复杂度的任务需要设计多少隐层?如何消除海量存在的冗余参数?何种网络连接为最优结构?)。因此其很难对效果超群的深度学习算法在具体问题上给出恰当的理论解释。</li>
<li>大规模神经网络容易过拟合数据，只有采集到充分大的标注且数据维度足够高时，有了大数据样本 才能缓解复杂模型的过度学习。因此深度学习性能依赖于海量的学习样本以及样本的质量，<strong>在小样本数据下无法获得有效的知识(概念)</strong>。</li>
<li>目前的深度学习方法，还是停留在统计学习和复杂模式识别与分类层面上，比起人的学习能力还有很多局限。比如，<strong>人的举一反三、触类旁通、无师自通所展现出的知识迁移的学习能力是现有统计学习所远远不能达到的</strong>。（这是统计学习的缺陷，不光是深度学习的缺陷）</li>
</ol>
<p>深度模型与知识的融合，外部记忆的增强，深度学习与贝叶斯学习推理的结合应该是其未来的研究方向。</p>
<h5 id="视觉认知中的深度学习层次结构"><a href="#视觉认知中的深度学习层次结构" class="headerlink" title="视觉认知中的深度学习层次结构"></a>视觉认知中的深度学习层次结构</h5><p>我就需要这一小节的内容来指导工作，这是个典型范例。</p>
<p>在视觉认知计算中，对深度学习层级结构的理解要避免走入一个误区:层级结构最顶层的输出是认知编码的目的。 <strong>实际上人对视觉刺激的认知编码的结果是整个层级结构，而不只是层级结构最顶层的输出</strong>（这个不就是 skip-layer 动机么）。 目前的深度学习和计算z机视觉只需要识别出图像中的对象，这种认知是面向对象的。人脑不仅能识别出输入图像中的对象，还能<strong>在一定程度上识别出构成这些场景和对象的细节</strong>(虽然不是像素级的细节)。 也就是说，<strong>在大脑层级编码模型中，底层的作用不仅是为了最终得到最顶层，而每一层本身就是对图像的部分编码</strong>。</p>
<p>另外，一种观点认为高级视觉认知就是对象认知，这种理解容易对视觉认知机制产生混淆和误导。比如 啮齿动物，它们并不需要识别出什么是建筑、什么是草坪、什么是公路，它们的高级视觉认知主要在于 复杂环境中的导航，比如快速识别出哪里可以逃跑，哪里存在障碍等 [27]。人脑认为草坪和道路作为两 个对象，其界线非常明显，而啮齿动物的高级视觉认知可能并不会对视觉场景做这样的划分。因此，构 造一个能很好的识别 “对象” 的算法只是解决 “眼前” 的问题。但是，对象识别只是人脑适应环境的结果， 仍然不是最根本的视觉认知机制。</p>
<h5 id="现有视觉计算架构的局限"><a href="#现有视觉计算架构的局限" class="headerlink" title="现有视觉计算架构的局限"></a>现有视觉计算架构的局限</h5><p>在初级特 征获取之前，大量未加工的、冗余的数据需要进行传输或者计算，从而消耗了大量的通讯带宽和计算资源。（这就是老大那篇 Paper 和 后续基金的 Motivation）</p>
<h5 id="脑启发的视觉处理计算架构"><a href="#脑启发的视觉处理计算架构" class="headerlink" title="脑启发的视觉处理计算架构"></a>脑启发的视觉处理计算架构</h5><p>视觉通道特别是视网膜的信息处理能力、大脑神经连接的网络化结构以及联想记忆启发我们设计和研究 新型的视觉计算模型和处理架构。这种架构的组成单元有:<strong>从帧驱动到事件驱动的信息获取单元(智能计算前移)、注意力选择 / 事件驱动的信息获取方式</strong>、时空动态的信息编码、网络化分布式的动态信息处理、结合长时和短时记忆功能的网络结构，以及条件要素的约束和引导的有效控制。实现大脑结构网 络、功能网络和有效网络在视觉处理架构不同层次的映射。</p>
<p>学习和记忆的基本过程是:信息获取、选择、巩固和再现。信息获 取是感知器官向大脑输入信号的阶段，注意力在信息的获取阶段影响很大。选择和巩固是信息在脑内进 行简单处理、决定是否需要保持和进一步强化形成长时记忆的阶段，其巩固程度和信息对于个体的意义 以及是否重复出现有关(增加曝光度会增加熟悉度和确定性，但不清楚是否影响记忆)。再现也即回 忆，是将脑中存储的长时记忆信息提取再现于意识，从而利用经验知识信息完成高层次的信息加工处理 的过程。</p>
<h3 id="28-机载激光雷达技术在长江三峡工程库区滑坡灾害调查和监测中的应用研究"><a href="#28-机载激光雷达技术在长江三峡工程库区滑坡灾害调查和监测中的应用研究" class="headerlink" title="[28] 机载激光雷达技术在长江三峡工程库区滑坡灾害调查和监测中的应用研究"></a>[28] 机载激光雷达技术在长江三峡工程库区滑坡灾害调查和监测中的应用研究</h3><p>里面给了用 LiDAR 的一个案例，具体是给定了 3 个点，计算这 3 个点在东西、南北方向上的距离，比较在前后两次测量时的距离，这个案例中这三个点，在南北方向上的距离的变化很小，但东西方向上的距离变化很大。 在这个案例中，可以看到是用特征点的分布（互相之间的距离）变化来检测滑坡的。</p>
<h3 id="29-基于地面-LiDAR-的滑坡地表整体变形监测"><a href="#29-基于地面-LiDAR-的滑坡地表整体变形监测" class="headerlink" title="[29] 基于地面 LiDAR 的滑坡地表整体变形监测"></a>[29] 基于地面 LiDAR 的滑坡地表整体变形监测</h3><p>目前，对于滑坡体的变形监测已有多种方式和手段。最多见的是在滑坡体关键部位布设人工监测点构成监测网，采用全站仪或 GPS 获取监测点的坐标值。 这样的方法能够得到精确的点位坐标，能够灵敏地捕捉到点位的变化信息，获取水平位移、垂直位移及变化速率。</p>
<h4 id="为什么要做点云匹配？"><a href="#为什么要做点云匹配？" class="headerlink" title="为什么要做点云匹配？"></a>为什么要做点云匹配？</h4><p>采集点云数据的时候，往往<strong>由于单幅数据无法覆盖整个区域</strong>，需要进行多次扫描或设置多个扫描测站得到多幅 点云数据。<strong>每次扫描基于不同的坐标系统</strong>，为了完整地重构地表形状，需将所有基于不同视角获取的点云<strong>转换至统一的坐标系下</strong>，这个过程就称为<strong>点云配准</strong>。</p>
<p>点云配准包括两个步骤: 对包含重叠区域的两幅点云采用两视配准的方法在重叠区域选取3至5对同名点进行<strong>初步配准</strong>; 采用最近邻点迭代算法(Iterative Closest Points， ICP)进行<strong>精确配准</strong>, 通过迭代获得均方差最小的最优化配准结果。（这是对同一期有重叠但不同区域的匹配）</p>
<p>对于<strong>多期</strong>的点云模型进行比对时，选取数据采集时<strong>多余观测测的稳定区域作为坐标基准</strong>（这个怎么选？怎么知道哪些是 稳定区域？），采用点云配准的方法使用多期稳定区域间的空间变换关系<strong>将多期点云统一到同一坐标系</strong>，无需额外设置固定扫描站。</p>
<h3 id="30-SuperPoint-Self-Supervised-Interest-Point-Detection-and-Description"><a href="#30-SuperPoint-Self-Supervised-Interest-Point-Detection-and-Description" class="headerlink" title="[30] SuperPoint: Self-Supervised Interest Point Detection and Description"></a>[30] SuperPoint: Self-Supervised Interest Point Detection and Description</h3><p>这是我看的第一篇特征点检测的文章…</p>
<p>这篇文章是 Magic Leap 的。</p>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>文章的主要贡献给出了一种不需要 human annotation，以 Self-Supervised 训练的 fully-convolutional CNN 来同时学 interest point detectors and descriptors。</p>
<p>Self-Supervised 是以 From Simple to Complex 的方式实现的，之所以这么做，在于作者相信能够 transfer knowledge from a synthetic dataset onto real-world images，具体步骤如下图所示：</p>
<ol>
<li>首先是 pre-train an initial interest point detector on synthetic data（这是 From Simple to Complex 的 Simple 阶段），这个阶段得到的 Detector 叫作 MagicPoint；MagicPoint 在 synthetic data 上表现比传统的特征点检测子要来得好，但对于 real images，相比于经典检测子还是会 misses many potential interest point locations. 后面这个能力会通过 Homographic Adaptation 这个 multi-scale, multi-transform technique 来补全（意思就是说作者认为，synthetic data 相比于 real images，缺少 multi-scale, multi-transform，所以 MagicPoint 缺的是在 multi-scale, multi-transform 下的鲁棒性？） </li>
<li>第二步是 Interest Point Self-Labeling，现在 unlabeled image（这是 real images，不是 synthetic data，这是 From Simple to Complex 的 Complex 阶段）上检测出特征点，然后再运用 Homographic Adaptation，就可以得到单应性变换后的特征点。<strong>Homographic Adaptation 的核心</strong>在于 <strong>刻画 repeatability</strong>，最后 detectors and descriptors 的 loss 其实惩罚的就是在 repeatability 上表现不好的，而非检测的特征点与 Human Annotated 特征点之间的 Loss。也就是说，<strong>之所以能够 Self-Supervised，在于 Loss 由刻画 Prediction 与 Groundtruth 之间的差距，变成了刻画 Prediction 在 Homographic Adaptation 下的 repeatability，也就是 Homographic Adaptation 前后的差距。</strong></li>
<li>最后就是 Joint Training，根据 Loss Function 用 ADAM 优化即可，在优化 Detector 的同时 Description 也学了，Description 就是 CNN 最后的特征表示。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Overview.png" alt=""></p>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p>网络结构就是 VGG 的，但因为 SuperPoint 是同时做 interest point detection 和 interest point description，这两个 task 是同时的，共同 share 一部分 computation（传统方式是先做 point Detection，完后了再做 point description），SuperPoint 之所以可以说是同时做，是因为它 Detection 和 description 两个 sub-network 的输出都是 W * H，也就是输入大小，所以 SuperPoint 是做了一个 Dense Output。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Decoders.png" alt=""></p>
<p>upsampling 没有学习，直接用的插值</p>
<p>对于 Interest Point Decoder，Softmax 之前的特征的尺寸是 $H <em> { c } \times W </em> { c } \times 65$，65 是因为 8 <em> 8 下采样一个 Cell 里面对应原来 64 个 Pixel，再加上一个额外的 “no interest point” dustbin，一共 65 个。原图是 W </em> H，现在变成了 W/8 <em> H/8 </em> 64，所以变成 W * H 的大小只要 reshape 就可以了</p>
<p>在 Training 阶段，Homographies 是被用来作为提高模型 repeatability 的约束条件 / Loss 来用；在 Test 阶段，Homographies 被拿来当做集成学习多个弱分类器组合成强分类器那样的味道来做，只不过这个组合方式是简单的对在做了单应性变换后检测出来的特征点再反变换回原图的结果的相加，具体公式如下</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Eq%2010.png" alt=""></p>
<p>图像表示如下</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Fig%205.png" alt=""></p>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p>注意，Loss 是建立在 $H <em> { c } \times W </em> { c }$ 的 Feature map （8 * 8 下采样）上，而不是在最后跟原图一样大小的 Output 的。</p>
<p>总的 Loss Function 如下：</p>
<p>$$<br>\begin{array} { l } { \mathcal { L } \left( \mathcal { X } , \mathcal { X } ^ { \prime } , \mathcal { D } , \mathcal { D } ^ { \prime } ; Y , Y ^ { \prime } , S \right) = } \ { \mathcal { L } <em> { p } ( \mathcal { X } , Y ) + \mathcal { L } </em> { p } \left( \mathcal { X } ^ { \prime } , Y ^ { \prime } \right) + \lambda \mathcal { L } _ { d } \left( \mathcal { D } , \mathcal { D } ^ { \prime } , S \right) } \end{array}<br>$$</p>
<h5 id="interest-point-detector-loss"><a href="#interest-point-detector-loss" class="headerlink" title="interest point detector loss"></a>interest point detector loss</h5><p>interest point detector loss 具体如下</p>
<p>$$<br>\mathcal { L } <em> { p } ( \mathcal { X } , Y ) = \frac { 1 } { H </em> { c } W <em> { c } } \sum </em> { h = 1 \atop w = 1 } ^ { H <em> { c } , W </em> { c } } l <em> { p } \left( \mathbf { x } </em> { h w } ; y _ { h w } \right)<br>$$</p>
<p>其中 </p>
<p>$$<br>l <em> { p } \left( \mathbf { x } </em> { h w } ; y \right) = - \log \left( \frac { \exp \left( \mathbf { x } <em> { h w y } \right) } { \sum </em> { k = 1 } ^ { 65 } \exp \left( \mathbf { x } _ { h w k } \right) } \right)<br>$$</p>
<p>Y 就是 MagicPoint 在原图上检测出的兴趣点，作为 pseudo groundtruth interest point，X 则是 SuperPoint 网络的 Detector 在原图上检测出的兴趣点，在第一次迭代的时候，X 和 Y 应该是一样的，因为 MagicPoint 就是没有 descriptor head 的 SuperPoint，但随着 SuperPoint 的迭代优化，X 是会发生变化的。<strong><em>这个时候这个 Loss 会去惩罚跟 Y 不一样的 X，也许 X 检出了更多的点呢，这一点合理吗？</em></strong> </p>
<p>虽然 MagicPoint 会 misses many potential interest point locations，但已经算是 performs surprising well on real images 了。所以 MagicPoint 检出的点作为 pseudo groundtruth，后面由 MagicPoint -&gt; SuperPoint，不再努力检测出更多的点，而是致力于 boost repeatability（检测出尽量多的点 与 检测出的点在 large viewpoint changes 还具有很好的 repeatability 这是两个性质。）</p>
<p>MagicPoint performs reasonably well on real world images 但还不够好，相比于其他经典检测子，但是 Homographic Adaptation 也就是  self-supervised approach for training on real-world images 提高了性能（提高性能不一定非要从减小 Prediction 和 Groundtruth Annotation 的任务中来，也可以是构建其他 Loss，这就是 Self-Supervised 的精髓吧 ）</p>
<h5 id="descriptor-loss"><a href="#descriptor-loss" class="headerlink" title="descriptor loss"></a>descriptor loss</h5><p>对于 Description 的 Loss，监督信息是通过 Homographic Adaptation 来实现的，从而完成了的监督学习, 具体计算如下，</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Eq%205.png" alt=""></p>
<p>其中 $S$ 是一个指示矩阵，指示第 $hw$ 个 cell 是否与 $h’w’$ 个 cell 是 corresponding 的。怎么判断两者是否 corresponding 依据下面这个公式，原图上的特征点做单应性变换后 与 原图做单应性变换检测出的特征点（先检测后变换，还是先变换后检测的区别），如果一个落在另一个的 邻域内，就算是 corresponding 的。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/SuperPoint%20Eq%204.png" alt=""></p>
<p>原图上的特征点做单应性变换后 与 原图做单应性变换检测出的特征点如果在完美的检测和单应性变换下应该是一致的，形式化表示如下。上面的公式其实就是把下面等式两边的两项差距小于 8 的都认为满足这个约束，是同一个点。其实下面这个式子是 <strong>repeatability 具体的形式化表示。</strong> repeatability 被刻画成了单应性约束。</p>
<p>$$<br>\mathcal { H } \mathbf { x } = f _ { \theta } ( \mathcal { H } ( I ) )<br>$$</p>
<p>有了 corresponding 关系后，两者之间的 Loss 就可以如下算出来</p>
<p>$$<br>\begin{aligned} l <em> { d } \left( \mathbf { d } , \mathbf { d } ^ { \prime } ; s \right) &amp; = \lambda </em> { d } <em> s </em> \max \left( 0 , m <em> { p } - \mathbf { d } ^ { T } \mathbf { d } ^ { \prime } \right) \ &amp; + ( 1 - s ) * \max \left( 0 , \mathbf { d } ^ { T } \mathbf { d } ^ { \prime } - m </em> { n } \right) \end{aligned}<br>$$</p>
<p>这项 Loss 容易理解。如果 s = 1，则表示这两个点是 corresponding 的，那么就是第一个惩罚项非零，惩罚两者的 Description 不够接近；如果 s = 0，则表示这两点是不 corresponding 的，那么就惩罚第二项，惩罚两者的 Description 接近了。其中，$m_p = 1, m_n = 0.2$</p>
<h3 id="32-Single-Shot-Object-Detection-with-Enriched-Semantics"><a href="#32-Single-Shot-Object-Detection-with-Enriched-Semantics" class="headerlink" title="[32] Single-Shot Object Detection with Enriched Semantics"></a>[32] Single-Shot Object Detection with Enriched Semantics</h3><p>CVPR 2018 的文章。</p>
<h4 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h4><p>这篇文章的 Motivation 是 SSD 算法对小目标的检测只是利用了浅层的特征，缺少高层语义，如果能让浅层特征能够有更多的语义，就可以改善小目标检测的性能。这篇文章给出了一种做 Semantic Enrichment 的方式。</p>
<p>语义可以简单地看做是数据所对应的现实世界中的事物所代表的概念的含义，以及这些含义之间的关系，是数据在某个领域上的解释和逻辑表示。</p>
<p>这个 Semantic Enrichment 是通过把一个高层语义分割任务最后的 Feature Map 作为 Attention 权重加到 Detection 的 低层特征上去，这种 Semantic Enrichment 具体的操作方式就是 spatial attention mechanism，通过 element-wise multiplication 上 Spatial Attention Weight Map，直接对最底层的 Detection Layer 的 Feature Map（conv4_3）来 suppress or emphasize 用于检测的低层 feature maps。Semantic Enrichment 具体的表现，用 BAM 论文的话来说则是：denoises low-level features such as background texture features at the early stage；focuses on the exact target which is a high-level semantic （Spatial 维度上）. </p>
<p>问题在于这个 Attention Map 怎么来的？作者是通过一个 Semantic Segmentation 分支得到的，只不过这里的标注有点粗略，就是把 BBox 里面的所有像素都标注成了那个 BBox 对应的 label。</p>
<p>下图把这种通过 weighted 上 Attention Map 来提高特征语义的思路展示得很清楚，A 是原图；B 应该是低层的 Detection Layer 对应的低层特征，上下两行分别对应的是 狗 对应类别的特征图 和 人 对应类别的特征图，可以看到不管是狗还是人的特征图上，都有大量无关的特征显著存在；C 是两者对应的由 Semantic Segmentation 得来的 Attention weight map；D 就是 B 和 C 做了 element-wise multiplication 之后的结果，可以看到对于每一类的无关特征都被压制的干净了。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/DES%20Fig%202.png" alt=""></p>
<h4 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h4><p>文章的整个网络架构如下图所示，由 Detection Module、Segmentation Module 和 Global Activation Module 三部分组成。其实 Detection Module 就是 SSD，Segmentation Module 就是 FCN（用 dilated convolution 的，为了保持大小不变），Global Activation Module 就是 Squeeze-and-Excitation Block。</p>
<p>使用 SE Block 的目的是不仅仅要在 Spatial 维度上 提升有用的特征并抑制对当前任务用处不大的特征，在 Channel 维度上也这么做。因此这篇论文和 BAM 和 CBAM 一样都是同时做了 Spatial 和 Channel Attention 的文章。文章中的说法是用于提高 high level 的 feature map 的语义信息，我理解的提高 feature map 的语义信息的方式还是通过 attention weight 抑制干扰特征来实现的。 SE Block 的思想是在 Channel 维度上解耦，让某些 channel 就对应某些 image class；这篇文章的思想也是如此，只不过是 channel 对应某些 object class。这篇文章里的 SE Block 也就是 global activate module 的作用是学习 channel 和 object class 的关系，以提高效果。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/DES%20Fig%201.png" alt=""></p>
<h5 id="Segmentation-branch"><a href="#Segmentation-branch" class="headerlink" title="Segmentation branch"></a>Segmentation branch</h5><p>需要一提的是，在这篇文章的 Semantic Branch 的 Output 有两个， 这是因为一个 Output 通道数是要等于类别数，这是为了计算 Loss；还有另外一个 Output 是为了生成 Attention Weight，通道数必须和 Detection Branch 的 Feature Maps 通道数一样。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/DES%20Fig%203.png" alt=""></p>
<h4 id="Loss-1"><a href="#Loss-1" class="headerlink" title="Loss"></a>Loss</h4><p>Loss 就是正常的 Object Detection 和 Semantic Segmentation 的 loss 之和，只不过 Semantic Segmentation 里的 Groundtruth Truth 就是 BBox 的范围，但是计算还是和一般的 Semantic Segmentation 是一样的。</p>
<h3 id="33-Residual-Attention-Network-for-Image-Classification"><a href="#33-Residual-Attention-Network-for-Image-Classification" class="headerlink" title="[33] Residual Attention Network for Image Classification"></a>[33] Residual Attention Network for Image Classification</h3><p>CVPR 2017 的文章，是比较早的一篇 Soft Attention 的工作。</p>
<h4 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h4><p>窃以为这篇文章的动机在于 bring more discriminative feature representation by the attention mechanism，具体地说是通过在 feedforward network structure 中 incorporates the soft attention 来 generate attention-aware features。这些 attention-aware features 质量更好，更有利于分类，因为这些特征 enhances different representations of objects at that location。为了实现这个目的，文章中给出了 Attention Module 的方式。但之间堆叠这些 Attention Module 会产生梯度消失问题，网络不能很深。为了解决这个问题，类似于 Residual Block，文章给出了一种 Attention Residual Learning 方式。因此，本文所提出的 Residual Attention Network 其实就是 Attention Module + Attention Residual Learning。</p>
<h4 id="Model-2"><a href="#Model-2" class="headerlink" title="Model"></a>Model</h4><p>Residual Attention Network = Attention Module + Attention Residual Learning</p>
<p>这篇文章的 Residual Attention Network 的架构如下 </p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Residual%20Attention%20Network%20Fig%202.png" alt=""></p>
<h5 id="Attention-Module"><a href="#Attention-Module" class="headerlink" title="Attention Module"></a>Attention Module</h5><p>Soft Attention 的方式就是学出一个权重分布，再拿这个权重分布施加在相应的特征之上。在目前我看过的 Attention 的论文里，比如 SENet、BAM、CBAM、DES，这个相应的特征就是计算 Attention 权重的输入。这是因为在 SENet 中，Attention Module 只是一个主要特征抽取模块之外 add-on 的模块，起到的作用是改进已经由主要模块抽取的特征的质量；而在本文中，本文的 Attention Module 是要成为像 Residual Block 那样的基础性模块，是用来抽取特征的，只不过抽取的是 attention-aware features 质量更好。因此，本文的 <strong>Attention Module = trunk branch + mask branch</strong>。 其中，trunk branch 负责往常的特征抽取，可以是 pre-activation Residual Unit, ResNeXt and Inception 中的任一种 state-of-the-art network structure。至于 Soft Mask Branch，如下图所示，是一个 hourglass 结构，encoder-decoder 结构。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Residual%20Attention%20Network%20Fig%203.png" alt=""></p>
<p><strong><em>但要注意的是，mask branch 和 trunk branch 的 receptive field 是不同的</em></strong></p>
<h5 id="Attention-Residual-Learning"><a href="#Attention-Residual-Learning" class="headerlink" title="Attention Residual Learning"></a>Attention Residual Learning</h5><p>在给定 trunk branch output $T(x)$ 和 mask branch output $M(x)$，通常按照一般的 Soft Attention 的方式，Attention Module $H$ 的输出会是</p>
<p>$$<br>H <em> { i , c } ( x ) = M </em> { i , c } ( x ) * T _ { i , c } ( x )<br>$$</p>
<p>$M(x)$ 的作用是 feature selectors，用来 enhance good features and suppress noises from trunk features. 然而，由于 mask 里的权重位于 0-1 之间，多个 Attention Module 堆叠后（网络变深），梯度就会消失，而网络深度加深是获取最后好性能的一大关键。文章给出了一种 attention residual learning 方式来解决这个问题，也就是把 Attention Module $H$ 的输出变成</p>
<p>$$<br>H _ { i , c } ( x ) = \left( 1 + M ( x ) \right) \cdot T ( x )<br>$$</p>
<p>注意，此 Residual 非彼 Residual。在 ResNet 中，Residual Learning 是 $H <em> { i , c } ( x ) = x + F </em> { i , c } ( x )$ 这样的。在同样 Soft Attention 的 BAM 和 CBAM 中也采用了 Residual Learning，但它们的 Residual 也是 ResNet 方式的标准的 Residual Learning 的方式，与本文不同。</p>
<h5 id="Spatial-Attention-and-Channel-Attention"><a href="#Spatial-Attention-and-Channel-Attention" class="headerlink" title="Spatial Attention and Channel Attention"></a>Spatial Attention and Channel Attention</h5><p>Attention 说白了就是一个 0 到 1 的权重，最后只要每个点的数值都在 0-1 之内就行，那这个权重具体怎么算出来呢？这就是公式（4）、（5）、（6）了。这三个公式分别对应着，是既对 Spatial 又对 Channel 做 Attention，还是只对 Channel 施加 Attention，后者只对对 Spatial 施加 Attention。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Residual%20Attention%20Network%20Eq%204%205%206.png" alt=""></p>
<p>这里有个概念要澄清一下。虽然都叫 Channel Attention，之前在 SENet、BAM、CBAM 中，我们说 Channel Attention 是 Channel-wise Attention，不同 Channel 不同，但同一个 Channel 内的所有 Spatial Position 都是同一个权重；因为做了 Global Average Pooling，这里整个特征的 Channel Attention Weight 是一个 Channel 数的<strong>向量</strong>。在<strong>这篇文章</strong>里的 Channel Attention 是说计算某个点（each spatial and channel position ）的时候，计算出来的<strong>权值仅与该点同个 Channel 上的其他点有关，与 Spatial 点无关</strong>；这里整个特征的 Channel Attention Weight 仍然是跟特征向量相同大小的张量，因为没有像 SENet 那样做 Global Average Pooling。</p>
<p>概括一下，在 SENet、BAM、CBAM 中的 Channel Attention 是只有 Channel 维度不一样，<strong>Spatial 维度所有点的权重都一样</strong>；而本文的 Channel Attention 是只在<strong>计算权重也就是归一化</strong>的时候考虑了 Channel 维度上的点，而没有考虑 Spatial 权重上的点，因此<strong>不同 Spatial 上点的权重还是不同的</strong>，因为他们各自 Channel 维度上的向量不同。</p>
<p>例如，公式（5）performs L2 normalization within all channels for each spatial position to remove spatial information. 这个的确是 remove spatial information 了，因为得到的权重只与一个 spatial position 上的所有点之间的相互大小有关，与其他 Spatial Point 无关；但需要一提的是，公式（5）的 channel Attention 得到的还是一个 $H \times W \times C$ 的张量。</p>
<p>最后的效果是 mixed attention，也就是既对 Spatial 又对 Channel 做 Attention 效果最好。这与我们的直觉也是相符的。</p>
<h4 id="Loss-2"><a href="#Loss-2" class="headerlink" title="Loss"></a>Loss</h4><p>这篇文章没有专门讲 Loss，但既然是分类，一般就是 cross entropy loss 吧。</p>
<h3 id="34-Semantic-Point-Detector"><a href="#34-Semantic-Point-Detector" class="headerlink" title="[34] Semantic Point Detector"></a>[34] Semantic Point Detector</h3><p>ACM Multimedia 2011 的文章。</p>
<h4 id="Motivation-3"><a href="#Motivation-3" class="headerlink" title="Motivation"></a>Motivation</h4><p>这篇文章的动机在于 local features （Feature point Detection + Feature Description）其实是图像内容的一种表示方式，在深度学习兴起之前的主流范式就是 SIFT 这些特征点检测器 + Bag of Visual Worlds 对局部特征编码 + SVM 分类。但是当前的 interest point detector 主要是为了图像匹配设计的，选取这些特征点的出发点是 invariant under a certain family of transformations，让 the correspondence establishment between images with the same object or scene 足够的robust，而非是针对描述图像内容、揭示语义信息来最优设计的。</p>
<p>这篇文章的 Motivation 就在于 propose a learning-based point detector called semantic point detector which aims to select a set of points that can better represent the image. 通俗地讲，就是 SIFT 这些特征点，会检测出图像中所有的特征点，包括我们不希望被检测出的背景或者其他类别中的特征点，而这篇文章的 semantic point detector 就是希望学习出一个仅会检测感兴趣类别物体上的兴趣点的兴趣点检测器。具体的效果如下图所示，第一排是传统检测子检测出的特征点，大量的特征点位于背景上；第二排是这篇文章的 semantic point detector 检出的，大多数特征点集中在前景目标上。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Semantic%20Point%20Detector%20Fig%202.png" alt=""></p>
<p>看到这里，会不由自主地产生一个疑问，<strong>单个局部的角点本身，也会具有语义吗？</strong>其实一个特征点不仅仅只是这个点本身，尤其是我们对特征点描述的时候，还包括了其周围的区域。具体到本文，与其说本文给出的是一个如其标题所说的 Semantic Point Detector，不如说实际上这篇文章做的是 <strong>Semantic Patch Detector</strong>。这篇文章具体的做法是将图像转化成 a set of 32x32 patches，步长是 4 个像素。论文中没说怎么在得到 Semantic Patch 后得到 Semantic Point，最直接的方式就是取中心点了。下图是一些 Semantic patches 的示例</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Semantic%20Point%20Detector%20Fig%203.png" alt=""></p>
<p>既然是 Semantic Point Detector，就要有 label 来赋予 Detector 语义信息。最直接的方式当然有现成的人肉 label 好的 Semantic interest point，但这个太不现实了。这篇文章采用的是一个 Weakly Supervised Learning 的方式。对于每一个类，可以根据是否还有该类物体，构造出相应的 a set of positive images 和 a set of negative images。基于 Background Patch 会在正类和负类中都出现，而 Semantic Patch 只会在正类中出现，以此来完成对于 Semantic Patch Detector 的学习。到这里，可以发现，本文要做的问题，很类似于一个 Weakly Supervised Object Detection 问题，区别在于，Object Detection 的 Object 大小会变动，而本文只要判断固定大小的 Patch 是 Positive 还是 Negative 即可。因此，本文实际上是一个 <strong>Weakly Supervised Patch Classification</strong> 问题。弱监督则是表现在，需要预测 Patch-level 的 label，但只给了 image-level 的 label。</p>
<p>Weakly Supervised Learning 的核心问题就在于怎么用 Weak label 上的 loss 来改善在具体任务上的 Prediction。这篇文章是怎么建立 Weak Label 和 Accurate Prediction 之间的关系的呢，具体地说，是怎么把 image-level classification 和 patch-level classification 给联系起来的呢？本文是用了两点，第一点是 image representation 是用所有 patch representation 的 <strong>linear combination</strong>，第二点是采用了 <strong>linear SVM</strong> 作为分类器。在给定 local patch descriptor $\phi \left( I _ { p } \right)$ 的情况下，image $I$ 的 Representation 就是 </p>
<p>$$<br>\Phi ( I ) = \frac { 1 } { P } \sum <em> { p = 1 } ^ { P } \phi \left( I </em> { p } \right)<br>$$</p>
<p>由此，图像 $I$ 在 linear SVM 下的 score $f ( I ) = w ^ { T } \Phi ( I )$ 也是所有 patch 在 linear SVM 下的 score 的 linear combination，具体如下式所示</p>
<p>$$<br>f ( I ) = w ^ { T } \Phi ( I ) = \frac { 1 } { P } \sum <em> { p = 1 } ^ { P } w ^ { T } \phi \left( I </em> { p } \right) = \frac { 1 } { P } \sum <em> { p = 1 } ^ { P } f \left( I </em> { p } \right)<br>$$</p>
<p>最后 patch-level classifier 的表示就是 $f \left( I <em> { p } \right) = w ^ { T } \phi \left( I </em> { p } \right)$。一言以蔽之，本文如何用 image-level classification 来指导 patch-level classification？本文的做法是，直接拿学到的 image-level classifier 作为 patch-level classifier 来使用，根本不是指导，而是自己亲下火线了。</p>
<h4 id="Model-3"><a href="#Model-3" class="headerlink" title="Model"></a>Model</h4><p>emmm… 好像 Model 上面说的差不多了，补充些细节。patch representation 用的是 Super-Vector coding，感兴趣的可以看原文的公式(1), (2), (3)。</p>
<p>对于 多类的情况，也很简单，相应的类别就是对应 score 最高的那个。</p>
<p>模型的示意图如下</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Semantic%20Point%20Detector%20Fig%201.png" alt=""></p>
<h4 id="Loss-3"><a href="#Loss-3" class="headerlink" title="Loss"></a>Loss</h4><p>Loss 就是正常的 SVM 的 hinge loss 啦，一个典型的正负类分类的 loss。</p>
<h3 id="35-图像物体分类与检测算法综述"><a href="#35-图像物体分类与检测算法综述" class="headerlink" title="[35] 图像物体分类与检测算法综述"></a>[35] 图像物体分类与检测算法综述</h3><ol>
<li>目的上：<strong>物体分类</strong>要回答的问题是这张图片中是否包含某类物体（我感觉是要回答包含哪类物体）；<strong>物体检测</strong>要回答的问题则是一张图像中在什么位置存在一个什么物体</li>
<li>手段上：<strong>物体分类</strong>算法通过手工特征或者特征学习方法<strong>对整个图像进行全局描述</strong>，然后使用分类器判断是否存在某类物体；除特征表达外，<strong>物体结构是物体检测任务不同于物体分类的最重要之处</strong></li>
<li>常用方法：物体分类方法多<strong>侧重于学习特征表达</strong>，典型的包括词包模型(Bag-of-Words)、深度学习模型；物体检测方法则<strong>侧重于结构学习</strong>，以形变部件模型为代表</li>
</ol>
<p>物体分类与检测研究存在的困难与挑战，下图概括的很好</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Huang%20Review%20Fig%201.png" alt=""></p>
<p>把物体分类与检测研究存在的困难与挑战分为了<strong>实例</strong>、<strong>类别</strong>和<strong>语义</strong>三个层次，这点我以前没有想到可以把难点扩为 3 个层次</p>
<ol>
<li>实例层次<ul>
<li>光照条件、拍摄视角、距离的不同、物体自身的非刚体形变以及其他物体的部分遮挡，使得物体<strong>实例的表观特征产生很大的变化</strong></li>
</ul>
</li>
<li>类别层次<ul>
<li>首先是<strong>类内差大</strong>，也即属于同一类的物体表观特征差别比较大，其原因有前面提到的各种实例层次的 变化，但<strong>这里更强调的是类内不同实例的差别</strong>（椅子形状千千万）</li>
<li>其次是<strong>类间模糊性</strong>，即不同类的物体实例具有一定的相似性（哈士奇和狼），特别是物体类别越多导致类间差越小</li>
<li>再次是<strong>背景的干扰</strong>，背景可能是非常复杂的、对我们感兴趣的物体存在干扰的</li>
</ul>
</li>
<li>语义层次<ul>
<li>多重稳定性，也就是图像的二义性（一张图像是年轻女人还是年老女人，是兔子还是鸭子）</li>
</ul>
</li>
</ol>
<p>主流物体分类与识别数据库下表概括的很好</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Huang%20Review%20Tab%201.png" alt=""></p>
<h4 id="基于词包模型的物体分类"><a href="#基于词包模型的物体分类" class="headerlink" title="基于词包模型的物体分类"></a>基于词包模型的物体分类</h4><p>作者这里把基于词包模型的物体分类分为了 <strong>底层特征、特征编码、空间约束、分类器设计、模型融合</strong>这几个层次，前三个跟 <a href="http://lowrank.science/Lazebnik-CVPR-06-SPM/">Notes on CVPR-06-Spatial Pyramid Matching</a> 里面总结的 local feature extraction、feature coding 和 feature pooling 三个是一样的，现在看来 feature pooling 叫作 Spatial Feature Pooling 更好一些。</p>
<p>下表对<strong>分类算法</strong>概括地很好：</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Huang%20Review%20Tab%202.png" alt=""></p>
<h5 id="Step-1-底层特征提取"><a href="#Step-1-底层特征提取" class="headerlink" title="Step 1: 底层特征提取"></a>Step 1: 底层特征提取</h5><p>底层特征提取方式有两种:一种 是基于兴趣点检测，另一种是采用密集提取的方式。</p>
<p>兴趣点检测算法通过某种准则选择具有明确定义的、局部纹理特征比较明显的像素点、边缘、角点、区块等，并且通常能够获得一定的几何不变性，从而可以在较小的开销下得到更有意义的表达，最常用的兴趣点检测算子有Haris 角点检测子、FAST 算子、LoG、DoG等.（注意，基于兴趣点检测的底层特征就是 像素点、边缘、角点、区块，这里的 兴趣点 是没有 descriptor 的，特征就是 兴趣点本身，而不是 descriptor）</p>
<p>密集提取的方式，从图像中按固定的步长、尺度提取出大量的<strong>局部特征描述</strong>，大量的局部描述尽管具有更高的冗余度，但信息更加丰富，后面再使用词包模型进行有效表达后通常可以得到比兴趣点检测更好的性能. 常用的局部特征包括SIFT、HOG、LBP等.（因为密集提取的方式提取的是 局部特征描述，而非仅仅点的位置，所以信息更加丰富）</p>
<p>为什么密集提取方式更好？在底层特征提取阶段，通过提取到大量的冗余特征，最大限度的对图像进行底层描述，防止丢失过多的有用信息，这些底层描述中的冗余信息主要靠后面的<strong>特征编码和特征汇聚得到抽象和简并</strong></p>
<p>为啥要特征学习？手工设计的底层特征描述子作为视觉信息处理的第一步，往往会<strong>过早地丢失有用的信息</strong>，直接从图像像素<strong>学习到任务相关的特征描述</strong>是比手工特征更为有效的手段</p>
<h5 id="Step-2-特征编码"><a href="#Step-2-特征编码" class="headerlink" title="Step 2: 特征编码"></a>Step 2: 特征编码</h5><p>这一块基于词包模型的物体分类算法的重点，绝大部分论文都是在这一块做工作。</p>
<p>一个很自然的问题，<strong>既然底层特征提取环节已经抽取了特征，直接用于后面的空间特征汇聚不好吗？为什么还要做特征编码？这个编码在编码什么？</strong> </p>
<p>论文中的说法是，密集提取的底层特征中包含了大量的冗余与噪声，为提高特征表达的鲁棒性，需要使用一种特征变换算法对底层特征进行编码，从而获得更具区分性、更加鲁棒的特征表达，这一步对物体识别的性能具有至关重要的作用，因而大量的研究工作都集中在寻找更加强大的特征编码方法。</p>
<p>但我觉得，真正的原因在于，底层特征，比如 SIFT， 虽然有 descriptor，但是这些 <strong>descriptor 只能用来计算两两相似度的，是无法用于空间特征汇聚环节的汇聚操作的</strong>（依据特征的什么性质汇聚？怎么汇聚？）。因此，肯定要有一个特征变换环节，由原来的底层特征变为空间特征汇聚环节能够使用的特征，这个环节就是特征编码环节。而且空间特征汇聚是对于图像分类必须的，因为底层特征提取到的是一个由元素数量不同的集合，不能直接输入需要固定长度向量输入的 SVM。因此，空间特征汇聚不可避免，由此，特征编码也是必须的。</p>
<h6 id="向量量化编码"><a href="#向量量化编码" class="headerlink" title="向量量化编码"></a>向量量化编码</h6><p>使用一个较小的特征集合(视觉词 典)来对底层特征进行描述，达到<strong>特征压缩</strong>的目的</p>
<p>向量量化编码只在最近的视觉单词上响应为1，因而又称为硬量化编码、硬投票编码，这意味着向量量化编码只能<strong>对局部特征进行很粗糙的重构</strong></p>
<h6 id="软量化编码"><a href="#软量化编码" class="headerlink" title="软量化编码"></a>软量化编码</h6><p>为什么要软量化？硬量化编码的缺点是什么？</p>
<p>图像局部特征常常存在一定的模糊性，即<strong>一个局部特征可能和多个视觉单词差别很小</strong>，这个时候若使用向量量化编码 将只利用距离最近的视觉单词，而<strong>忽略了其他相似性很高的视觉单词</strong></p>
<p>软量化编码(又称核视觉词典编码)算法，局部特征不再使用一个视觉单词描述，而是由距离最近的 K 个视觉单词加权后进行描述，有效解决了视觉单词的模糊性问题</p>
<h6 id="稀疏编码"><a href="#稀疏编码" class="headerlink" title="稀疏编码"></a>稀疏编码</h6><p>将稀疏编码应用到物体分类领域，替代了之前的向量量化编码和软量化编码，得到<strong>一个高维的高度稀疏的特征表达，大大提高了特征表达的线性可分性</strong>，仅仅使用线性分类器就得到了当时最好的物体分类结果，将物体分类的研究推向了一个新的高度上（关于 稀疏表示分类的文章太多了，关键在于 高维的高度稀疏的特征表达导致了 线性可分性）</p>
<p>这里岔开去一点，找 Bag of visual words 的模式套一下 Sparse Representation Classification，SRC 的分类器的样本就是图像，所以这里的底层特征不再是 local features 的 集合，就是图像本身；因为不再是 local features，所以也就不存在 local features 聚合的问题；因此，稀疏编码本身就是在做分类。</p>
<h6 id="局部线性约束编码"><a href="#局部线性约束编码" class="headerlink" title="局部线性约束编码"></a>局部线性约束编码</h6><p>稀疏编码的问题？为什么要引入局部线性约束?稀疏编码存在一个问题，即相似的局部特征可能经过稀疏编码后在不同的视觉单词上产生响应，这种变换的不连续性必然会产生编码后特征的不匹配，影响特征的区分性能</p>
<p>局部线性约束编码的提出就是为了解决这一问题，它通过加入局部线性约束，在一个局部流形上对底层特征进行编码重构，这样既可以保证得到的特征编码不会有稀疏编码存在的不连续问题，也保持了稀疏编码的特征稀疏性.局部线性约束编码中，局部性是局部线性约束编码中的一个核心思想，通过引入局部性，一定程度上改善了特征编码过程的连续性问题，即距离相近的局部特征在经过编码之后应该依然能够落在一个局部流形上.局部线性约束编码可以得到稀疏的特征表达，与稀疏编码不同之处就在于稀疏编码无法保证相近的局部特征编码之后落在相近的局部流形</p>
<h6 id="显著性编码"><a href="#显著性编码" class="headerlink" title="显著性编码"></a>显著性编码</h6><p>显著性编码引入了视觉显著性的概念，如果一个局部特征到最近和次近的视觉单词的距离差别很小，则认为这个局部特征是不“显著的”，从而编码后的响应也很小.</p>
<h6 id="Fisher-向量编码"><a href="#Fisher-向量编码" class="headerlink" title="Fisher 向量编码"></a>Fisher 向量编码</h6><p>基本思想: 编码局部特征和视觉单词的差</p>
<h5 id="Step-3-空间特征汇聚"><a href="#Step-3-空间特征汇聚" class="headerlink" title="Step 3: 空间特征汇聚"></a>Step 3: 空间特征汇聚</h5><p>空间特征汇聚是特征编码后进行的<strong>特征集整合操作</strong>，通过对编码后的特征，每一维都取其最大值或者平均值，<strong>得到一个紧致的特征向量作为图像的特征表达</strong>. 这一步得到的图像表达可以<strong>获得一定的特征不变性</strong>，同时也避免了使用特征集进行图像表达的高额代价.（目的是从 局部特征集合 set/collection 中得到 整个图像的一个特征表示向量）</p>
<h6 id="空间金字塔匹配"><a href="#空间金字塔匹配" class="headerlink" title="空间金字塔匹配"></a>空间金字塔匹配</h6><p>由于图像通常具有极强的空间结构约束，空间金字塔匹配(Spatial Pyramid Matching，<br>SPM)[9]提出将图像均匀分块，然后每个区块里面单独做特征汇聚操作并将所有特征向量拼接起来作为图像最终的特征表达</p>
<h4 id="深度学习模型"><a href="#深度学习模型" class="headerlink" title="深度学习模型"></a>深度学习模型</h4><p>自动编码器是<strong>基于 特征重构的无监督特征学习单元</strong>，加入不同的约 束，可以得到不同的变化，包括去噪自动编码器、稀 疏 自 动 编 码 器</p>
<h5 id="词包模型与-CNN-的异同"><a href="#词包模型与-CNN-的异同" class="headerlink" title="词包模型与 CNN 的异同"></a>词包模型与 CNN 的异同</h5><p>在词包模型中，对底层特征进行特征编码的过程，实际上近似等价于卷积神经网络中的卷积层，而汇聚层所进行的操作也与词包模型中的汇聚操作一样.</p>
<p>不同之处在于，<strong>词包模型实际上相当于只包含了一个卷积层和一个汇聚层，且模型采用无监督方式进行特征表达学习</strong>，而卷积神经网络则包含了更多层的简单、复杂细胞，可以进行更为复杂的特征变换，并且其学习过程是有监督过程的，滤波器权重可以根据数据与任务不断进行调整，从而学习到更有意义的特征表达.（因此，卷积神经网络具有更为强大的特征表达能力）</p>
<p>要注意分类算法的发展趋势：高计算强度，高内存消耗等，多特征、非线性分类器等这些在 PASCAL 竞赛中广为使用的算法和策略无法在 ImageNet 这样规模的数据库上高效实现.在性能和效率的权衡中，逐渐被更为简单高效的算法(单特征、特征压缩、线性分类器等)替代（总而言之是要 简单、有效）</p>
<h4 id="物体检测"><a href="#物体检测" class="headerlink" title="物体检测"></a>物体检测</h4><p>物体检测任务与物体分类任务最重要的不同在于，<strong>物体结构信息</strong>在物体检测中起着至关重要的作用，而物体分类则更多考虑的是物体或者图像的全局表达。（那这些物体检测算法是怎么刻画物体结构信息的？）</p>
<p>物体检测的输入是包含物体的窗口，而物体分类则是整个图像，就给定窗口而言，物体分类和物体检测在特征提取、特征编码、分类器设计方面很大程度是相通的。（物体检测与物体分类的相似之处）</p>
<p><strong>滑动窗口</strong>方法比较简单，它是通过使用训练好的模板在输入图像的多个尺度上进行滑动扫描，通过确定最大响应位置找到目标物体的外接窗口.（训练好的模板是什么？）</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Huang%20Review%20Tab%203.png" alt=""></p>
<p>相比 分类算法，检测算法多了一个 检测策略，把 空间特征汇聚 换成了 上下文，但其实 检测算法也有空间特征汇聚，比如 faster r-cnn，但是 上下文是目标检测独有的。</p>
<h5 id="结构化学习"><a href="#结构化学习" class="headerlink" title="结构化学习"></a>结构化学习</h5><p>与物体分类问题不同，物体检测问题从数学上是研究输入图像X与输出物体窗口Y之间的关系，这里Y的取值不再是一个实数，而是<strong>一组“结构化”的数据，指定了物体的外接窗口和类别</strong>，是一个典型的<strong>结构化学习问题</strong>.</p>
<p>弱标签结构化支持向量机(Weak-Label Structrual SVM，WL-SSVM)是一种更加一般的结构化学习框架，它的提出主要是为了处理标签空间和输出空间不一致的问题，对于多个输出符合一个标签的情况，每个样本标签都被认为是“弱标签”（可以认为弱监督学习问题是<strong>标签空间和输出空间不一致的问题</strong>）</p>
<h4 id="物体检测和物体分类的统一性"><a href="#物体检测和物体分类的统一性" class="headerlink" title="物体检测和物体分类的统一性"></a>物体检测和物体分类的统一性</h4><p>分类模型建立了以词包模型和深度学习模型 为基础的体系框架，检测模型则以可形变模型为核 心发展出多种方法</p>
<h5 id="物体检测可以取代物体分类"><a href="#物体检测可以取代物体分类" class="headerlink" title="物体检测可以取代物体分类?"></a>物体检测可以取代物体分类?</h5><p>在模型区分性比较强的情况下，也就是物体检测能给出准确的结果的情况下，物体检测在一定程度上可以回答“什么物体在什么地方”，但在真实的世界中，很多情况下模版不能唯一的反映出物体类别的唯一性，只能给 出“可能有什么物体在什么地方”，此时物体分类的介入就很有必要了.由此可见，物体检测是不能替代 物体分类的.（这里讲得其实是 Object Detection 会有 Classification Loss 和 Localization Loss 两个，Object Detection 还可以有先 Localization 再 Recognition 的思路，所以不一定要同时做出来，可以有先后的做出来；如果粒度比较细，就让专注分类的来做，而不是让 Localization 来拖累网络的专注性）</p>
<h5 id="物体检测和物体分类之间的差异性和互补性"><a href="#物体检测和物体分类之间的差异性和互补性" class="headerlink" title="物体检测和物体分类之间的差异性和互补性"></a>物体检测和物体分类之间的差异性和互补性</h5><p><strong>物体检测</strong>主要采用的是可变的部件模型，更多的<strong>关注局部特征</strong>，物体分类中主要的模型是词包模型，从两者的处理流程来看，他们利用的信息是不同的，物体检测更多的是利用了物体自身的信息，也就是局部信息，<strong>物体分类</strong>更多的是利用了图像的信息，也就是<strong>全局的信息</strong>. 他们各有优劣，局部信息考虑了 更多的物体结构信息，这使得物体检测和分类的准确性更高，但同时也带来物体分类的鲁棒性不强的问题; 全局信息考虑了更多的是图像的全局统计信息，尤其是图像的语义信息，这使得能考虑更多的信 息来进行判断，但信息量的增加可能带来准确度的提高，也可能由于冗余降低分类的性能，但是从统计意义而言，其鲁棒性是能够得到一定的提高的.由此 可见，物体检测和物体分类之间存在着较大的差异 性，同时也就说明存在着比较大的互补性.（我是没看懂，物体检测不就是 Classification on Proposed Region 这个思路么？没啥区别吧？）</p>
<h4 id="物体分类与检测的发展方向"><a href="#物体分类与检测的发展方向" class="headerlink" title="物体分类与检测的发展方向"></a>物体分类与检测的发展方向</h4><p>物体分类中<strong>全局表达</strong>更关键；物体检测中<strong>物体结构</strong>更为关键。</p>
<ol>
<li>专注于学习结构，即结构化学习.观察变量与其他变量构成结构化的图模型，<strong>通过学习得到各个变量之间的关系</strong>，结构包括有向图模型(贝叶斯网络)、无向图模型(马尔科夫网络). 结构化学习通常变量具有显式的物理意义，变量之间的连接也具有较强的因果关系，解释性较好.</li>
<li>专注于学习层次化表达，即深度学习.（这个好像做不动了吧？基础架构不动了，但通过Attention改善特征质量好像还可以做）</li>
</ol>
<p>两条思路各有侧重，但并不是互相独立的.在这两条发展线路的基础上，建立更为统一的物体识别框架，同时处理物体分类与检测任务，是一个更加值得研究的方向. 如何<strong>利用物体检测和物体分类之间的互补性去构建统一的物体识别框架</strong>（利用物体检测和物体分类的统一性）是计算机视觉和视觉认知领域的研究热点，也是视觉认知计算模型研究的重点之一</p>
<h4 id="层次化学习-深度学习-存在的难点与挑战"><a href="#层次化学习-深度学习-存在的难点与挑战" class="headerlink" title="层次化学习(深度学习)存在的难点与挑战"></a>层次化学习(深度学习)存在的难点与挑战</h4><ol>
<li>解释性差（真的差吗？文章里的解释没法说服我）</li>
<li>模型复杂度高，优化困难.</li>
<li>计算强度高：对于深度学习，输入一个视觉信号，所有的神经元都会进行计算，人为加的一些稀疏约束只是会使某些神经元输出为0，但不代表该神经元“处于不活动”状态</li>
<li><strong>模型缺少结构约束</strong>.深度学习模型通常只对网络的“输入-输出”进行建模，却<strong>缺少必要的结构先验的约束</strong>.例如，对人脸关键点可以采用卷积神经网络进行回归，网络学习到的是一种隐式的“输入-输出”结构，却完全没有加入显式的结构先验；<strong>将显式结构先验嵌入深度学习模型中</strong>，可以有效降低网络参数空间的规模，减少局部极值的问题（怎么将显式结构先验嵌入深度学习模型中？）</li>
</ol>
<h3 id="36-Biological-Explanation-on-Visual-Selective-Attention"><a href="#36-Biological-Explanation-on-Visual-Selective-Attention" class="headerlink" title="[36] Biological Explanation on Visual Selective Attention"></a>[36] Biological Explanation on Visual Selective Attention</h3><p>这篇是对下面两篇文章的笔记整理。两篇文章都是同一个作者，前一篇是 ECCV 2018 的文章，后面作者挂在了 arXiv 上，看样子是投 CVPR 的。</p>
<blockquote>
<p>Yohanandan, ECCV 2018, Saliency Preservation in Low-Resolution Grayscale Images<br>Yohanandan, arXiv 2018, Fast Efficient Object Detection Using Selective Attention</p>
</blockquote>
<p>注意力机制一般分为 bottom-up 和 top-down 两种。Bottom-up 的注意力机制是外界刺激和特征驱动的，负责快速、自动且不由自主的注意力和凝视的快速转变；相反，top-down 机制是任务驱动、基于经验（记忆）的，因人而已。这篇笔记，或者说上面两篇论文<strong>只针对 Bottom-up</strong>，主要内容是<strong>从生理机制上解释为何人类的显著性检测和选择性注意力能够如此高效</strong>。此外，对我而言，读完这两篇文章另外一个收获是对于 Object Detection 中的 One-stage method 和 Two-stage method 中的选择，让我坚定了对 Two-stage method 的信仰，因为在我看来人眼 Bottom-up 的视觉选择性注意就是一个 two-stage 过程。</p>
<h4 id="Biological-Explanation"><a href="#Biological-Explanation" class="headerlink" title="Biological Explanation"></a>Biological Explanation</h4><p>作者先是讲了一个生物视觉进化的故事：</p>
<ol>
<li>最早的生物都是没有视觉的</li>
<li>后来略微进化出一点感光细胞，有了 discriminated night and day 的能力；</li>
<li>再后来，有了一点光源定位的能力，能够 distinguishing light from shadow；</li>
<li>接着是进化出能够粗略认出周围物体的能力，这被认为是 the birth of stimulus-driven, bottom-up visual salience detection，到这一阶段是 <strong>blurry achromatic peripheral vision</strong>，模糊的消色的外围视觉</li>
<li>然后，进化出 focus-sharpening lenses</li>
<li>再然后是 foveated central vision</li>
<li>最后才是感知 彩色 的能力，到这一步才有 <strong>high-acuity chromatic central vision</strong>，高度敏锐的彩色中央视觉</li>
</ol>
<p>从这个故事叙述里，作者已经开始在暗示 彩色 和 高敏锐度的视觉都是在 粗略认出周围物体的能力 之后发展出来的。如果只是要模拟粗略认出周围物体的能力（这就是 Region Proposal 干的事情），那么就不需要在我们的 computational model 里面去涉及对彩色 和 高敏锐度的视觉的模拟，彩色就是彩色，高敏锐度的视觉则指的是高分辨率图像，也就是说 Region Proposal 或者 Saliency Detection 不需要在彩色高分辨率图像上做，只要 blurry achromatic peripheral vision 就可以了。</p>
<p>由此可见，人类 bottom-up Attention 其实有两部分构成，一部分是 blurry achromatic peripheral vision，另一部分是 high-acuity chromatic central vision。<strong>Peripheral vision 相当于 Region Proposal</strong>，负责 bottom-up visual salience detection，用来指示 the sharper, high-resolution foveated (sometimes chromatic) vision to investigate objects and regions further，由此实现了人类可以 rapidly shift foveal gaze to salient regions 的能力。但这样机制的背后，是有具体的生理基础。</p>
<p>总所周知，我们视网膜上的感光细胞分为<strong>视杆细胞</strong>（Rods）和<strong>视锥细胞</strong>（Cones），这两种细胞在分工、数量、分布和连接的后续细胞的数量上都有以下不同：</p>
<ol>
<li>在分工上，Rods primarily encode achromatic luminance (brightness) information；而 cones encode chrominance (color)；</li>
<li>在分布上，Rods have a higher distribution outside the fovea；而 are concentrated in the fovea (center of the retina)，这就是为什么一个被叫做 peripheral vision，一个被叫做 foveated central vision，下图展示了 Rods 和 Cones 的分布情况</li>
<li>在连接的后续细胞的数量上，multiple rods converge to and activate a single retinal ganglion neuron，而 each cone activates multiple ganglion neurons。这个 Retinal ganglion cells (RGCs) 是 Final output neurons of the retina，可以代表输出图像的分辨率。因为连接的后续细胞的数量上，Rods 少于 Cones，这也是为什么 rod vision has <strong>lower spatial resolution</strong>，而 Cones vision 是 higher acuity vision。为什么最后输出的图像分辨率会不同，因为决定感知到的图像分辨率（perceived image resolution）的 并不是 光感受体（photoreceptors）数量，而是 视网膜传入神经节神经元 RGCs，而视锥细胞连接的 视网膜传入神经节神经元 比 视杆细胞多。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Yohanandan%20ECCV%2018%20Fig%202a.png" alt=""></p>
<p>因此，可以稍微概括下，<strong>Cones</strong>, concentrated in the <strong>fovea</strong>, encode <strong>high-resolution color</strong>. <strong>Rod</strong> photoreceptors distributed <strong>outside the fovea</strong> encode <strong>low-resolution grayscale</strong> information。</p>
<p>那么具体 Rods 对应的 RGCs 和 Cones 对应的 RGCs 分别是什么，又是怎么一个数量关系呢？RGCs 作为视网膜最后的输出神经元，有 P$\beta$ RGCs，P$\alpha$ RGCs 和 P$\gamma/\epsilon$ RGCs 三类。其中，P$\beta$ RGCs 负责通过 longwave (red), medium-wave (green), and shortwave (blue) sensitive detectors 表现 color opponency 颜色对比。什么是这些 RGB sensitive detectors 呢？就是 Cones 视锥细胞。而 P$\alpha$ RGCs 和 P$\gamma/\epsilon$ RGCs 是消色的，编码 primarily luminance information。P$\beta$ RGCs，P$\alpha$ RGCs 和 P$\gamma/\epsilon$ RGCs 的分布分别是 Laplacian，Gaussian 和 Poisson，前两者分布相对集中、中心化，而后者分布相对均匀、平坦。具体如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Yohanandan%20Selective%20Attention%20Fig%202.png" alt=""></p>
<p>在所有 RGCs 中，大约 80% 是表现颜色对比度的 P$\beta$ RGCs；大约 10% 的 RGCs 是消色的 P$\alpha$ neurons，他们的轴突投射都是从视网膜的 foveal region 到 lateral geniculate nucleus 外侧膝状体，简称 LGN。只有剩下 10% 的 P$\gamma$ RGCs 和 P$\epsilon$ RGCs 以及某一些 P$\alpha$ RGCs，被投影到了一个叫做  optic tectum，中文叫视顶盖，也叫 superior colliculus，中文叫 上丘 的结构，简称 SC。由于 SC 接收到的信息只有所有 RGCs 的 10%，这就是为什么论文中会说 SC 所接收的信息只有输入视网膜信息的 10% 的依据，是属于较低分辨率的视觉信息，且是消色的。而近年来神经科学的研究发现，<strong>SC 是负责产生显著性图的区域</strong>，是 bottom-up salience detection 的先驱。 此外，SC 还对眼部肌肉有着直接控制，direct retinal input into the SC 引发由脑干动眼神经核控制的反射般的跳视 reflex-like saccades，这个应该是为了输入 High-Resolution Color Foveal Image。Eye movements align objects with the high-acuity fovea of the retina, making it possible to gather detailed information about the world。因此，在我看来，Rods 负责完成消色下采样，<strong>SC 负责 generate Saliency map 并在此基础上做 Region Proposal（通过 control eye muscles 实现）</strong>。</p>
<p>多提一句 Saliency map 的价值在于 This mapping projects the locations of salient and interesting regions in visual space, thus making vision more efficient by narrowing down the regions an observer must attend to in a typically large visual field. 这是 Region Proposal 的作用。</p>
<p>这里表明，人类视觉有两条视觉通路，一条是开始与消色的 Rods 光感受体 -&gt; P$\gamma$ RGCs 和 P$\epsilon$ RGCs -&gt; SC 的通路，这条通路叫作 the retinocollicular pathway，也是下图中红色箭头所指的；另一条是 Cones -&gt; P$\beta$ RGCs 和 P$\alpha$ RGCs -&gt; LGN -&gt; V1 的通路，具体如下图所示。两条通路这件事对我震动蛮大的，以前人们认为 the saliency map was generated in the primary visual cortex (V1)，也就是认为图像只有一个输入，就是高分辨率的输入；但现在发现，图像其实有 两个输入，<strong>different visual pathways</strong>，一个是输入 SC 的 LG 图像，另一个是根据 SC 产生的 Saliency Map 指导下输入的 high-resolution region。但目前我们 end-to-end 的算法都只有高分辨率输入，要不要为此做出相应的改变呢？</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Yohanandan%20ECCV%2018%20Fig%202b.png" alt=""></p>
<p>现在我们已经知道人类视觉是在以 Rods 为起点的 blurry achromatic peripheral vision 通路，和以 Cones 为起点的 high-acuity chromatic central vision 两条通路合作下工作的，那么它们是怎么合作的呢？</p>
<p>快速高效的 Bottom-Up Visual Attention 的机制如下，关键在于 processing low-resolution achromatic visual information from the retina:</p>
<ol>
<li>首先，the retinocollicular pathway shrinks the high-resolution color image projected onto the retina from the visual field into a tiny colorless, e.g. low-resolution grayscale, image, which can then be scanned quickly by the SC to highlight peripheral regions worth attending to via the saliency map. </li>
<li>The SC then aligns the fovea to attend to one of these regions, thereby sending higher-acuity, e.g. high-resolution color, visual information to the LGN and beyond for further processing.</li>
<li>In doing so, a new image of the visual field is now projected onto the retina, and the cycle repeats. </li>
</ol>
<p>具体的示意图如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Yohanandan%20Selective%20Attention%20Fig%203.png" alt=""></p>
<p>由此可见，Visual signals from the retina to the cerebral cortex are mediated through the primary visual cortex (V1) and the superior colliculus (sSC and dSC). 输入大脑皮层的视觉信息同时受到 primary visual cortex (V1)  和 superior colliculus 的调解，这也应了一个很早以前就看到的观点，大脑并不是一个中央处理器，而是由多个处理器共同决策、控制的结果。</p>
<p>除了输入是 Low-Resolution Grayscale Images 之外，Visual Saliency Attention 能够快速有效的还有一个生理基础在于存在一个 <strong>a shortcut</strong> from the superficial (sSC) to the deep (dSC) superior colliculus, which then sends outputs directly to the brainstem oculomotor nuclei, resulting in rapid saccades，也就是 现在的 shortcut 是 retina -&gt; sSC -&gt; dSC -&gt; brain-stem -&gt; retina，否则得话，我看图上有 retina -&gt; LGN -&gt; V1 -&gt; V4 -&gt; FEF -&gt; dSC -&gt; brain-stem -&gt; retina 这种绕一圈的通路，这就没法 rapid 了。</p>
<h4 id="Computational-Approximation"><a href="#Computational-Approximation" class="headerlink" title="Computational Approximation"></a>Computational Approximation</h4><p>我们稍微概括一下上面的生物机制，然后看看如何用计算模型模拟这种生物机制</p>
<ol>
<li>SC 是产生显著性图的地方</li>
<li>SC 的输入是 low-resolution grayscale image</li>
</ol>
<p>对上述生物机制的计算模型逼近显然易见，就是<strong>在 Low-Resolution Grayscale Images 上计算显著性图，并以此显著性检测的结果作为高分辨彩色输入图像上的 Region Proposal，然后再进一步识别。</strong> 在 Low-Resolution Grayscale Images 上计算显著性图先比在 High-Resolution Color Images 上计算并不会有什么信息缺失，这一点不管是上面的生物机制启示还是论文后面的数值实验都验证了，这就是论文标题的 Argument， Saliency Preservation in Low-Resolution Grayscale Images，而这么做的好处是非常高效。</p>
<ol>
<li>怎么将 High-Resolution Color Images 降采样成 Low-Resolution Grayscale Images？（模拟 视杆细胞 负责对图像下采样）</li>
<li>怎么在 Low-Resolution Grayscale Images 上实现显著性检测？（模拟 SC 负责的基于显著性检测的 Region Proposal）</li>
<li>怎么对 High-Resolution Color 的 Foveal Image（比如上图右上角的长颈鹿所在的 Region Proposal）实现识别？</li>
</ol>
<p>那么降采样改怎么做呢？分为两步。感觉这个就很平常了</p>
<ol>
<li>第一步，transforming the color space of high-resolution color (HC) images $I_HC$ to 8-bit grayscale $I_HG$</li>
<li>第二步是 down-sampled the original image resolution using bicubic interpolation (单边 64 像素，或者 downsample the original image to 10% of its original size，但作者也提到了，10% 是依据猕猴的生理机制来的，未必对于计算机视觉里的Object Detection 就是最优的)</li>
</ol>
<p>但作者说这篇文章是提出本文是最早提出 Saliency Preservation in Low-Resolution Grayscale Images，感觉也有点过了，我印象里至少 Hou Xiaodi 在 CVPR 2007 的 Spectral Residual 里面就提到 64 x 64 就足好地可以计算显著性了。</p>
<p>至于怎么实现显著性检测，作者虽然给了方案，但是太简略了，也不是我对论文感兴趣的原因。至于第三个问题，论文中也没有提。</p>
<h4 id="对目标检测的启示"><a href="#对目标检测的启示" class="headerlink" title="对目标检测的启示"></a>对目标检测的启示</h4><p>目前方法慢的原因：不管是 One-Stage 还是 Two-Stage 的 Region Proposal 阶段，虽然他们都是在 downsampling 后的 feature map 上做的，但是这些 feature map 都来自于 <strong>High-Resolution</strong> Color Images。从 <strong>High-Resolution</strong> Color Images 得到这些 Feature Map 需要消耗很多的计算量，这是一个慢的原因。另一个慢的原因是 exhaustive classification，为了要 densely covering many different spatial positions, scales, and aspect ratios 需要 evaluate $10^4 − 10^5$ candidate regions per image. 但我不是很赞成后面一点，因为在不知道目标可能在哪之前，总要考虑所有区域，做 Saliency Detection 本身也是一件需要 exhaustive search 的事情，计算的绝大多数区域而是一样的 uninformative background。</p>
<p>而在 Low-Resolution Grayscale Images 上做 Region Proposal 的好处是，significantly reducing the visual search space of objects and regions of interest；因为 输入是 LG，总的计算量不大，另外一个好处是，generating a saliency map 只要 a relatively small and simple neural network 就够了，模型不需要很大。</p>
<p>作者这里还有一个 argument，是 detecting the presence of an object，如果仅仅是 detect 也就是判断有无，而不是判断哪一类（Classification），那么high-resolution details about objects, such as texture, patterns, and shape 这些是没有用的，color or other feature-specific properties are seem only essential for classification，这个 argument 支持了为什么 natural vision 会有 LG transformation。但作者这里的 Detect 是 Region Proposal 阶段的 Saliency Detection，并不是现在我们常说的 Object Detection 的 Detection。</p>
<p>最后的最后，这篇文章给我留下的疑惑是，这丫不就又回到 R-CNN，Region Proposal 和后面的 Classification 分开做，且 Region 要 resize 之后的路线了么？</p>
<h2 id="To-Read-List"><a href="#To-Read-List" class="headerlink" title="To Read List"></a>To Read List</h2><ol>
<li>[1] 郑南宁. 认知过程的信息处理和新型人工智能系统[J]. 中国基础科学. 2000(8): 9-18.</li>
<li>[2] ICCV-2015-Look and think twice: capturing top-down visual attention with feedback convolutional neural networks（在 CNN 的卷积层加上层间的反馈连接，将高级的语义和全局信息传到下层，通过语义标签的反馈，可以激活特定的与目标语义相关的神经元，从而实现自顶向下的视觉注意，定位复杂背景中的潜在目标。）</li>
<li>[3] NIPS-2014-Attentional neural network: feature selection using cognitive feedback （通过 top- down 的反馈连接和乘法机制引入注意力模型）</li>
<li>[4] CVPR-2015-Recurrent convolutional neural network for object recognition 和 [5] NIPS-2015-Convolutional neural networks with intra-layer recurrent connections for scene labeling （在 CNN 的卷积层加上层内连接的方法，使每个单 元可同时接收前馈和反馈的输入）</li>
<li>Scale-Aware Trident Networks for Object Detection 主要要解决的问题便是目标检测中最为棘手的 scale variation 问题</li>
</ol>
<p>[2 - 5] 都是为了更好地模拟脑，将将反馈引入神经网络的尝试（脑皮层中反馈神经元连接比前 馈多得多，但是传 统的深度神经网络模型里一般只有前馈连接，尚缺乏对 反馈的建模）</p>
<h3 id="Deep-Image-Homography-Estimation"><a href="#Deep-Image-Homography-Estimation" class="headerlink" title="Deep Image Homography Estimation"></a>Deep Image Homography Estimation</h3><p>还是 Magic Leap 的文章，跟上一篇 SuperPoint 是同一个作者 Daniel DeTone。</p>
<blockquote>
<p>Instead of manually engineering corner-ish features, line-ish features, etc, is it possible for the algorithm to learn its own set of primitives?</p>
</blockquote>
<p><strong><em>从上面这段话看来，作者不是要做不是人工设计的特征，而是自己学特征么？但从标题看，作者更是要做学怎么估计 Homography?</em></strong></p>
<hr>
<p>如果您觉得我的文章对您有所帮助，不妨小额捐助一下，您的鼓励是我长期坚持的一大动力。</p>
<table>
<thead>
<tr>
<th style="text-align:left"><img src="https://raw.githubusercontent.com/YimianDai/images/master/Alipay_Middle.png" alt="Alipay_Middle"></th>
<th style="text-align:left"><img src="https://raw.githubusercontent.com/YimianDai/images/master/Wechat_Middle.png" alt="Wechat_Middle"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h3 id="Discriminative-learning-of-deep-convolutional-feature-point-descriptors"><a href="#Discriminative-learning-of-deep-convolutional-feature-point-descriptors" class="headerlink" title="Discriminative learning of deep convolutional feature point descriptors"></a>Discriminative learning of deep convolutional feature point descriptors</h3><p>ICCV 2015 的文章。</p>
<p>本文想做的目的是 Representing local image patches in an invariant and dis- criminative manner</p>
<blockquote>
<p>In our case discriminative training does not rely on labels of individual patches, but rather on pairs of cor- responding, or non-corresponding patches. 这是不是也是一种 weakly supervised learning？</p>
</blockquote>
<p>use a Siamese network architecture </p>
<p>treating the CNN outputs as patch descriptors（CNN 的 output 直接作为 patch descriptor）</p>
<p>minimize a loss that enforces the L2 norm of their difference to be small for corresponding patches and large otherwise （Loss 就是 让一样的 pair 之间的 L2 norm 尽量小，不一样的尽量的大）</p>
<p>作者提出的 a strategy of aggressive mining of “hard” positives and negatives 到底是什么？</p>
<p>这里首先有个问题，it may be unclear whether CNNs are equally appropriate for patch-level applications where semantic information may be missing，也就是说 Patch 里面的 semantics 不够，那么 CNN 适合来做 patch 吗？</p>
<p>simply use the L2 distance to compare patches 是本文的一个卖点</p>
<p>这篇文章其实是来解决 descriptor 的 repeatability 的，这里的 repeatability 由同一对 pair 通过 Siamese 网络判断是不是同一对来保证，对于实际上是同一对但有视角变化的 pair 来说，应该要输出相近的 descriptor</p>
<p>最后一层的输入是 8<em>8，滤波器是 5</em>5，如果不做 padding，那么滤波后的大小就是 4 <em> 4，又因为 Pooling 的 大小是 4 </em> 4，所以最后输出的就是一个 1 <em> 1 </em> 128 的向量</p>
<h3 id="3DMatch-Learning-Local-Geometric-Descriptors-from-RGB-D-Reconstructions"><a href="#3DMatch-Learning-Local-Geometric-Descriptors-from-RGB-D-Reconstructions" class="headerlink" title="3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions"></a>3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions</h3><p>2017 CVPR</p>
<p>本文的目标是想做什么？ our goal is to create a function ψ that maps the local volumetric region (or 3D patch) around a point on a 3D surface to a descriptor vector，也就是想做一个 3D 的 local Feature descriptor</p>
<p>同 2D 一样，这个 3D 的 local Feature descriptor 也要具有 repeatability，具体表现就是 Given any two points, an ideal function ψ maps their local 3D patches to two descrip- tors, where a smaller l2 distance between the descriptors in- dicates a higher likelihood of correspondence.</p>
<p>本文的 3D local Feature descriptor 是学出来的，要学习就要有 data 和 label，本文的 data 和 label 的来源是 making use of data from existing high quality RGB-D scene reconstructions，<strong>我没看到是什么意思，到底怎么来的？</strong></p>
<p>文章说，each reconstruction contains millions of points that are observed from multiple different scanning views，是说 3D 模型可以转啊，也就是不同的视角，这就是现成的匹配数据（同 Homographical adaptation 在 2D 平面上变化一个道理，都是用来做自监督的）</p>
<p><strong><em>文章通篇再讲 descriptor，关键在于 Detector 是怎么产生的，文中的 interest point 是怎么产生的？</em></strong></p>
<p>是直接对 patch 做的，关键是你怎么知道哪些 patch 是相关的，哪些不是的？</p>
<p>因为知道 3D 点的 3D position，然后随着帧的变化，只要在不同帧的这个 3D position 周围的两个 3D patch 就是匹配的 pair</p>
<h3 id="Discriminative-learning-of-deep-convolutional-feature-point-descriptors-1"><a href="#Discriminative-learning-of-deep-convolutional-feature-point-descriptors-1" class="headerlink" title="Discriminative learning of deep convolutional feature point descriptors"></a>Discriminative learning of deep convolutional feature point descriptors</h3><p>ICCV 2015 的文章</p>
<blockquote>
<p>使用卷积网络提取图像局部特征，得到的 patch 描述之间的距离可以直接用欧式距离度量。可以在各种应用中直接替代 SIFT 等特征。网络是用 Contrastive Loss 训练的</p>
</blockquote>
<p>develop 128-D descriptors whose euclidean distances reflect patch similarity，and which can be used as a drop-in replacement for any task involving SIFT</p>
<h3 id="Quad-networks-unsupervised-learning-to-rank-for-interest-point-detection"><a href="#Quad-networks-unsupervised-learning-to-rank-for-interest-point-detection" class="headerlink" title="Quad-networks: unsupervised learning to rank for interest point detection"></a>Quad-networks: unsupervised learning to rank for interest point detection</h3><p>用人肉去 label interest point 是不现实的，it is often unclear what points are “interesting”, human labelling cannot be used to find a truly unbiased solution. Therefore, the task requires an unsupervised formulation. 所以作者打算用无监督的来做</p>
<p>有意思的是作者还尝试了 cross-modal interest point detection between RGB and depth images，这个可以关注一下</p>
<p>Learning an interest point detector is a task where labelling ambiguity goes to extremes. 对于 label 而言，interest point 的 label 是非常模棱两可的，没有很清晰的标准。</p>
<p>我们想要得到什么？a sparse set of image locations which can be detected repeatably even if the image undergoes a significant viewpoint or illumination change</p>
<p><strong><em>retaining the top/bottom quantiles of the response function (contrast filtering) 和 retaining the local extrema of the response function (non-maximum suppression) 这两有啥区别啊？</em></strong></p>
<p>为什么叫做  quadruple？因为这个涉及到 ranking，所以按照公式（1），会有两个点，然后又是经过变换 t 前后的，所以一共会有 4 个点。</p>
<p>按照公式（2），因为 H 是一个  a single real-valued response function，所以如果两个点在变换前后的 ranking 不发生改变，则这个 ranking agreement function 的数值会是正的，否则就会是 负的</p>
<p>$$<br>\begin{array} { l } { R \left( p <em> { d } ^ { i } , p </em> { d } ^ { j } , p <em> { t ( d ) } ^ { i } , p </em> { t ( d ) } ^ { i } , p <em> { t ( d ) } ^ { j } | w \right) = } \ { \left( H \left( p </em> { d } ^ { i } | w \right) - H \left( p <em> { d } ^ { j } | w \right) \right) \left( H \left( p </em> { t ( d ) } ^ { i } | w \right) - H \left( p _ { t ( d ) } ^ { j } | w \right) \right) } \end{array}<br>$$</p>
<p>the set of corresponding point indexes 到底是什么？是变换前后 matching 的点。</p>
<p>那这里只讲了 detect 到一些特征点后，怎么保证 detect 的 invariant，<strong><em>但没有涉及如何 detect，以及如何做 matching？</em></strong></p>
<p>observation 2 里面的 visible 到底是什么意思？</p>
<h3 id="LIFT-Learned-Invariant-Feature-Transform"><a href="#LIFT-Learned-Invariant-Feature-Transform" class="headerlink" title="LIFT: Learned Invariant Feature Transform"></a>LIFT: Learned Invariant Feature Transform</h3><p>ECCV 2016 的文章</p>
<p>LIFT 的训练数据来源于使用 Structure from Motion(SfM) 算法得到的特征点</p>
<p>Feature descriptors are designed to provide discriminative representations of salient image patches, while being robust to transformations such as viewpoint or illumination changes. 注意哦，Feature descriptor 就是来描述 salient image patches 的。</p>
<p>There are datasets that can be used to train feature descriptors [24] and orientation estimators [9]. 竟然有特征点描述子的训练集？</p>
<p>特征点检测就是通过 provides a score map S 以及 perform a soft argmax [12] on the score map S and return the location x of a single potential feature point 来完成的吧，那么问题来了，就是<strong>首先你的 score map S 是怎么产生的？</strong></p>
<p>learning first the Descriptor, then the Orientation Estimator given the learned descriptor, and finally the Detector, conditioned on the other two 是说这三部分网络，当学习一个的时候，就固定另外两个的权重？</p>
<p>发现好像这类文章里的 Descriptor 网络的输入都是 Patch</p>
<p>softargmax is a function which computes the Center of Mass，万万没想到 softargmax 竟然是计算重心的？真的假的？按照公式 7 真的是的啊</p>
<h3 id="LF-Net-Learning-Local-Features-from-Images"><a href="#LF-Net-Learning-Local-Features-from-Images" class="headerlink" title="LF-Net: Learning Local Features from Images"></a>LF-Net: Learning Local Features from Images</h3><p>NIPS 2018 的文章</p>
<p>涉及到了 depth maps 啊，可以是 laser scanners 或者说 shape-from-structure algorithms 来的</p>
<h3 id="Scale-Aware-Trident-Networks-for-Object-Detection"><a href="#Scale-Aware-Trident-Networks-for-Object-Detection" class="headerlink" title="Scale-Aware Trident Networks for Object Detection"></a>Scale-Aware Trident Networks for Object Detection</h3><p><strong><em>本文说的 generate scale-specific feature maps 是怎么一回事？</em></strong></p>
<p>a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields</p>
<p>a scale-aware train- ing scheme to specialize each branch by sampling object instances of proper scales for training</p>
<p><strong><em>本文说的  sacrificing the feature consistency across different scales. 怎么理解？</em></strong></p>
<p>Both image pyramid and feature pyramid methods <strong>share the same motivation</strong> that <strong>models should have different receptive fields for objects of different scales</strong>.</p>
<p> With the help of dilated convolutions [43], different branches of trident blocks have the same network structure and share the same parameter weights yet have different receptive fields. 权重是一样的，只是 dilated conv 的间隔不一样，这就是 Trident Networks 的关键吗？</p>
<p>to avoid training objects with extreme scales, we leverage a scale-aware training scheme to make each branch specific to a given scale range matching its receptive field.</p>
<p><strong><em>approximate the full TridentNet with only one major branch during inference 不理解，怎么做到的？</em></strong> 敢这么做的原因是 This approximation only brings marginal performance degradation.</p>
<p>影响 backbone network 的要素：downsample rate, network depth and receptive field.</p>
<p>Section 3 的 Table 1 是本文最基础的来源，目标的感受野要与目标的尺度相适应</p>
<p>自然而然地有了一个想法，如果可以对不同尺度的目标、同时能有大小不同的感受野去反应它们的特征</p>
<p>以 ResNet 的 residual block 为例，一般是 1x1,3x1,1x1 连续三个卷积核，其中 3x3 的卷积将用上述方法改良，获得三个由不同 rate 的 dilate 卷积构成的并行分支，改良后的结构称之为 trident blocks。</p>
<p>由于存在多分支，最后 inference 的时候，会像 SNIP 一样，每个分支预测出来的结果，只有大小没超出该分支合法范围的 gt 才作数</p>
<p>作者提出的 fast inference approximation 就是用 rate 为 2 的分支近似替代三个分支，来加速预测</p>
<p>考虑对于一个 detector 本身而言，backbone 有哪些因素会影响性能。总结下来，无外乎三点：network depth（structure），downsample rate 和 receptive field。</p>
<p>对于前两者而言，其影响一般来说是比较明确的，即网络越深（或叫表示能力更强）结果会越好，下采样次数过多对于小物体有负面影响。</p>
<p>但是没有工作，单独分离出 receptive field，保持其他变量不变，来验证它对 detector 性能的影响</p>
<p>我们很惊奇地发现，<strong>不同尺度物体的检测性能和 dilation rate 正相关</strong>！（应该是 物体尺度与 dilation rate 的匹配程度 与 检测性能正相关才对），更大的 receptive field 对于大物体性能会更好，更小的 receptive field 对于小物体更加友好</p>
<p><strong>先 investigate receptive field size 对检测性能的影响</strong>，而 receptive field size 这里的表现是 dilation rate，而不是 深度（receptive field size 既可以被 dilation rate 影响，也可以被 receptive field size 影响，还有 anchor size 也会影响）。在发现了 物体尺度与 dilation rate 的匹配程度 与 检测性能正相关之后，<strong>要解决的问题是 有没有办法把不同 receptive field 的优点结合在一起呢？</strong></p>
<p>Tridentnet is a <strong>backbone</strong>, yolo is a detector. You can apply trident block to yolo to get the best of both worlds.</p>
<p>Tridentnet 是个 backbone，是个从 Detection 出发考虑的 backbone，想 ResNet 这种 Backbone 就不是专门为 Detection 考虑的</p>
<p>总结一下，我们的 TridentNet 在原始的 backbone 上做了三点变化：</p>
<ol>
<li>第一点是构造了不同 receptive field 的 parallel multi-branch，</li>
<li>第二点是对于 trident block 中每一个 branch 的 weight 是 share 的。</li>
<li>第三点是对于每个 branch，训练和测试都只负责一定尺度范围内的样本，也就是所谓的 scale-aware。</li>
</ol>
<p>这三点在任何一个深度学习框架中都是非常容易实现的。</p>
<p>对 scale variation issue 解决的尝试：</p>
<ol>
<li>leverage multi-scale image pyramids<ul>
<li>SNIP 就是这一流派的</li>
</ul>
</li>
<li>employ in-network feature pyramids<ul>
<li>最早是 Fast feature pyramids for object detection 这篇文章</li>
<li>然后是 SSD，接着是 FPN</li>
</ul>
</li>
</ol>
<h3 id="A-Unified-Multi-scale-Deep-Convolutional-Neural-Network-for-Fast-Object-Detection"><a href="#A-Unified-Multi-scale-Deep-Convolutional-Neural-Network-for-Fast-Object-Detection" class="headerlink" title="A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection"></a>A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</h3><p>ECCV 2016</p>
<p>multi-scale CNN (MS-CNN)</p>
<p>这篇文章是 最早的一个解决尝试目标检测的尺度变化问题的</p>
<p><strong><em>这文章里面说 RPN 是 A single receptive field，所以不能 handle multi-scale issue 我没看懂</em></strong></p>
<p><strong><em>什么是 the inconsistency between the sizes of objects and receptive fields？</em></strong></p>
<p>这篇文章还是跟 SSD 思路差不多啊，The detection is preformed at various intermediate network layers, whose receptive fields match various object scales</p>
<p><strong><em>既然是跟 SSD 差不多，那么为什么要有一个上采样的过程呢？</em></strong> 是为了小目标检测，enabling small objects to produce larger regions of strong response，对 Features 做上采样是用来代替对 input 做上采样，可以节省计算量。</p>
<p><strong><em>从 Fig 3 看，MSCNN 跟 SSD 有什么区别？</em></strong></p>
<blockquote>
<p>MSCNN 中提出的多层特征金字塔上各个分支独立做目标检测，跟 ECCV2016_SSD 的思想很相似，但又往前迈进了一步：SSD 各个分支上预定义 anchor 与 gt bbox 的匹配是粗放的，一个 gt bbox 可能与多个分支上的 anchor 匹配；但 MSCNN 做到了将与 gt bbox 匹配的 anchor 固定到了特定的某一层，某层 feature map 上的 anchor 仅用于匹配特定尺度范围的 gt bbox；预测时，每层分支上独立预测特定尺度范围的目标，这样 MSCNN 每个分支就相当于一个只对特定尺度范围目标做检测的专家系统，然后各个专家系统结果融合，形成三个臭皮匠，顶个诸葛亮的效果，最终能 cover 所有目标尺度的算法；</p>
<p>在 proposal 生成子网络中，基于多尺度特征金字塔分支独立做 proposal 预测，每层 feature map 上特定范围的感受野来匹配特定尺度范围的目标；这些只检测特定尺度范围的检测分支最终联合成一个能处理多尺度目标的检测器；可以达到输入单尺度图像，却能检测多尺度目标之目的，而且不需要图像金字塔操作，检测速度也快；</p>
</blockquote>
<p>别人是怎么 handle multi-scale 的？</p>
<ol>
<li>Fast R-CNN 在单尺度 feature map 上预定义多尺度 anchor 达到能覆盖多尺度目标之目的；</li>
</ol>
<h3 id="Attention-guided-Unified-Network-for-Panoptic-Segmentation"><a href="#Attention-guided-Unified-Network-for-Panoptic-Segmentation" class="headerlink" title="Attention-guided Unified Network for Panoptic Segmentation"></a>Attention-guided Unified Network for Panoptic Segmentation</h3><p>panoptic segmentation 的定义：a task which segments foreground (FG) objects at the instance level as well as background (BG) contents at the semantic level</p>
<p>FG objects provide complementary cues to assist BG understanding</p>
<p>Attention-guided Unified Network (AUNet)</p>
<p>之前的方法 ignores the underlying relationship and fails to borrow rich contextual cues between things and stuff, e.g., people are more likely to stand on grass, instead of trees, although they often have similar appearances.</p>
<p>To facilitate in- formation flow between FG things and BG stuff</p>
<h4 id="Panoptic-Segmentation-的-发展"><a href="#Panoptic-Segmentation-的-发展" class="headerlink" title="Panoptic Segmentation 的 发展"></a>Panoptic Segmentation 的 发展</h4><p>[19] gave a bench- mark of panopic segmentation，应该也是第一次提出 Panoptic Segmentation 这个 Task</p>
<p>然后是 [20]，weakly-supervised</p>
<p>然后是 [8] </p>
<p>[19] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dolla ́r. Panoptic segmentation. arXiv:1801.00868, 2018.<br>[20] Q. Li, A. Arnab, and P. H. Torr. Weakly-and semi-supervised panoptic segmentation. In ECCV, 2018.<br>[8] D. de Geus, P. Meletis, and G. Dubbelman. Panoptic seg- mentation with a joint semantic and instance segmentation network. arXiv:1809.02110, 2018.</p>
<h3 id="Panoptic-Segmentation"><a href="#Panoptic-Segmentation" class="headerlink" title="Panoptic Segmentation"></a>Panoptic Segmentation</h3><p><strong>在这篇开创性的文章中，作者并没有提出关于全景分割的新算法，只是定义了新的问题</strong>，文章中关于全景分割的效果是通过将语义分割（PSPNet）与实例分割（Mask R-CNN）的结果联合在一起得到的</p>
<h3 id="Learning-Spatial-Context-Using-Stuff-to-Find-Things"><a href="#Learning-Spatial-Context-Using-Stuff-to-Find-Things" class="headerlink" title="Learning Spatial Context: Using Stuff to Find Things"></a>Learning Spatial Context: Using Stuff to Find Things</h3><p>第二作者是 Daphne Koller，就是写 PGM book 的 Koller</p>
<p>Heitz ECCV 2008<br>做目标检测的时候，我们人类是会利用 two primary signals</p>
<ol>
<li>The first is the <strong>local appearance</strong> in the window near the potential car. </li>
<li>The second is our knowledge that cars appear on roads. This second signal is a form of <strong>contextual knowledge</strong></li>
</ol>
<p>那么 Spatial Context 的关键就在于怎么去刻画 contextual knowledge 么？</p>
<p>对于 context 的重要性，下图说的太好了。如果仅仅用 local appearance，两个都会被认为是 cars，但是看见 context 之后，地下的那个人就不会认为是 car 了</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Heitz%20ECCV%202008%20Fig%202.png" alt=""></p>
<p>context 有 4 种:</p>
<ol>
<li><strong>Scene-Thing context</strong>，allows scene-level information, such as the scale or the “gist” [5], to determine location priors for objects</li>
<li><strong>Stuff-Stuff context</strong> captures the notion that sky occurs above sea and road below building</li>
<li><strong>Thing-Thing context</strong> considers the co-occurrence of objects， encoding, for example, that a tennis racket is more likely to occur with a tennis ball than with a lemon</li>
<li><strong>Stuff-Thing context</strong> allows the texture regions (e.g., the roads and buildings in Figure 1) to add predictive power to the detection of objects (e.g., the cars in Figure 1)</li>
</ol>
<p>刻画 context 的方式：</p>
<ol>
<li>刻画成 co-occurrence context，可以是 the presence of a certain object class in an image probabilistically influences the presence of a second class 也可以是 certain objects occur more frequently in cer- tain rooms, as monitors tend to occur in offices</li>
<li>一些方法去刻画 Spatial Context 的方式就是 detect objects using a descriptor with a large capture range, allowing the detection of the object to be influenced by surrounding image features</li>
<li>Another approach to modeling spatial relationships is to use a Markov Random Field (MRF) or variant (CRF,DRF) [14, 15] to encode the preferences for certain spatial relationships.</li>
</ol>
<h3 id="Context-Driven-Focus-of-Attention-for-Object-Detection"><a href="#Context-Driven-Focus-of-Attention-for-Object-Detection" class="headerlink" title="Context Driven Focus of Attention for Object Detection"></a>Context Driven Focus of Attention for Object Detection</h3><p>很有意思，作者来自于 Slovenia</p>
<p>WAPCV 2007</p>
<p>International Workshop on Attention in Cognitive Systems</p>
<p>In the proper context, humans can identify a given object in a scene, even if they would not normally recognize the same object when it is presented in isolation. </p>
<p>The limitation of a local appearance being too vague is resolved by using contextual information and by applying a reasoning mechanism to identify the object of interest.</p>
<p>刻画 context 的方式其实本质上是在刻画 a reasoning mechanism，co-occurrence context 中 reasoning 的逻辑就是 co-occurrence 的就符合上下文，不 co-occurrence 就不符合</p>
<p>context 的作用就在于拯救 <strong>local appearance is rather weak for a unique object recognition</strong></p>
<p>本文的 context 是用来 determine a focus of attention, that represents a prior for object detection，感觉像是 Scene-Thing context</p>
<p>In the field of <strong>cognitive attention for object detection</strong>, researchers usually decouple the contextual information processing from object detection. These can be <strong>performed in a cascade or calculated in parallel and fused in the final stage</strong>（分为 performed in a cascade，或者 calculated in parallel and fused in the final stage，也就是 cascade 串行 和 parallel 并行两种，前者就很像 RPN 了）. Using such an approach, any object detector can be combined with the contextual processing. Therefore, the related work is focused on how context can improve object detection rather than on object detection approaches. （本文想研究的是 context 来改善 Object Detection）</p>
<p>我现在明白 Context Driven Focus of Attention for Object Detection 这题目的意思了，作者要表达的是 context driven focus of attention for object detection，attention 或者说 Saliency Detection 具体怎么计算的，这里说是来自于 context</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Perko%20WAPCV%202007%20Fig%202.png" alt=""></p>
<p>Torralba and Oliva 是做 context 领域的大牛</p>
<p><strong>Scene-Thing context</strong> 用 gist of the scene 来刻画：The main idea is to categorize scenes based on the properties of the power spectrum of images. Semantic categories are extracted from the spectrum in order to grasp the so called gist of the scene. As the category of an image is determined, the average position of objects of interest within the image (e.g., a pedestrian, a car) is learned from a large database.</p>
<p>spatial context 的另一种刻画方式，extract a geometric context from a single image，extended classical object detection into 3D space by calculating a coarse viewpoint prior [18]. <strong>The knowledge of the viewpoint limits the search space for object detection</strong>, e.g. cars should not appear above the horizon. （the reasoning mechanism 就是 某些 Object 不会出现在某些区域）（这一点蛮有意思的，把 3D Vision 和 2D vision 结合起来了）</p>
<p><strong>how to depict the context </strong></p>
<p>这篇文章是把 context Information 作为 a focus of attention，用 context 来做 Region Proposal？ 说法是 Context provides cues of information about an object’s location within an image. 而通常 object detectors typically ignore this information. This context was then used to calculate the focus of attention, that presents a prior for object detection.</p>
<p>下图把这种 context Information 作为 a focus of attention 也就是 cues of information about an object’s location 展示的很明显。</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Perko%20WAPCV%202007%20Fig%206.png" alt=""></p>
<p><strong><em>我搞不懂的是 geometry 和 viewpoint 的区别在哪里？</em></strong></p>
<p>the viewpoint prior 是 the tilt angle of the camera’s orientation</p>
<p>the viewpoint prior, derived from a horizon estimate，We calculate the position of the horizon in the image, which is described by the tilt angle of the camera’s orientation </p>
<h3 id="Attentive-Contexts-for-Object-Detection"><a href="#Attentive-Contexts-for-Object-Detection" class="headerlink" title="Attentive Contexts for Object Detection"></a>Attentive Contexts for Object Detection</h3><h4 id="现有方法的局限："><a href="#现有方法的局限：" class="headerlink" title="现有方法的局限："></a>现有方法的局限：</h4><p>typically classify candidate proposals using their interior features</p>
<p>localize objects using only information within a specific proposal that may be insufficient for accurately <strong>detecting challenging objects</strong>, such as the ones with low resolution, small scale or heavy occlusion.</p>
<h4 id="context-的好处"><a href="#context-的好处" class="headerlink" title="context 的好处"></a>context 的好处</h4><p>global and local surrounding contexts that are believed to be valuable for object detection</p>
<p>extract and utilize contextual information to facilitate object detection</p>
<p>context 还分为 global contextual information 和  local context，自然就会有怎么去刻画 global context 和 local context 的问题</p>
<ol>
<li>how to identify useful global contextual information for detecting a certain object? </li>
<li>how to exploit local context surrounding a proposal for better inferring its contents?</li>
</ol>
<p>considerable improvements for object detection brought by exploiting contextual information</p>
<p> a global view on background of an image can provide useful con- textual information；if one wants to detect a specific car within a scene image, those objects such as person, road or another car that usually co-occur with the target may provide useful clues for detecting the target.</p>
<p>但是 incorporating meaningless background noise may even hurt the detection performance；那么关键问题就变成了 <strong>identifying useful contextual information</strong> is necessarily the first step towards effectively utilizing context for object detection 我怎么知道什么是 useful？</p>
<p>a local view into the neighborhood of one object proposal region can also provide some useful cues for inferring con- tents of a specific proposal. For example, surrounding environment (e.g. “road”) and discriminative part of the object (e.g. “wheels”) could benefit detecting the object (e.g. “car”) obviously</p>
<h4 id="related-works"><a href="#related-works" class="headerlink" title="related works"></a>related works</h4><p>Chen et al. [20] proposed a contextualized SVM model for complementary object detection and classification</p>
<p>[20] Chen, Q., Song, Z., Dong, J., Huang, Z., Hua, Y., Yan, S.: Contextualizing object detection and classification. IEEE Transactions on Pattern Recognition and<br>Machine Intelligence 37(1) (2015) 13–27</p>
<p>Bell et al. [21] presented a seminal work devoted to integrating contextual information into the Fast-RCNN framework. In [21], <strong>a spatial recurrent neural network</strong> was employed to <strong>model the contextual information of interest around proposals</strong>.</p>
<p>[21] Bell, S., Zitnick, C.L., Bala, K., Girshick, R.: <strong>Inside-outside net</strong>: Detecting objects in context with skip pooling and recurrent neural networks. arXiv preprint<br>arXiv:1512.04143 (2015)</p>
<p>问题：<strong>How to effectively model and integrate contextual information into current state-of-the-art detection pipelines</strong> is still a worthwhile problem yet has not been fully studied.</p>
<p><strong><em>incorporate the most discriminative global context into local object detection 具体是怎么实现的？</em></strong></p>
<p>global attention map 的作用：background noise that may hurt the detection performance is successfully suppressed on the attention map</p>
<p>说白了就是要 exploit cues from context</p>
<p>之前 RPN 虽然有多个多尺度的 Anchor，但每个 Anchor 都是独立经过 ROI Pooling 的；这个 Multi-scale contextualized sub-network 干的事情就是把 多个尺度的 Anchor（以同一点为中心、但 Anchor Size 不同）做完 ROI-Pooling 的 features concatenating 在一起，作者把这个叫做 Multi-scale contextualized sub-network，具体如下图所示。这么做背后的依据是觉得 local surrounding contexts 会 benefit object Detection 吧</p>
<p><img src="https://raw.githubusercontent.com/YimianDai/images/master/Jianan%20Li%20TMM%202017.png" alt=""></p>
<p>Global context 是怎么刻画的？a recurrent attention model including three LSTM layers is adopted to recurrently <strong>detect useful regions from a global view</strong> （怎么从 a global view 里来 detect useful regions 应该是靠 LSTM 来实现的吧）</p>
<p>算法流程：</p>
<ol>
<li>An image is first fed into a convolutional network to produce the feature cube 抽取共享的特征</li>
<li>Then the feature cub passes through the multi-scale contextualized sub-network for local context information extraction. <ul>
<li>The bounding box of each proposal is scaled with three pre-defined factors and feature representations from the bounding boxes are extracted by an RoI pooling layer. (Multi-scale ROI-Pooling，背后的 Motivation 是 Beyond the original bounding box of a specific proposal, two more scales used to exploit inside and outside contextual information are employed to enhance the feature representation.)</li>
<li>Each feature representation, after L2-normalization, concatenation, scaling, dimension-reduction, is then fed into two fully-connected layers （normalization, concatenation, scaling, dimension-reduction）</li>
</ul>
</li>
<li>In the attention-based contextualized sub-network, <ul>
<li>the feature cube is first pooled into a cube with a fixed scale （是对 shared conv 得到的整个 feature cube 做 pooling，理由是 Since input images usually have different sizes, shapes of feature cubes from the last convolutional layer are also different. To calculate the global context with consistent parameters, the feature cube is pooled into a fixed shape of K × K × D (20 × 20 × 512 in our experiments) ）</li>
<li>Then, a recurrent attention model including three LSTM layers is adopted to recurrently detect useful regions from a global view （<strong>这一块没看懂，框图里面的输入 lt, xt 分别是什么？</strong> $x_t$ 并不是 $K^2$ 中的第 1 个 tube， $x_t$ 具体的计算方式见公式（5）,$l_t$ 具体的计算方式见公式 (4), $l<em>t$ 是一个 K × K 大的 Saliency Map, $X</em>{t,i}$ 是什么？公式（5）很像是 Weighted 的 Global Average Pooling 啊，LSTM 负责计算 Weight Map, 直接乘上原来的 Features Cube，然后做 Global Averaging Pooling ，作为 Global context，concat 到 Region ROI 出来的</li>
</ul>
</li>
<li>Finally, a global context feature is pooled based on the calculated attention map and fed into two fully-connected layers.</li>
</ol>
<p>$I$ 是 input image，$p$ 是 one specific proposal<br>The proposal p is encoded by its size (w, h) and coordinates of its center $(c_x,c_y)$, i.e., $p = (c_x,c_y,w,h)$.</p>
<p>To exploit inside and outside contextual information of p, we crop p from the feature cube with three scaling factors: $λ_1 = 0.8, λ_2 = 1.2 and λ_3 = 1.8$. </p>
<p>denote the features of p pooled from crops of the feature cube with different sizes as ${v_p^i |i = 1, 2, 3}$</p>
<p><strong><em>没看懂，Based on the feature cube, the recurrent model learns an attention map with the shape of K2 to highlight the region that may benefit the object detection from the global view. 这句话在讲什么？</em></strong></p>
<p>Denote feature slices in the feature cube as X = [xi,···,xK2], where xi(i = 1,···,K2) is with D dimension. </p>
<p>At each time-step t, the attention model predicts a weighted map lt+1, a softmax over K×K locations. <strong>This is probabilistic estimation about whether the corresponding region in the input image is beneficial for the object classification from the global view.</strong></p>
<p> a K × K attentive location map is cal- culated to selectively combine the features of all locations into a 1 × 1 × D global attention-based feature. 这个怎么做到的？</p>
<h3 id="Inside-Outside-Net-Detecting-Objects-in-Context-With-Skip-Pooling-and-Recurrent-Neural-Networks"><a href="#Inside-Outside-Net-Detecting-Objects-in-Context-With-Skip-Pooling-and-Recurrent-Neural-Networks" class="headerlink" title="Inside-Outside Net: Detecting Objects in Context With Skip Pooling and Recurrent Neural Networks"></a>Inside-Outside Net: Detecting Objects in Context With Skip Pooling and Recurrent Neural Networks</h3><blockquote>
<p>在 F-RCNN 的框架下如何对特征进行增强，文章主要考虑了 multi-layer fusion 和 context 信息。</p>
<p>Contextual 和 multi-scale representations 对于视觉识别是很重要的。论文考虑结合 ROI 内外的有用信息，其中 ROI 外部 Contextual 信息的集成利用 spatial RNN。其四向 IRNN 结构如下，之后再将此 context 特征与局部特征结合：</p>
<p>ION（inside-outside-network）：这个工作的主要贡献有两个，第一个是如何在 Fast R-CNN 的基础之上增加 context 信息，所谓 context 在目标检测领域是指感兴趣的 ROI 周围的信息，可以是局部的，也可以是全局的。为此，作者提出了 IRNN 的概念，这也就是 outside-network。第二个贡献是所谓 skip-connection，通过将 deep ConvNet 的多层 ROI 特征进行提取和融合，利用该特征进行每一个位置的分类和进一步回归，这也就是 inside-network。</p>
</blockquote>
<p>探索怎么利用 two additional sources of information，一个是 a multi- scale representation，captures fine-grained details by pooling from multiple lower-level convolutional layers in a ConvNet。These skip-layers [36, 25, 26, 13] span multiple spatial resolutions and levels of feature abstraction. The information gained is especially important for small objects, which require the higher spatial resolution provided by lower-level layers. 另一个是 the use of contextual information.</p>
<p>一个纵向从 Multi-scale feature 上找，另一个横向从 context 上找</p>
<p>context and multi-scale, are complementary in na- ture. This matches our intuition that context features look broadly across the image, while multi-scale features capture more fine-grained details.</p>
<p>[3, 42] explore running an RNN spatially (or laterally) over a feature map in place of convolutions <strong><em>什么是 Spatial RNNs？</em></strong></p>
<p>[3] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene labeling with LSTM recurrent neural networks. In CVPR, 2015.<br>[42] F. Visin, K. Kastner, K. Cho, M. Matteucci, A. Courville, and Y. Bengio. ReNet: A recurrent neural network based alternative to convolutional networks. arXiv e-prints, arXiv:1505.00393 [cs.CV], 2015.</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Computer-Vision/" rel="tag"># Computer-Vision</a>
          
            <a href="/tags/Cognition/" rel="tag"># Cognition</a>
          
            <a href="/tags/Attention/" rel="tag"># Attention</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Biological-VSA/" rel="next" title="Biological Explanation on Visual Selective Attention">
                <i class="fa fa-chevron-left"></i> Biological Explanation on Visual Selective Attention
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Notes 2019-02/" rel="prev" title="Notes 2019-02">
                Notes 2019-02 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Yimian Dai</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">41</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">37</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#论文"><span class="nav-number">1.</span> <span class="nav-text">论文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-时空异常探测方法研究综述"><span class="nav-number">1.1.</span> <span class="nav-text">[1] 时空异常探测方法研究综述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是异常"><span class="nav-number">1.1.1.</span> <span class="nav-text">什么是异常</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#现有方法的局限性"><span class="nav-number">1.1.2.</span> <span class="nav-text">现有方法的局限性:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#传统异常探测方法"><span class="nav-number">1.1.3.</span> <span class="nav-text">传统异常探测方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-弱监督深层神经网络遥感图像目标检测模型"><span class="nav-number">1.2.</span> <span class="nav-text">[2] 弱监督深层神经网络遥感图像目标检测模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-人工智能的回顾与展望"><span class="nav-number">1.3.</span> <span class="nav-text">[3] 人工智能的回顾与展望</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习"><span class="nav-number">1.3.1.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#深度学习的基本动机"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">深度学习的基本动机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#深度学习的不足"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">深度学习的不足</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#类脑计算"><span class="nav-number">1.3.2.</span> <span class="nav-text">类脑计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习中引入记忆机制"><span class="nav-number">1.3.3.</span> <span class="nav-text">深度学习中引入记忆机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#智能制造"><span class="nav-number">1.3.4.</span> <span class="nav-text">智能制造</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重点资助方向"><span class="nav-number">1.3.5.</span> <span class="nav-text">重点资助方向</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是双清论坛？"><span class="nav-number">1.3.6.</span> <span class="nav-text">什么是双清论坛？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-人工智能中的推理-进展与挑战"><span class="nav-number">1.4.</span> <span class="nav-text">[4] 人工智能中的推理:进展与挑战</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是推理？"><span class="nav-number">1.4.1.</span> <span class="nav-text">什么是推理？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#因果推理"><span class="nav-number">1.4.2.</span> <span class="nav-text">因果推理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#记忆驱动的推理"><span class="nav-number">1.4.3.</span> <span class="nav-text">记忆驱动的推理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是逻辑学派"><span class="nav-number">1.4.4.</span> <span class="nav-text">什么是逻辑学派</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是知识工程学派"><span class="nav-number">1.4.5.</span> <span class="nav-text">什么是知识工程学派</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-人工智能的未来-–-神经科学启发的类脑计算综述"><span class="nav-number">1.5.</span> <span class="nav-text">[5] 人工智能的未来 – 神经科学启发的类脑计算综述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Neurobiology-and-Computational-Neuroscience"><span class="nav-number">1.5.1.</span> <span class="nav-text">Neurobiology and Computational Neuroscience</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#人脑强大但低功耗"><span class="nav-number">1.5.2.</span> <span class="nav-text">人脑强大但低功耗</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三代人工神经网络"><span class="nav-number">1.5.3.</span> <span class="nav-text">三代人工神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#类脑计算的研究趋势"><span class="nav-number">1.5.4.</span> <span class="nav-text">类脑计算的研究趋势</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-史忠植：人工智能-第十二章-类脑智能"><span class="nav-number">1.6.</span> <span class="nav-text">[6] 史忠植：人工智能 第十二章 类脑智能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-李德毅院士：从脑认知到人工智能"><span class="nav-number">1.7.</span> <span class="nav-text">[7] 李德毅院士：从脑认知到人工智能</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#从功能上逼近脑"><span class="nav-number">1.7.1.</span> <span class="nav-text">从功能上逼近脑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#记忆认知"><span class="nav-number">1.7.2.</span> <span class="nav-text">记忆认知</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重新认识卷积（利用卷积表达认知和记忆）"><span class="nav-number">1.7.3.</span> <span class="nav-text">重新认识卷积（利用卷积表达认知和记忆）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交互认知"><span class="nav-number">1.7.4.</span> <span class="nav-text">交互认知</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络的三大局限"><span class="nav-number">1.7.5.</span> <span class="nav-text">卷积神经网络的三大局限</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#脑认知单元-vs-图灵模型"><span class="nav-number">1.7.6.</span> <span class="nav-text">脑认知单元 vs 图灵模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#脑认知形式化的尺度选择"><span class="nav-number">1.7.7.</span> <span class="nav-text">脑认知形式化的尺度选择</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-郑南宁-人工智能的下一步是什么"><span class="nav-number">1.8.</span> <span class="nav-text">[8] 郑南宁:人工智能的下一步是什么?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#非完整信息处理问题"><span class="nav-number">1.8.1.</span> <span class="nav-text">非完整信息处理问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-脑科学与类脑研究概述"><span class="nav-number">1.9.</span> <span class="nav-text">[9] 脑科学与类脑研究概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Minsky-干了神经网络两次"><span class="nav-number">1.9.1.</span> <span class="nav-text">Minsky 干了神经网络两次</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图灵机计算的本质"><span class="nav-number">1.9.2.</span> <span class="nav-text">图灵机计算的本质</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#脑科学与类脑研究"><span class="nav-number">1.9.3.</span> <span class="nav-text">脑科学与类脑研究</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-人工智能中的联结主义和符号主义"><span class="nav-number">1.10.</span> <span class="nav-text">[10] 人工智能中的联结主义和符号主义</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#符号主义"><span class="nav-number">1.10.1.</span> <span class="nav-text">符号主义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#海马体与梦"><span class="nav-number">1.10.2.</span> <span class="nav-text">海马体与梦</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#高级中枢向低级中枢反馈"><span class="nav-number">1.10.3.</span> <span class="nav-text">高级中枢向低级中枢反馈</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#层次特征"><span class="nav-number">1.10.4.</span> <span class="nav-text">层次特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-视觉选择性注意的模型化计算及其应用前景"><span class="nav-number">1.11.</span> <span class="nav-text">[11] 视觉选择性注意的模型化计算及其应用前景</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是注意？"><span class="nav-number">1.11.1.</span> <span class="nav-text">什么是注意？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Marr-视觉理论"><span class="nav-number">1.11.2.</span> <span class="nav-text">Marr 视觉理论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#视觉信息处理通路"><span class="nav-number">1.11.3.</span> <span class="nav-text">视觉信息处理通路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VSA-模型化计算"><span class="nav-number">1.11.4.</span> <span class="nav-text">VSA 模型化计算</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#VSA-模型化计算的关键环节"><span class="nav-number">1.11.4.1.</span> <span class="nav-text">VSA 模型化计算的关键环节</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#自下而上兴趣图"><span class="nav-number">1.11.4.2.</span> <span class="nav-text">自下而上兴趣图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#物体知识的表达"><span class="nav-number">1.11.4.3.</span> <span class="nav-text">物体知识的表达</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#在目标检测中的应用"><span class="nav-number">1.11.4.4.</span> <span class="nav-text">在目标检测中的应用</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#值得研究的问题"><span class="nav-number">1.11.5.</span> <span class="nav-text">值得研究的问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-刘成林-从模式识别到类脑研究"><span class="nav-number">1.12.</span> <span class="nav-text">[12] 刘成林:从模式识别到类脑研究</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#模式识别"><span class="nav-number">1.12.1.</span> <span class="nav-text">模式识别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#类脑智能"><span class="nav-number">1.12.2.</span> <span class="nav-text">类脑智能</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-深度学习-多层神经网络的复兴与变革"><span class="nav-number">1.13.</span> <span class="nav-text">[13] 深度学习:多层神经网络的复兴与变革</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习成功的启示"><span class="nav-number">1.13.1.</span> <span class="nav-text">深度学习成功的启示</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#优化方法的变革是开启深度学习复兴之门的钥匙"><span class="nav-number">1.13.1.1.</span> <span class="nav-text">优化方法的变革是开启深度学习复兴之门的钥匙</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#从经验驱动的人造特征范式到数据驱动的表示学习范式"><span class="nav-number">1.13.1.2.</span> <span class="nav-text">从经验驱动的人造特征范式到数据驱动的表示学习范式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#从“分步分治”到“端到端的学习”"><span class="nav-number">1.13.1.3.</span> <span class="nav-text">从“分步分治”到“端到端的学习”</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#脑神经科学启发的思路值得更多的重视"><span class="nav-number">1.13.1.4.</span> <span class="nav-text">脑神经科学启发的思路值得更多的重视</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何赋予机器演绎推理能力"><span class="nav-number">1.13.2.</span> <span class="nav-text">如何赋予机器演绎推理能力</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-从脑网络到人工智能-——-类脑计算的机遇与挑战"><span class="nav-number">1.14.</span> <span class="nav-text">[14] 从脑网络到人工智能 —— 类脑计算的机遇与挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-人工智能在军事领域的渗透与应用思考"><span class="nav-number">1.15.</span> <span class="nav-text">[15] 人工智能在军事领域的渗透与应用思考</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#人工智能的三个层次"><span class="nav-number">1.15.1.</span> <span class="nav-text">人工智能的三个层次</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-感知智能到认知智能中对知识的思考"><span class="nav-number">1.16.</span> <span class="nav-text">[16] 感知智能到认知智能中对知识的思考</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-机器学习-现在与未来"><span class="nav-number">1.17.</span> <span class="nav-text">[17] 机器学习: 现在与未来</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-小样本的类人概念学习与大数据的深度强化学习"><span class="nav-number">1.18.</span> <span class="nav-text">[18] 小样本的类人概念学习与大数据的深度强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-人工智能-“热闹”背后的“门道”"><span class="nav-number">1.19.</span> <span class="nav-text">[19] 人工智能: “热闹”背后的“门道”</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#人类智慧、人类智能、人工智能"><span class="nav-number">1.19.1.</span> <span class="nav-text">人类智慧、人类智能、人工智能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AI-时代的工作方式"><span class="nav-number">1.19.2.</span> <span class="nav-text">AI 时代的工作方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#20-人工智能研究的三大流派-比较与启示"><span class="nav-number">1.20.</span> <span class="nav-text">[20] 人工智能研究的三大流派: 比较与启示</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#符号主义-1"><span class="nav-number">1.20.1.</span> <span class="nav-text">符号主义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#联结主义"><span class="nav-number">1.20.2.</span> <span class="nav-text">联结主义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#行为主义"><span class="nav-number">1.20.3.</span> <span class="nav-text">行为主义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三大流派的比较"><span class="nav-number">1.20.4.</span> <span class="nav-text">三大流派的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#归纳和演绎"><span class="nav-number">1.20.5.</span> <span class="nav-text">归纳和演绎</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#21-类脑智能研究的回顾与展望"><span class="nav-number">1.21.</span> <span class="nav-text">[21] 类脑智能研究的回顾与展望</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#图灵机模型和冯·诺依曼计算机体系结构"><span class="nav-number">1.21.1.</span> <span class="nav-text">图灵机模型和冯·诺依曼计算机体系结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#类脑智能-1"><span class="nav-number">1.21.2.</span> <span class="nav-text">类脑智能</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#什么是类脑智能？"><span class="nav-number">1.21.2.1.</span> <span class="nav-text">什么是类脑智能？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#为什么是借鉴而非模仿脑？"><span class="nav-number">1.21.2.2.</span> <span class="nav-text">为什么是借鉴而非模仿脑？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#认知科学中的类脑智能研究"><span class="nav-number">1.21.2.3.</span> <span class="nav-text">认知科学中的类脑智能研究</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#计算神经学中的类脑智能研究"><span class="nav-number">1.21.2.4.</span> <span class="nav-text">计算神经学中的类脑智能研究</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#传统计算神经学与类脑智能的区别"><span class="nav-number">1.21.2.4.1.</span> <span class="nav-text">传统计算神经学与类脑智能的区别</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#人工智能中的类脑智能研究"><span class="nav-number">1.21.2.5.</span> <span class="nav-text">人工智能中的类脑智能研究</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#认知脑计算模型的构建"><span class="nav-number">1.21.3.</span> <span class="nav-text">认知脑计算模型的构建</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#认知脑计算模型研究内容"><span class="nav-number">1.21.3.1.</span> <span class="nav-text">认知脑计算模型研究内容</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#认知脑计算模型-和-类脑信息处理-的区别？"><span class="nav-number">1.21.3.2.</span> <span class="nav-text">认知脑计算模型 和 类脑信息处理 的区别？</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#类脑信息处理"><span class="nav-number">1.21.4.</span> <span class="nav-text">类脑信息处理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#研究内容："><span class="nav-number">1.21.4.1.</span> <span class="nav-text">研究内容：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#我感兴趣的内容"><span class="nav-number">1.21.4.2.</span> <span class="nav-text">我感兴趣的内容</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#22-类脑计算芯片与类脑智能机器人发展现状与思考"><span class="nav-number">1.22.</span> <span class="nav-text">[22] 类脑计算芯片与类脑智能机器人发展现状与思考</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#类脑芯片研究的两大方向"><span class="nav-number">1.22.1.</span> <span class="nav-text">类脑芯片研究的两大方向</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#方向一：神经形态芯片"><span class="nav-number">1.22.1.1.</span> <span class="nav-text">方向一：神经形态芯片</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#方向二："><span class="nav-number">1.22.1.2.</span> <span class="nav-text">方向二：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#冯诺依曼架构-vs-大脑架构"><span class="nav-number">1.22.2.</span> <span class="nav-text">冯诺依曼架构 vs 大脑架构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#23-国务院关于印发新一代人工智能发展规划的通知"><span class="nav-number">1.23.</span> <span class="nav-text">[23] 国务院关于印发新一代人工智能发展规划的通知</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#适合我的人工智能关键共性技术"><span class="nav-number">1.23.1.</span> <span class="nav-text">适合我的人工智能关键共性技术</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#24-谈方法论：归纳与统计"><span class="nav-number">1.24.</span> <span class="nav-text">[24] 谈方法论：归纳与统计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#归纳"><span class="nav-number">1.24.1.</span> <span class="nav-text">归纳</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#统计"><span class="nav-number">1.24.2.</span> <span class="nav-text">统计</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#25-人工智能-天使还是魔鬼"><span class="nav-number">1.25.</span> <span class="nav-text">[25] 人工智能: 天使还是魔鬼?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#26-类脑智能研究现状与发展思考"><span class="nav-number">1.26.</span> <span class="nav-text">[26] 类脑智能研究现状与发展思考</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#以往的人工智能"><span class="nav-number">1.26.1.</span> <span class="nav-text">以往的人工智能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#脑与神经科学对人工智能的潜在启发"><span class="nav-number">1.26.2.</span> <span class="nav-text">脑与神经科学对人工智能的潜在启发</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#脑的多尺度结构"><span class="nav-number">1.26.2.1.</span> <span class="nav-text">脑的多尺度结构</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#新一代人工神经网络模型"><span class="nav-number">1.26.3.</span> <span class="nav-text">新一代人工神经网络模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于记忆、推理和注意的认知"><span class="nav-number">1.26.4.</span> <span class="nav-text">基于记忆、推理和注意的认知</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#关于类脑智能模型的进一步思考"><span class="nav-number">1.26.5.</span> <span class="nav-text">关于类脑智能模型的进一步思考</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#目前脉冲神经网络的缺陷"><span class="nav-number">1.26.5.1.</span> <span class="nav-text">目前脉冲神经网络的缺陷</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#27-类脑-受脑启发的-计算的问题与视觉认知"><span class="nav-number">1.27.</span> <span class="nav-text">[27] 类脑(受脑启发的)计算的问题与视觉认知</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#目前深度学习模型的缺点："><span class="nav-number">1.27.0.1.</span> <span class="nav-text">目前深度学习模型的缺点：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#图灵机的局限（判符号主义死刑）"><span class="nav-number">1.27.0.2.</span> <span class="nav-text">图灵机的局限（判符号主义死刑）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#大脑认知的层次"><span class="nav-number">1.27.0.3.</span> <span class="nav-text">大脑认知的层次</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#传统人工智能的局限性"><span class="nav-number">1.27.0.4.</span> <span class="nav-text">传统人工智能的局限性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#冯诺依曼计算架构的面临的困境"><span class="nav-number">1.27.0.5.</span> <span class="nav-text">冯诺依曼计算架构的面临的困境</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#大脑网络连接与认知的关系"><span class="nav-number">1.27.1.</span> <span class="nav-text">大脑网络连接与认知的关系</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#大脑的结构连接"><span class="nav-number">1.27.1.1.</span> <span class="nav-text">大脑的结构连接</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#大脑的功能连接"><span class="nav-number">1.27.1.2.</span> <span class="nav-text">大脑的功能连接</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#大脑的记忆"><span class="nav-number">1.27.2.</span> <span class="nav-text">大脑的记忆</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#神经记忆的特征"><span class="nav-number">1.27.2.1.</span> <span class="nav-text">神经记忆的特征</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#人脑强悍的计算能力"><span class="nav-number">1.27.2.2.</span> <span class="nav-text">人脑强悍的计算能力</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#三种类脑-受脑启发的-认知计算模型"><span class="nav-number">1.27.2.3.</span> <span class="nav-text">三种类脑(受脑启发的)认知计算模型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#视觉计算"><span class="nav-number">1.27.3.</span> <span class="nav-text">视觉计算</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#视觉交互行为与注意力集中"><span class="nav-number">1.27.3.1.</span> <span class="nav-text">视觉交互行为与注意力集中</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#深度学习的问题"><span class="nav-number">1.27.3.2.</span> <span class="nav-text">深度学习的问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#视觉认知中的深度学习层次结构"><span class="nav-number">1.27.3.3.</span> <span class="nav-text">视觉认知中的深度学习层次结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#现有视觉计算架构的局限"><span class="nav-number">1.27.3.4.</span> <span class="nav-text">现有视觉计算架构的局限</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#脑启发的视觉处理计算架构"><span class="nav-number">1.27.3.5.</span> <span class="nav-text">脑启发的视觉处理计算架构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#28-机载激光雷达技术在长江三峡工程库区滑坡灾害调查和监测中的应用研究"><span class="nav-number">1.28.</span> <span class="nav-text">[28] 机载激光雷达技术在长江三峡工程库区滑坡灾害调查和监测中的应用研究</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#29-基于地面-LiDAR-的滑坡地表整体变形监测"><span class="nav-number">1.29.</span> <span class="nav-text">[29] 基于地面 LiDAR 的滑坡地表整体变形监测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要做点云匹配？"><span class="nav-number">1.29.1.</span> <span class="nav-text">为什么要做点云匹配？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#30-SuperPoint-Self-Supervised-Interest-Point-Detection-and-Description"><span class="nav-number">1.30.</span> <span class="nav-text">[30] SuperPoint: Self-Supervised Interest Point Detection and Description</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivation"><span class="nav-number">1.30.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model"><span class="nav-number">1.30.2.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss"><span class="nav-number">1.30.3.</span> <span class="nav-text">Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#interest-point-detector-loss"><span class="nav-number">1.30.3.1.</span> <span class="nav-text">interest point detector loss</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#descriptor-loss"><span class="nav-number">1.30.3.2.</span> <span class="nav-text">descriptor loss</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#32-Single-Shot-Object-Detection-with-Enriched-Semantics"><span class="nav-number">1.31.</span> <span class="nav-text">[32] Single-Shot Object Detection with Enriched Semantics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivation-1"><span class="nav-number">1.31.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-1"><span class="nav-number">1.31.2.</span> <span class="nav-text">Model</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Segmentation-branch"><span class="nav-number">1.31.2.1.</span> <span class="nav-text">Segmentation branch</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-1"><span class="nav-number">1.31.3.</span> <span class="nav-text">Loss</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#33-Residual-Attention-Network-for-Image-Classification"><span class="nav-number">1.32.</span> <span class="nav-text">[33] Residual Attention Network for Image Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivation-2"><span class="nav-number">1.32.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-2"><span class="nav-number">1.32.2.</span> <span class="nav-text">Model</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Attention-Module"><span class="nav-number">1.32.2.1.</span> <span class="nav-text">Attention Module</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Attention-Residual-Learning"><span class="nav-number">1.32.2.2.</span> <span class="nav-text">Attention Residual Learning</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Spatial-Attention-and-Channel-Attention"><span class="nav-number">1.32.2.3.</span> <span class="nav-text">Spatial Attention and Channel Attention</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-2"><span class="nav-number">1.32.3.</span> <span class="nav-text">Loss</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#34-Semantic-Point-Detector"><span class="nav-number">1.33.</span> <span class="nav-text">[34] Semantic Point Detector</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivation-3"><span class="nav-number">1.33.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-3"><span class="nav-number">1.33.2.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-3"><span class="nav-number">1.33.3.</span> <span class="nav-text">Loss</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#35-图像物体分类与检测算法综述"><span class="nav-number">1.34.</span> <span class="nav-text">[35] 图像物体分类与检测算法综述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于词包模型的物体分类"><span class="nav-number">1.34.1.</span> <span class="nav-text">基于词包模型的物体分类</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Step-1-底层特征提取"><span class="nav-number">1.34.1.1.</span> <span class="nav-text">Step 1: 底层特征提取</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Step-2-特征编码"><span class="nav-number">1.34.1.2.</span> <span class="nav-text">Step 2: 特征编码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#向量量化编码"><span class="nav-number">1.34.1.2.1.</span> <span class="nav-text">向量量化编码</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#软量化编码"><span class="nav-number">1.34.1.2.2.</span> <span class="nav-text">软量化编码</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#稀疏编码"><span class="nav-number">1.34.1.2.3.</span> <span class="nav-text">稀疏编码</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#局部线性约束编码"><span class="nav-number">1.34.1.2.4.</span> <span class="nav-text">局部线性约束编码</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#显著性编码"><span class="nav-number">1.34.1.2.5.</span> <span class="nav-text">显著性编码</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Fisher-向量编码"><span class="nav-number">1.34.1.2.6.</span> <span class="nav-text">Fisher 向量编码</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Step-3-空间特征汇聚"><span class="nav-number">1.34.1.3.</span> <span class="nav-text">Step 3: 空间特征汇聚</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#空间金字塔匹配"><span class="nav-number">1.34.1.3.1.</span> <span class="nav-text">空间金字塔匹配</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习模型"><span class="nav-number">1.34.2.</span> <span class="nav-text">深度学习模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#词包模型与-CNN-的异同"><span class="nav-number">1.34.2.1.</span> <span class="nav-text">词包模型与 CNN 的异同</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#物体检测"><span class="nav-number">1.34.3.</span> <span class="nav-text">物体检测</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#结构化学习"><span class="nav-number">1.34.3.1.</span> <span class="nav-text">结构化学习</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#物体检测和物体分类的统一性"><span class="nav-number">1.34.4.</span> <span class="nav-text">物体检测和物体分类的统一性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#物体检测可以取代物体分类"><span class="nav-number">1.34.4.1.</span> <span class="nav-text">物体检测可以取代物体分类?</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#物体检测和物体分类之间的差异性和互补性"><span class="nav-number">1.34.4.2.</span> <span class="nav-text">物体检测和物体分类之间的差异性和互补性</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#物体分类与检测的发展方向"><span class="nav-number">1.34.5.</span> <span class="nav-text">物体分类与检测的发展方向</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#层次化学习-深度学习-存在的难点与挑战"><span class="nav-number">1.34.6.</span> <span class="nav-text">层次化学习(深度学习)存在的难点与挑战</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#36-Biological-Explanation-on-Visual-Selective-Attention"><span class="nav-number">1.35.</span> <span class="nav-text">[36] Biological Explanation on Visual Selective Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Biological-Explanation"><span class="nav-number">1.35.1.</span> <span class="nav-text">Biological Explanation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computational-Approximation"><span class="nav-number">1.35.2.</span> <span class="nav-text">Computational Approximation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对目标检测的启示"><span class="nav-number">1.35.3.</span> <span class="nav-text">对目标检测的启示</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#To-Read-List"><span class="nav-number">2.</span> <span class="nav-text">To Read List</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Image-Homography-Estimation"><span class="nav-number">2.1.</span> <span class="nav-text">Deep Image Homography Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discriminative-learning-of-deep-convolutional-feature-point-descriptors"><span class="nav-number">2.2.</span> <span class="nav-text">Discriminative learning of deep convolutional feature point descriptors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3DMatch-Learning-Local-Geometric-Descriptors-from-RGB-D-Reconstructions"><span class="nav-number">2.3.</span> <span class="nav-text">3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discriminative-learning-of-deep-convolutional-feature-point-descriptors-1"><span class="nav-number">2.4.</span> <span class="nav-text">Discriminative learning of deep convolutional feature point descriptors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quad-networks-unsupervised-learning-to-rank-for-interest-point-detection"><span class="nav-number">2.5.</span> <span class="nav-text">Quad-networks: unsupervised learning to rank for interest point detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LIFT-Learned-Invariant-Feature-Transform"><span class="nav-number">2.6.</span> <span class="nav-text">LIFT: Learned Invariant Feature Transform</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LF-Net-Learning-Local-Features-from-Images"><span class="nav-number">2.7.</span> <span class="nav-text">LF-Net: Learning Local Features from Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scale-Aware-Trident-Networks-for-Object-Detection"><span class="nav-number">2.8.</span> <span class="nav-text">Scale-Aware Trident Networks for Object Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Unified-Multi-scale-Deep-Convolutional-Neural-Network-for-Fast-Object-Detection"><span class="nav-number">2.9.</span> <span class="nav-text">A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-guided-Unified-Network-for-Panoptic-Segmentation"><span class="nav-number">2.10.</span> <span class="nav-text">Attention-guided Unified Network for Panoptic Segmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Panoptic-Segmentation-的-发展"><span class="nav-number">2.10.1.</span> <span class="nav-text">Panoptic Segmentation 的 发展</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Panoptic-Segmentation"><span class="nav-number">2.11.</span> <span class="nav-text">Panoptic Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Spatial-Context-Using-Stuff-to-Find-Things"><span class="nav-number">2.12.</span> <span class="nav-text">Learning Spatial Context: Using Stuff to Find Things</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Context-Driven-Focus-of-Attention-for-Object-Detection"><span class="nav-number">2.13.</span> <span class="nav-text">Context Driven Focus of Attention for Object Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attentive-Contexts-for-Object-Detection"><span class="nav-number">2.14.</span> <span class="nav-text">Attentive Contexts for Object Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#现有方法的局限："><span class="nav-number">2.14.1.</span> <span class="nav-text">现有方法的局限：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#context-的好处"><span class="nav-number">2.14.2.</span> <span class="nav-text">context 的好处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#related-works"><span class="nav-number">2.14.3.</span> <span class="nav-text">related works</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inside-Outside-Net-Detecting-Objects-in-Context-With-Skip-Pooling-and-Recurrent-Neural-Networks"><span class="nav-number">2.15.</span> <span class="nav-text">Inside-Outside Net: Detecting Objects in Context With Skip Pooling and Recurrent Neural Networks</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yimian Dai</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.4.4</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  


  


  





  

  

  

  

  
  

  
  

  


  

  

  

  

  

  

  

  

</body>
</html>
