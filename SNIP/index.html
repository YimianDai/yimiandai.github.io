<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"/><meta content="yes" name="apple-mobile-web-app-capable"/><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"/><meta content="telephone=no" name="format-detection"/><meta name="description"/><title>Notes on Scale Normalization for Image Pyramids (SNIP) | Grok</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"/><link rel="stylesheet" type="text/css" href="/css/highlight.css"/><link rel="stylesheet" type="text/css" href="/css/font.css"/><link rel="stylesheet" type="text/css" href="/css/noise.css"/><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"/><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"/></head><body><article class="wrapper"><div class="post-main"><div class="nav"><nav class="container"><a class="sidebar-nav-item active" href="/">Home</a><a class="sidebar-nav-item" href="/archives">Archives</a></nav><div class="container post-meta"><div class="post-tags"><a class="post-tag-link" href="/tags/Computer-Vision/">Computer-Vision</a><a class="post-tag-link" href="/tags/Convolutional-Network/">Convolutional-Network</a><a class="post-tag-link" href="/tags/Deep-Learning/">Deep-Learning</a><a class="post-tag-link" href="/tags/Machine-Learning/">Machine-Learning</a><a class="post-tag-link" href="/tags/Object-Detection/">Object-Detection</a><a class="post-tag-link" href="/tags/Supervised-Learning/">Supervised-Learning</a></div><div class="post-time">2018-08-19</div></div></div><div class="container post-header"><h1>Notes on Scale Normalization for Image Pyramids (SNIP)</h1></div><div class="container post-content"><p>è¿™ç¯‡æ—¥å¿—è®°å½•ä¸€äº›å¯¹ä¸‹é¢è¿™ç¯‡ CVPR 2018 Oral æ–‡ç« çš„ç¬”è®°ã€‚</p>
<blockquote>
<p>Singh B, Davis L S. An Analysis of Scale Invariance in Object Detectionâ€“SNIP[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3578-3587.<br>è®ºæ–‡é“¾æ¥ï¼š<a href="https://arxiv.org/abs/1711.08189" target="_blank" rel="noopener">https://arxiv.org/abs/1711.08189</a><br>ä»£ç é“¾æ¥ï¼š<a href="https://github.com/bharatsingh430/snip" target="_blank" rel="noopener">https://github.com/bharatsingh430/snip</a></p>
</blockquote>
<h2 id="è®ºç‚¹"><a href="#è®ºç‚¹" class="headerlink" title="è®ºç‚¹"></a>è®ºç‚¹</h2><p>è®ºæ–‡ä¸€å¼€å§‹ä½œè€…æ‘†äº†ä¸ªäº‹å®ï¼Œå¯¹äº Image Classification å·²ç»èƒ½å¤Ÿåšåˆ° super-human level performance äº†ï¼Œä½†æ˜¯ Object Detection è¿˜å·®å¾—å¾ˆè¿œå¾ˆè¿œï¼Œæ‰€ä»¥ä½œè€…é—®äº†ä¸€ä¸ªé—®é¢˜ï¼š<strong>Why is object detection so much harder than image classification?</strong></p>
<p>ä½œè€…ç»™å‡ºçš„è§£é‡Šæ˜¯ <strong>Large scale variation across object instances</strong>ï¼Œè¿™ä¸ª scale variation ä¸ä»…ä»…å­˜åœ¨äºè¦ apply çš„ target dataset è‡ªèº«å†…éƒ¨ï¼Œè¿˜å­˜åœ¨ä¸ pre-trained dataset å’Œè¦ apply çš„ target dataset ä¹‹é—´ã€‚</p>
<ul>
<li>å¯¹äºè¦ apply çš„ target dataset è‡ªèº«å†…éƒ¨çš„ extreme scale variationï¼Œä½œè€…ç»™äº†ä¸‹é¢è¿™å¼ å›¾æ¥è¯´æ˜ã€‚çºµåæ ‡çš„ CDF æ˜¯ Cumulative Distribution Functionï¼Œç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼›è¿™ä¸ª Relative Scale åº”è¯¥å°±æ˜¯ Object åº”è¯¥æ˜¯é•¿æˆ–å®½ï¼Œå æ®å›¾åƒçš„é•¿æˆ–å®½çš„æ¯”ä¾‹ã€‚ä»è¿™å¼ å›¾ä¸Šèƒ½çœ‹å‡ºï¼ŒCOCO çš„å¤§éƒ¨åˆ†ç›®æ ‡é›†ä¸­åœ¨ relative scale 0.1 ä¹‹ä¸‹ï¼Œé¢ç§¯å°äº 1 %ã€‚è¿™é‡Œå…¶å®æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š<ul>
<li>ä¸€ä¸ªæ˜¯ç›®æ ‡æœ¬èº«å¾ˆå°ï¼Œæ€ä¹ˆæ ·æ‰èƒ½æ¯”è¾ƒå¥½çš„ç‰¹å¾è¡¨ç¤ºå°ç›®æ ‡ï¼Œä¹Ÿå°±æ˜¯è®© CNN æœ¬èº«èƒ½æ£€æµ‹å‡ºå°ç›®æ ‡ã€‚</li>
<li>å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå› ä¸º COCO é‡Œé¢å¤§éƒ¨åˆ†éƒ½æ˜¯å°ç›®æ ‡ï¼Œè€Œå°ç›®æ ‡å› ä¸ºå¾ˆå°ï¼Œæ‰€ä»¥å½¼æ­¤ä¹‹é—´å°ºåº¦çš„å€æ•°å…¶å®å¾ˆå¤§ï¼Œscale ä¸º 0.0001 çš„å’Œ scale ä¸º 0.1 çš„ä¹‹é—´å·®äº† 1000 å€ï¼Œä½†å› ä¸ºä¸€åŠçš„ç›®æ ‡éƒ½é›†ä¸­åœ¨ 0.1 ä¸€ä¸‹ï¼Œæ‰€ä»¥ç‰¹åˆ«å°çš„å°ºåº¦çš„å…¶å®ä¹Ÿæœ‰å¤§é‡ç›®æ ‡ï¼Œå¹¶ä¸èƒ½è¢«å¿½ç•¥ã€‚ä¹Ÿå°±æ˜¯è¯´æ˜¯å¤§é‡ã€æ•°ç›®ä¸å¯å¿½ç•¥çš„ very small objects çš„å­˜åœ¨ï¼Œä½¿å¾— object detection æ•°æ®é›†ä¸Šçš„ scale variation å¾ˆå¤§ã€‚æ‰€ä»¥å¯¹äº COCOï¼Œå°±è¦æ±‚ CNN å¿…é¡»è¦æœ‰åœ¨æå°çš„ scale å’Œ å¾ˆå¤§çš„ scale ä¸Šï¼ˆè¿™ä¸¤è€…ä¹‹é—´çš„æ¯”ä¾‹å€¼å¾ˆå¤§ï¼Œæ¯”å¦‚ 0.0001 vs 0.9ï¼‰ä¹‹é—´çš„ç›®æ ‡éƒ½æœ‰å¾ˆå¥½çš„åˆ†ç±»èƒ½åŠ›æ‰ä¼šæœ‰å¾ˆå¥½çš„æ€§èƒ½ï¼Œä¹Ÿå°±è¦è¦æœ‰å¯¹ extreme scale variation çš„é²æ£’æ€§ï¼Œå³ scale-invariantã€‚</li>
</ul>
</li>
</ul>
<p><img src="http://ohm5uq281.bkt.clouddn.com/2018-08-16-15344139030795.jpg" alt=""></p>
<ul>
<li>å¯¹äº pre-trained dataset å’Œè¦ apply çš„ target dataset ä¹‹é—´çš„ scale variationï¼Œä½œè€…ç»™äº† domain-shift è¿™ä¸ªåè¯æ¥å½¢å®¹ã€‚ImageNet æ˜¯ç”¨æ¥å›¾åƒåˆ†ç±»çš„ï¼Œç›®æ ‡ä¸€èˆ¬ scale æ¯”è¾ƒå¤§ï¼Œè€Œ object detection æ•°æ®é›†ä¸­çš„ç›®æ ‡çš„ scale å·®å¼‚å°±å¾ˆå¤§äº†ã€‚åœ¨ ImageNet è¿™ç§å¤§ç›®æ ‡æ•°æ®é›†ä¸Š pre-trained çš„ featuresï¼Œç›´æ¥ç”¨åœ¨æ£€æµ‹ object detection ä¸­çš„é‚£äº›å°ç›®æ ‡ï¼Œå¯ä»¥é¢„æœŸåˆ°æ•ˆæœå¹¶ä¸ä¼šå¾ˆå¥½ï¼Œè¿™å°±æ˜¯ domain-shift é€ æˆçš„ã€‚</li>
</ul>
<p>å½’æ ¹åˆ°åº•ï¼Œobject detection ç›®å‰åšä¸å¥½ï¼Œè¿˜æ˜¯å› ä¸ºæœ‰å¤§é‡ very small objects å­˜åœ¨çš„æœ¬èº«ï¼Œè€Œ small objects æ£€æµ‹å¾ˆéš¾ï¼Œå› ä¸ºï¼š</p>
<ol>
<li>small objects å› ä¸º smallï¼Œ<strong>å†…éƒ¨ scale å°±å·®å¼‚å¾ˆå¤§</strong>ï¼ˆå€æ•°ï¼Œå› ä¸ºåˆ†æ¯å¾ˆå°ï¼Œä¸€é™¤å°±ä¼šå¾ˆå¤§ï¼‰ï¼Œæ£€æµ‹å™¨éœ€è¦å¾ˆå¼ºçš„ scale-invariance èƒ½åŠ›ï¼Œè€Œ CNN å°±å…¶è®¾è®¡æœ¬èº«å…¶å®æ˜¯æ²¡æœ‰ scale-invariance çš„ï¼›</li>
<li>small objects æœ¬èº« smallï¼Œåœ¨ ImageNet è¿™æ · Median Scale Objects ä¸ºä¸»çš„ Datasets ä¸Š Pre-trained çš„ Features ç›´æ¥ç”¨æ¥ Detect Small Objects æ•ˆæœä¸å¥½ï¼Œå› ä¸º <strong>domain-shift</strong>ã€‚</li>
<li><strong>CNN æŠ½å– semantic feature å¯¼è‡´çš„ coarse representation å’Œ detect small objects éœ€è¦ fine resolution ä¹‹é—´çš„çŸ›ç›¾</strong>ï¼Œsmall objects å› ä¸º smallï¼Œå¾ˆéš¾åœ¨ coarse representation è¿˜æœ‰å¾ˆå¥½çš„è¡¨ç¤ºï¼Œå¾ˆå¯èƒ½å°±è¢«å¿½ç•¥äº†<ul>
<li>The deeper layers of modern CNNs have large strides (32 pixels) that lead to a very coarse representation of the input image, which makes small object detection very challenging. </li>
<li>æ‰€ä»¥æœ¬è´¨ä¸Šæ˜¯å› ä¸º strides å¤ªå¤§å¯¼è‡´çš„ åŸå›¾åƒ çš„è¡¨ç¤ºæ˜¯éå¸¸ coarse çš„è¡¨ç¤ºï¼Œåœ¨è¿™ç§ coarse çš„è¡¨ç¤ºä¸­ï¼Œå°ç›®æ ‡æœ¬èº«å¾ˆå®¹æ˜“å°±ä¼šè¢«å¿½è§†æ‰äº†</li>
<li>å®é™…ä¸Šè¿™ä¸ªé—®é¢˜ï¼Œåœ¨ Semantic Segmentation ä¸­ä¹Ÿæ˜¯å­˜åœ¨çš„ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæœ‰æ—¢æ˜¯ Fine Resolution åˆæ˜¯ Semantic çš„è¡¨ç¤ºï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåé¢çš„ä¸€äº›æ”¹è¿›æ–¹æ³•å’Œ Semantic Segmentation æ–¹æ³•åšæ³•ç›¸åŒçš„åŸå› ã€‚</li>
</ul>
</li>
</ol>
<p>ä¸ºäº† alleviate ç”± scale variation å’Œ small object instances æœ¬èº«å¯¼è‡´çš„é—®é¢˜ï¼Œç›®å‰å¤§æ¦‚æœ‰ä¸‹é¢è¿™äº›æ€è·¯ï¼š</p>
<ol>
<li>features from the layers near to the input, referred to as shallow(er) layers, are combined with deeper layers for detecting small object instances [23, 34, 1, 13, 27]ï¼Œ<ul>
<li>å…¸å‹ä»£è¡¨æ˜¯ FPNã€SSD</li>
<li>è¿™ä¸ªè·¯çº¿å…¶å®ç”¨æ¥å¤„ç†ä¸Šé¢çš„éš¾ç‚¹ 3ï¼Œcoarse representation vs fine resolution çš„</li>
<li>ä½†ä½œè€…ç‚¹å‡ºäº† the high level semantic features (at conv5) generated even by feature pyramid networks will not be useful for classifying small objectsï¼Œé«˜å±‚ç‰¹å¾æ²¡ç”¨å¦‚æœç›®æ ‡å°ï¼Œè¿™ä¸ªå¾ˆåˆç†ï¼Œå› ä¸ºè¿™ä¸ªé€”å¾„å¹¶æ²¡æœ‰æ¥å¤„ç†éš¾ç‚¹ 1 å’Œ éš¾ç‚¹ 2 æ‰€ä»¥å½“ç„¶æŸæ‰‹æ— æª</li>
</ul>
</li>
<li>dilated/deformable convolution is used to increase receptive fields for detecting large objects [32, 7, 37, 8]<ul>
<li>è¿™ä¸ªè·¯çº¿ä¹Ÿæ˜¯ç”¨æ¥å¤„ç†ä¸Šé¢çš„éš¾ç‚¹ 3ï¼Œä¸ºäº†æœ€åæœ‰ä¸€ä¸ª fine resolution çš„ representation</li>
</ul>
</li>
<li>independent predictions at layers of different resolutions are used to capture object instances of different scales [36, 3, 22]<ul>
<li>è¿™ä¸ªè·¯çº¿è¿˜æ˜¯ç”¨æ¥å¤„ç†ä¸Šé¢çš„éš¾ç‚¹ 3ï¼Œåš Prediction çš„æ—¶å€™èƒ½å¤Ÿåœ¨åˆé€‚çš„ Feature æŠ½è±¡ç¨‹åº¦ï¼ˆResolutionï¼‰ä¸Šåš</li>
</ul>
</li>
<li>context is employed for disambiguation [38, 39, 10]<ul>
<li><em>è¿™ä¸ªä¸äº†è§£ï¼Œéœ€è¦å»çœ‹è®ºæ–‡</em></li>
</ul>
</li>
<li>training is performed over a range of scales [7, 8, 15] or, inference is performed on multiple scales of an image pyramid <ul>
<li>è¿™æ¡è·¯çº¿å¯¹äºå°ç›®æ ‡æ¥è¯´å…¶å®ä¹Ÿå°±æ˜¯ ä¸Šé‡‡æ ·ï¼Œå¾ˆæš´åŠ›ä½†ä¹Ÿå¾ˆæœ‰æ•ˆï¼ŒåŒæ—¶æ¥å¤„ç†ä¸Šé¢çš„éš¾ç‚¹ 1 scale variation å’Œ éš¾ç‚¹ 3 ç›®æ ‡å¤ªå°åœ¨ coarse representation ä¸­æ®‹ç•™ä¸å¤šçš„é—®é¢˜ï¼Œå½“ç„¶è¿™ç§æ–¹å¼ä¹Ÿæœ‰é—®é¢˜ï¼Œè¿™ä¸ªä¼šåœ¨åé¢è®¨è®º</li>
</ul>
</li>
<li>predictions are combined using nonmaximum suppression [7, 8, 2, 33]</li>
</ol>
<p>æ€»ä¹‹ï¼Œæ£€æµ‹å°ç›®æ ‡ï¼Œ<strong>è¦ä¹ˆè§£å†³é—®é¢˜</strong>ï¼Œä¹Ÿå°±æ˜¯å¯¹å°ç›®æ ‡åšå¾ˆå¥½çš„ç‰¹å¾è¡¨ç¤ºï¼Œ<strong>è¦ä¹ˆæ¶ˆç­é—®é¢˜æœ¬èº«</strong>ï¼ŒæŠŠå°ç›®æ ‡æ¶ˆç­æ‰ï¼Œç»Ÿç»Ÿ upsampling æˆå¤§ç›®æ ‡ï¼Œåœ¨å¯¹å°ç›®æ ‡è¿›è¡Œ scale-invariant çš„ç‰¹å¾è¡¨ç¤ºæŸæ‰‹æ— ç­–çš„æƒ…å†µä¸‹ï¼Œupsampling ä¼¼ä¹å°±æˆäº†æ¯”è¾ƒå¯è¡Œçš„æ–¹æ¡ˆäº†ã€‚ä¸è¿‡è¿˜éœ€è¦æœ‰å¾ˆå¤šé—®é¢˜è¦ææ¸…æ¥šï¼š</p>
<ul>
<li>upsampling åˆ°åº•æœ‰æ²¡æœ‰ç”¨ï¼Ÿ</li>
<li>åˆ°åº•è¦æ€ä¹ˆåš upsamplingï¼Ÿ</li>
<li>è¦å¯¹è°åš upsamplingï¼Ÿåªå¯¹ trainingï¼Œè¿˜æ˜¯åªå¯¹ testï¼Œè¿˜æ˜¯éƒ½åšï¼Ÿ</li>
<li>å¦‚æœéƒ½åš upsamplingï¼Œå½¼æ­¤åˆè¯¥æ€ä¹ˆç”¨ï¼Ÿéƒ½éå†æ‰€æœ‰å°ºåº¦ä¹ˆï¼Ÿè¿˜æ˜¯è¦å›ºå®šå°ºåº¦ï¼Œä¸ºäº†å’Œ pre-trained datasets çš„å°ºåº¦ä¸€è‡´ã€‚</li>
</ul>
<p>å¯¹åº”åˆ°ä½œè€…åŸæ–‡ä¸­ï¼Œä½œè€…é—®çš„æ˜¯ä¸‹é¢ä¸¤ä¸ªé—®é¢˜ï¼š</p>
<blockquote>
<ul>
<li>Is it critical to upsample images for obtaining good performance for object detection? Even though the typical size of images in detection datasets is 480x640, why is it a common practice to up-sample them to 800x1200? Can we pre-train CNNs with smaller strides on low resolution images from ImageNet and then fine-tune them on detection datasets for detecting small object instances?</li>
<li>When fine-tuning an object detector from a pre-trained image classification model, should the resolution of the training object instances be restricted to a tight range (from 64x64 to 256x256) after appropriately re-scaling the input images, or should all object resolutions (from 16x16 to 800x1000, in the case of COCO) participate in training after up-sampling input images?</li>
</ul>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…ä¾æ¬¡å›ç­”äº†ä¸Šé¢è¿™äº›é—®é¢˜ï¼š</p>
<ul>
<li>é¦–å…ˆï¼Œup-sampling å¯¹äº small object detection æ¥è¯´éå¸¸é‡è¦ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå¯¹äº detection datasets, it is a common practice to up-sample 480x640 to 800x1200ã€‚</li>
<li>pre-train CNNs with smaller strides on low resolution images from ImageNet ç„¶åå† fine-tune them on detection datasets for detecting small object instances è¿™ç§æ˜¯æ–¹å¼æ˜¯å¯ä»¥çš„è€Œä¸”æ˜¯æœ¬æ–‡æå€¡çš„ï¼Œåªä¸è¿‡ fine-tuning å’Œ test éƒ½è¦åœ¨æœ¬æ–‡æå‡ºçš„ç‰¹æ®Šçš„ Pyramid ä¸Šåš</li>
<li>ä¸ºäº†æ¶ˆé™¤ domain-shiftï¼Œåœ¨åš fine-tuning çš„æ—¶å€™ï¼Œéœ€è¦å°† training object instances å¤§å°é™åˆ¶åœ¨ a tight range (from 64x64 to 256x256) ä»¥ä¿æŒä¸ pre-trained datasets çš„ object scales ä¸€è‡´è¿™ç§æ–¹å¼æ•ˆæœæœ€å¥½ï¼Œè€Œä¸æ˜¯ all object resolutions (from 16x16 to 800x1000, in the case of COCO) éƒ½å‚ä¸åˆ°è®­ç»ƒä¸­ã€‚</li>
</ul>
<p>å› æ­¤ï¼Œç»¼ä¸Šæ‰€è¿°ï¼Œæœ¬æ–‡çš„è´¡çŒ®æˆ–è€…è¯´ argument å°±åœ¨äºæå€¡è®­ç»ƒ detector çš„æ—¶å€™è¦ç”¨ Pyramidï¼Œä½†åªæœ‰å›ºå®šå°ºåº¦å†…çš„ç›®æ ‡æ‰è¢«æ‹¿æ¥å‚ä¸è®­ç»ƒï¼Œä½œè€…æŠŠè¿™ç§<strong>è®­ç»ƒæ–¹å¼</strong>å«ä½œ <strong>Scale Normalization for Image Pyramids (SNIP)</strong>ã€‚æœ¬æ–‡æœ¬è´¨ä¸Šæ˜¯ä¸€ç¯‡è®¨è®ºæ€ä¹ˆæ¥ä½¿ç”¨ Image Pyramid çš„è®ºæ–‡ï¼Œæ•…è€Œåé¢çš„è®ºæ–‡éƒ½æ˜¯æ¯”è¾ƒä¸åŒçš„ Image Pyramid ä½¿ç”¨æ–¹å¼çš„ã€‚</p>
<p>æœ€å…¸å‹çš„å°±æ˜¯ä¸‹é¢ä¸¤ç§ä½¿ç”¨æ–¹å¼ï¼š</p>
<ol>
<li><strong>scale-specific detectors</strong>ï¼švariation in scale is handled by training separate detectors - one for each scale range.<ul>
<li>ä¸€ä¸ª detector è´Ÿè´£ä¸€ä¸ª scale çš„ objects</li>
<li><strong>è¿™é‡Œçš„æ ·æœ¬åº”è¯¥æ˜¯æ²¡æœ‰åšè¿‡ Image Pyramid çš„ Datasets</strong>ï¼Œè¿™æ ·çš„è¯ï¼Œå¯¹äºæ¯ä¸ª scale æ¥è¯´ï¼Œæ ·æœ¬æ•°é‡å°±å‡å°‘äº†ï¼Œè®­ç»ƒæ ·æœ¬å°‘äº†ï¼Œå¯¹äºè®­ç»ƒä¸€ä¸ª detector æ¥è¯´ï¼Œå¹¶æ²¡æœ‰æŠŠå…¨éƒ¨çš„ samples ç”¨ä¸Š</li>
</ul>
</li>
<li><strong>scale invariant detector</strong>ï¼štraining a single object detector with all training samples<ul>
<li>è¿™ä¸ªè™½ç„¶å«ä½œ scale invariant detectorï¼Œå…¶å®ä¸è¿‡åªæ˜¯ä¸€ä¸ªç¾å¥½çš„æœŸè®¸è€Œå·²ï¼Œå®é™…ä¸Š CNN æœ¬èº«æ˜¯æ²¡æœ‰ scale invariance è¿™ä¸ªæ€§è´¨çš„ã€‚å³ä½¿æœ€åè¡¨ç°å‡ºäº†ä¸€å®šçš„èƒ½å¤Ÿæ£€æµ‹ multi-scale object çš„èƒ½åŠ›ï¼Œä½†è¿™åªæ˜¯ã€Œã€å‡è±¡ã€ï¼Œé‚£ä¸è¿‡ CNN æ˜¯ç”¨å…¶å¼ºå¤§çš„æ‹Ÿåˆèƒ½åŠ›æ¥å¼ºè¡Œ memorize ä¸åŒ scale çš„ç‰©ä½“æ¥è¾¾åˆ°çš„capacity æ¥å¼ºè¡Œmemorize ä¸åŒ scale çš„ç‰©ä½“æ¥è¾¾åˆ°çš„capacityï¼Œè¿™å…¶å®æµªè´¹äº†å¤§é‡çš„ capacityã€[1]ï¼Œä¹Ÿå°±æ˜¯è¯´ capacity å¹¶æ²¡æœ‰è¢«ç”¨åˆ°è¯¥ç”¨çš„åœ°æ–¹å»</li>
</ul>
</li>
</ol>
<p>æ‰€ä»¥ï¼Œè¿™é‡Œå°±æœ‰ä¸€ä¸ªå–èˆäº†ï¼Œscale-specific detector æ²¡æœ‰ç”¨ä¸Šå…¨éƒ¨ samples å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼›scale invariant detector æµªè´¹äº†å¤§é‡çš„ capacity æ¥å¼ºè¡Œ memorize ä¸åŒ scale çš„ç‰©ä½“ï¼Œè€Œä¸æ˜¯ç”¨æ¥å­¦ä¹ è¯­ä¹‰ä¿¡æ¯ï¼Œä¹Ÿä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ€å¥½çš„å½“ç„¶æ˜¯ï¼Œä¸åšå–èˆï¼Œä¸¤ä¸ªéƒ½è¦ï¼Œå³èƒ½ç”¨ä¸Šå…¨éƒ¨ samplesï¼Œè€Œä¸”ä¸å°† capacity æµªè´¹åœ¨å¼ºè¡Œ memorize ä¸åŒ scale çš„ç‰©ä½“ä¸Šã€‚å®é™…ä¸Šï¼Œè¿™ä¸ªæ˜¯å¯ä»¥åšåˆ°çš„ã€‚</p>
<p>æœ¬æ–‡çš„ SNIPï¼Œé€šè¿‡ Image Pyramidï¼Œä½¿å¾—æ¯ä¸ª Object éƒ½èƒ½æœ‰ä¸€ä¸ªè½åœ¨ä¸ Pre-trained çš„ ImageNet æ•°æ®é›†çš„ Scale ç›¸åŒçš„è¡¨è¾¾ï¼Œå¹¶ä¸”åªå¯¹ç»è¿‡ Image Pyramid åä¸ Pre-trained çš„ ImageNet æ•°æ®é›†çš„ Scale ç›¸åŒçš„ Sample è¿›è¡Œè®­ç»ƒï¼Œæ—¢ä¿è¯äº†ç”¨ä¸Šå…¨éƒ¨ samplesï¼Œåˆå°†capacity éƒ½ç”¨åœ¨äº†å­¦ä¹ è¯­ä¹‰ä¿¡æ¯ä¸Šã€‚</p>
<h2 id="è®ºè¯"><a href="#è®ºè¯" class="headerlink" title="è®ºè¯"></a>è®ºè¯</h2><p>ä½œè€…åœ¨ã€Œ3. Image Classification at Multiple Scalesã€å’Œã€Œ5. Data Variation or Correct Scale?ã€ä¸¤å¤„å®‰æ’äº†ä¸¤ä¸ªè®ºè¯å®éªŒã€‚</p>
<h3 id="Fining-tuning-whether-or-not"><a href="#Fining-tuning-whether-or-not" class="headerlink" title="Fining-tuning, whether or not?"></a>Fining-tuning, whether or not?</h3><p>ã€Œ3. Image Classification at Multiple Scalesã€è¿™ä¸€èŠ‚ç ”ç©¶ domain shift çš„å½±å“ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œä½œè€…å…¶å®è¿˜è¦å›ç­”å¦å¤–ä¸€ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯æ—¢ç„¶ domain-shift æœ‰å½±å“ï¼Œé‚£è¿˜è¦ä¸è¦é‡‡ç”¨ fine-tuning è¿™ç§æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯æ‹¿ pre-trained weights åšåˆå§‹åŒ–ï¼Œ<strong>ç›´æ¥åœ¨ object detection çš„ target dataset ä¸Š train from scratch ä¸å¥½å—ï¼Ÿ</strong></p>
<p>ä½œè€…å®‰æ’äº†ä¸‰ä¸ªè®ºè¯å®éªŒï¼Œæœ€åè¯æ˜äº†å³ä½¿æœ‰ domain shiftï¼Œè¿˜æ˜¯åº”è¯¥è¦é‡‡ç”¨ pre-trained + fine-tuning è¿™ç§æ–¹å¼ã€‚ä¹Ÿå°±æ˜¯å›ç­”äº†ä½œè€…ä¸€å¼€å§‹æå‡ºæ¥çš„é—®é¢˜ï¼š</p>
<blockquote>
<p>Can we pre-train CNNs with smaller strides on low resolution images from ImageNet and then fine-tune them on detection datasets for detecting small object instances?</p>
</blockquote>
<p>ç­”æ¡ˆæ˜¯ yes, we can.</p>
<p>æ­¤å¤–ï¼Œå…¶å® domain shift ä¸ä»…ä»…æ˜¯åœ¨ pre-trained datasets å’Œ target datasets ä¹‹é—´å­˜åœ¨ï¼Œå…¶å®æˆ‘ä»¬åœ¨åš Test çš„æ—¶å€™ï¼Œä¸ºäº†æ£€æµ‹å°ç›®æ ‡é€šå¸¸ä¼šåš Image Pyramidï¼Œä¼šç¼©å°ã€æ”¾å¤§å›¾åƒï¼Œè¿™ä¸ªæ—¶å€™ï¼ŒTest çš„ Pyramid é‡Œé¢çš„ object ä¹Ÿä¼šè·Ÿ Training æ—¶å€™çš„ object scale ä¸ä¸€è‡´ï¼Œæ‰€ä»¥è¿™é‡Œå°±æé†’æˆ‘ä»¬ä¸€ç‚¹ï¼Œ<strong>åœ¨ä½¿ç”¨ Image Pyramid çš„æ—¶å€™è¿˜è¦è€ƒè™‘ domain shift çš„å½±å“</strong>ã€‚</p>
<p>å› æ­¤ï¼ŒPre-trained Data ä¸ Training Data ä¹‹é—´æœ‰ domain shiftï¼ŒTraining Data ä¸ Test Data ä¹‹é—´ä¹Ÿä¼šæœ‰ domain shift. ä½†è¿™ä¸¤ä¸ªè™½ç„¶éƒ½å« Domain Shiftï¼Œå…¶å®è¿˜æœ‰ç‚¹ä¸åŒï¼ŒPre-trained Data ä¸ Training Data ä¹‹é—´æœ‰ domain shift æ˜¯ç”±äº Object åœ¨åŸæœ‰ Resolution ä¸‹æœ¬èº«çš„ Scale åˆ†å¸ƒé€ æˆçš„ï¼›è€Œ Training Data ä¸ Test Data ä¹‹é—´çš„ domain shiftï¼Œåˆ™æ˜¯ç”± Test æ—¶å€™é‡‡ç”¨ Image Pyramid é€ æˆçš„ã€‚</p>
<h4 id="Naive-Multi-Scale-Inference"><a href="#Naive-Multi-Scale-Inference" class="headerlink" title="Naive Multi-Scale Inference"></a>Naive Multi-Scale Inference</h4><ol>
<li><strong>è¿™ä¸ªå®éªŒæ‰€é‡‡ç”¨çš„æ˜¯ç›´æ¥æ‹¿åœ¨ Full Resolution çš„æ•°æ®é›†ä¸Šå¾—åˆ°çš„ Pre-trained Weights æ‹¿æ¥åº”ç”¨äº Target Datasetï¼Œä¸åš Fine-tuningã€‚</strong></li>
<li>ä½†æ˜¯å¯¹äº Detectionï¼Œè¿™ä¸ªå®éªŒçš„å¯ç¤ºæ˜¯ Training Data ä¸ Test Data ä¹‹é—´ç”±äº Image Pyramid é€ æˆçš„ domain shift çš„å½±å“ã€‚</li>
<li>è¿™ä¸ªå®éªŒæ˜¯åœ¨åŸå°ºå¯¸çš„ ImageNet ä¸Š Trainingï¼Œç„¶ååœ¨ç»è¿‡ down-sampling å† up-sampling çš„å›¾åƒä¸Šåš Testï¼›</li>
<li>å¯¹åŸå›¾åƒåš down-sampling æ˜¯ä¸ºäº†è·å¾— low-resolution çš„å›¾ï¼Œå†æŠŠ low-resolution çš„å›¾ up-sampling æˆè·Ÿ training image ä¸€æ ·å¤§å°æ˜¯ä¸ºäº†æ¨¡æ‹Ÿ Pyramid é‡Œé¢çš„ up-sampling çš„è¡Œä¸ºï¼Œå› ä¸º Detection æœ€åè¿˜æ˜¯å¯¹ä¸€ä¸ª Region Proposal çš„åŒºåŸŸåš Classificationï¼Œå› æ­¤ï¼Œè¿™ä¸ªå®éªŒè™½è¯´æ˜¯åœ¨å®¡è§† Training Set å’Œ Test Set åœ¨ Resolution ä¸Šçš„å·®å¼‚å¯¹ Classification çš„å½±å“ï¼Œä½†å…¶å®ä¹Ÿè§£é‡Šäº†åš Detection çš„æ—¶å€™ï¼ŒTraining Set å’Œ Test Set åœ¨ Resolution ä¸Šçš„å·®å¼‚çš„å½±å“ã€‚</li>
<li>è¿™é‡Œçš„ Resolution æŒ‡çš„æ˜¯å›¾åƒçš„æ¸…æ™°ç¨‹åº¦ã€‚</li>
<li>ç»“è®ºè‡ªç„¶æ˜¯ Training Set å’Œ Test Set çš„ Resolution å·®å¼‚è¶Šå¤§ï¼Œæ•ˆæœè¶Šå·®ï¼Œå› æ­¤è¦ä¿è¯ Training Set å’Œ Test Set çš„ Resolution ä¸€è‡´ã€‚</li>
</ol>
<blockquote>
<p><strong>è¯´æ˜ç›´æ¥æ”¾å¤§å°ç‰©ä½“å»æµ‹è¯•æ˜¯ä¸è¡Œçš„ï¼Œæ˜¯è¦æŠŠæ”¾å¤§åçš„å°ç‰©ä½“çœŸæ­£å‚ä¸åˆ°è®­ç»ƒé‡Œã€‚</strong></p>
</blockquote>
<h4 id="Resolution-Specific-Classifiers"><a href="#Resolution-Specific-Classifiers" class="headerlink" title="Resolution Specific Classifiers"></a>Resolution Specific Classifiers</h4><ol>
<li><strong>è¿™ä¸ªå®éªŒæ‰€é‡‡ç”¨çš„æ˜¯ç›´æ¥åœ¨ Low Resolution çš„ Target Dataset ä¸Š Training from scratchï¼Œä¸åš pre-trainingã€‚</strong></li>
<li>Naive Multi-Scale Inference è¿™ä¸ª Network æ˜¯åº”ç”¨äº Full Resolution æ•°æ®çš„ç½‘ç»œï¼Œç½‘ç»œæœ¬èº«ç›¸å¯¹å¤æ‚ï¼ŒCNN-B çš„ B åº”è¯¥æ˜¯ Base çš„æ„æ€å§ï¼Œä¹Ÿå°±æ˜¯åŸºå‡†ç½‘ç»œï¼Œæ¨¡æ‹Ÿçš„æ˜¯åœ¨ Full Resolution ä¸Šè®­ç»ƒçš„åŸºå‡†ç½‘ç»œåœ¨ Low Resolution å›¾åƒä¸Šæµ‹è¯•çš„æ•ˆæœã€‚</li>
<li>Resolution Specific Classifiers è¿™ä¸ª Networkï¼Œæ˜¯åœ¨ Low Resolution æ•°æ®ä¸Šè®­ç»ƒå¹¶åœ¨ Low Resolution æ•°æ®ä¸Šæµ‹è¯•ï¼Œä½†æ˜¯ä¸ºäº†èƒ½å¤Ÿè®©ç½‘ç»œåº”ç”¨åœ¨ Low Resolution çš„å›¾åƒä¸Šï¼Œé‡‡ç”¨çš„æ˜¯ Simplified networkï¼Œæ‰€ä»¥å« CNN-Sã€‚æ­¤æ—¶ï¼Œè™½ç„¶ Training Data å’Œ Test Data çš„ Resolution ä¸€è‡´äº†ï¼Œä½†æ˜¯å› ä¸º Network ç®€å•äº†ï¼Œcapacity å¼±äº†ï¼Œä¹Ÿä¼šé€ æˆé¢„æµ‹æ•ˆæœä¸å¥½ã€‚</li>
<li>è¿™æ—¶å€™å°±è¦çœ‹ï¼Œç©¶ç«Ÿæ˜¯ ç®€åŒ–ç½‘ç»œé€ æˆçš„é¢„æµ‹æ•ˆæœä¸å¥½å½±å“å¤§ï¼Œè¿˜æ˜¯ Training å’Œ Test æ•°æ®çš„ Resolution ä¸ä¸€è‡´çš„ Domain Shift å¯¹é¢„æµ‹æ•ˆæœä¸å¥½çš„å½±å“å¤§äº†ï¼Œä»å®éªŒç»“æœä¸Šçœ‹ï¼ŒCNN-S è¿œå¥½äº CNN-Bï¼Œæ³¨æ„è¿™é‡Œçš„å‰ææ˜¯ æ•°æ®å……è¶³ã€‚</li>
<li>å› æ­¤å¯ä»¥å¾—åˆ°çš„ç»“è®ºæ˜¯ï¼Œåœ¨æ•°æ®å……è¶³çš„å‰æä¸‹ï¼ŒDomain Shift ä¼šé€ æˆå¾ˆå¤§çš„æ€§èƒ½æŸå¤±ï¼Œä¹Ÿå°±æ˜¯è¯´ CNN å¹¶æ²¡æœ‰å­¦ä¹  Scale Invariance çš„èƒ½åŠ›ï¼Œå¯ä»¥é‡è§å³ä½¿åœ¨ Image Pyramid åš Test çš„æ—¶å€™ï¼ŒCNN å¯¹äºåœ¨ Training æ²¡è§è¿‡çš„ Scale çš„ Object çš„æ—¶å€™ï¼Œæ•ˆæœä¼šå¾ˆå·®ï¼Œè¿™å…¶å®ä¹Ÿè¯´æ˜äº†ä¸€å®šè¦è®© Training Data å’Œ Test Data åœ¨ä¸€ä¸ªå°ºåº¦çš„é‡è¦æ€§ã€‚</li>
</ol>
<h4 id="Fine-tuning-High-Resolution-Classifiers"><a href="#Fine-tuning-High-Resolution-Classifiers" class="headerlink" title="Fine-tuning High-Resolution Classifiers"></a>Fine-tuning High-Resolution Classifiers</h4><ol>
<li><strong>è¿™ä¸ªå®éªŒæ‰€é‡‡ç”¨çš„æ˜¯å…ˆåœ¨ Full Resolution çš„ Pre-trained Dataset ä¸Šåš Pre-trainingï¼Œç„¶åå†åœ¨ Low Resolution çš„ Target Dataset ä¸Šåš Fine-tuningã€‚</strong>å½“ç„¶ä¸ºäº†è¾“å…¥ç½‘ç»œï¼ŒLow Resolution Image è¦åšä¸‹ Upsampling.</li>
<li>å› ä¸ºè¿™ä¸ªæ˜¯åœ¨ CNN-B çš„åŸºç¡€ä¸Šåšäº† Fine-tuningï¼Œå› æ­¤å«ä½œ CNN-B-FTã€‚</li>
<li>CNN-B-FT çš„æ•ˆæœæ˜æ˜¾å¥½äº CNN-Sï¼Œè¿™è¯´æ˜ä¸ºäº† Low Resolution Data å»å‰Šè¶³é€‚å±¥é‡‡ç”¨ low capacity çš„ç®€å•ç½‘ç»œï¼Œä¸å¦‚è¿˜æ˜¯é‡‡ç”¨ Pre-trained on Full Resolution Dataset + Fine-tuning on Low Resolution Dataset è¿™ç§æ–¹å¼ã€‚</li>
<li>å…¶å®è¿™æ˜¯å¾ˆåˆç†çš„ï¼Œç›¸æ¯” Learning from Scratch çš„éšæœºåˆå§‹åŒ–æƒé‡ï¼ŒPre-trained weights è‡³å°‘ç»™äº†ä¸€ä¸ªåˆé€‚çš„æƒé‡åˆå§‹åŒ–ã€‚åæ­£æœ€åè¿˜æ˜¯è¦åœ¨ Target Dataset ä¸Šåšã€‚ä½†è¦æ³¨æ„ï¼ŒFine-tuning çš„æ—¶å€™ï¼ŒTarget Dataset æ˜¯è¢« up-scaling äº†è·Ÿ Pre-trained Dataset ä¸€æ ·çš„å¤§å°ã€‚è¿™æ ·åšåº”è¯¥æ˜¯ä¸ºäº†ä¿è¯ pre-trained datasets å’Œ target datasets ä¹‹é—´çš„ object å¤§å°ä¸€è‡´ã€‚</li>
</ol>
<!--down-sample æ˜¯ä¸ºäº†è·å¾— small objectsï¼Œå†æŠŠ down-sampled çš„å›¾åƒ up-sample æ˜¯ä¸ºäº†æ¨¡æ‹Ÿ small objects è¢« up-sampling çš„æƒ…å½¢ã€‚
2. 
down-sample æ˜¯ä¸ºäº†è·å¾— small objectsï¼Œå†æŠŠ down-sampled çš„å›¾åƒ up-sample æ˜¯æ¨¡æ‹Ÿ small objects è¢« up-sampling çš„æƒ…å½¢ã€‚-->
<h3 id="Fine-tuning-how"><a href="#Fine-tuning-how" class="headerlink" title="Fine-tuning, how?"></a>Fine-tuning, how?</h3><h4 id="Training-on-800-x-1400ï¼Œtest-on-1400-x-2000"><a href="#Training-on-800-x-1400ï¼Œtest-on-1400-x-2000" class="headerlink" title="Training on 800 x 1400ï¼Œtest on 1400 x 2000"></a>Training on 800 x 1400ï¼Œtest on 1400 x 2000</h4><ol>
<li>è¿™ä¸ªæ˜¯æ¨¡æ‹Ÿä»…ä»… inference is performed on multiple scales of an image pyramidï¼›åœ¨ 800 x 1400 çš„å›¾ç‰‡ä¸Š Trainingï¼Œç„¶ååœ¨ 1400 x 2000 ä¸Šçš„å›¾ç‰‡ä¸Šåš Test æ˜¯ä¸ºäº†æ£€æµ‹å°ç›®æ ‡å¸¸å¸¸é‡‡ç”¨çš„æ˜¯ç­–ç•¥ã€‚</li>
<li>è¿™æ˜¯åŸºå‡†ï¼Œåé¢çš„éƒ½è¦è·Ÿè¿™ä¸ªæ¯”ï¼Œè¿™ä¸ªå«åš 800-allã€‚</li>
</ol>
<h4 id="Training-on-1400-x-2000ï¼Œtest-on-1400-x-2000"><a href="#Training-on-1400-x-2000ï¼Œtest-on-1400-x-2000" class="headerlink" title="Training on 1400 x 2000ï¼Œtest on 1400 x 2000"></a>Training on 1400 x 2000ï¼Œtest on 1400 x 2000</h4><ol>
<li>è¿™ä¸ª upsampling äº† å°ç›®æ ‡ï¼Œè€Œä¸” Training å’Œ Test åœ¨åŒä¸€å°ºåº¦ä¸Šï¼Œä½†æœ€åçš„æ•ˆæœä»…ä»…æ¯” 800-all å¥½äº†ä¸€ç‚¹ç‚¹ï¼Œå¯ä»¥å¿½ç•¥çš„ä¸€ç‚¹ã€‚</li>
<li>ä½œè€…ç»™çš„è¯å°±æ˜¯ up-sampling ä¼š blows up the medium-to-large objects which degrades performanceï¼Œmedian size object become too big to be correctly classified!</li>
<li>æˆ‘è‡ªå·±çš„ç†è§£æ˜¯ up-samplingï¼Œè™½ç„¶å‡å°äº†å°ç›®æ ‡åœ¨ target dataset ä¸ pre-trained dataset ä¹‹é—´ domain shiftï¼Œä½†æ˜¯åˆå¢åŠ äº† medium size çš„ objects åœ¨ target dataset ä¸ pre-trained dataset ä¹‹é—´ domain shiftï¼Œå¤§é‡ median objects å˜æˆäº†è¶…å¤§ç›®æ ‡ï¼Œ scale å’Œ ImageNet è¿™æ ·çš„ pretrained dataset ä¸Šå¤§éƒ¨åˆ†ç›®æ ‡çš„ scale ä¸ä¸€è‡´</li>
</ol>
<h4 id="Scale-specific-detectors"><a href="#Scale-specific-detectors" class="headerlink" title="Scale specific detectors"></a>Scale specific detectors</h4><ol>
<li>ä¸ºäº†å»é™¤ Scale Variation è®© CNN æŠŠèƒ½åŠ›éƒ½ç”¨åœ¨ Memorizing è€Œä¸æ˜¯ Learning Semantic ä¸Šå¸¦æ¥çš„æ€§èƒ½ä¸‹é™ï¼Œä½œè€…åªå¯¹ä¸€å®š scale çš„å°ç›®æ ‡åšäº†è®­ç»ƒï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰äº† scale variationï¼Œä½† training data çš„æ•°é‡å‡å°‘äº†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™æ¯” 800-all çš„æ•ˆæœè¿˜è¦å·®ï¼Œå› ä¸ºå»é™¤æ‰äº† median-to-large çš„ objectsï¼Œå¹¶ä¸æœ‰åˆ©äº CNN å­¦ä¹ è¯­ä¹‰ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå»æ‰ä¸€äº› scale çš„æ ·æœ¬ä¸åˆ©äºå­¦ä¹ è¯­ä¹‰ï¼Œå¡ç»™ CNN å„ç§ scale çš„æ ·æœ¬è®©å®ƒå»å¼ºè¡Œè®°å¿†ä¹Ÿä¸åˆ©äº CNN å­¦ä¹ è¯­ä¹‰ã€‚</li>
</ol>
<blockquote>
<p>å•çº¯åªç”¨å°ç‰©ä½“æ•ˆæœä¹Ÿä¸å¥½æ˜¯å› ä¸ºæ•°æ®ä¸è¶³ï¼Œå¤§ç‰©ä½“å…¶å®å¯¹äºè¯­ä¹‰ä¿¡æ¯æ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚ä½ åªç”¨äº†éƒ¨åˆ†æ•°æ®è¿˜ä¸å¦‚å…¨ç”¨äº†è™½ç„¶ç”¨çš„ä¸ç‰¹åˆ«å¥½ã€‚</p>
</blockquote>
<h4 id="Multi-Scale-Training-MST"><a href="#Multi-Scale-Training-MST" class="headerlink" title="Multi-Scale Training (MST)"></a>Multi-Scale Training (MST)</h4><ol>
<li>ç”¨ Image Pyramid ç”Ÿæˆå¤šä¸ª Resolutionï¼Œç„¶åç”¨ä¸€ä¸ª CNN å» fit æ‰€æœ‰è¿™äº›ä¸åŒ Resolution çš„ objectï¼Œæœ€åçš„ç»“æœæ˜¯è·Ÿ 800-all å·®ä¸å¤šã€‚</li>
<li>è¯´æ˜ CNN æ²¡æœ‰å­¦ä¹  Scale Invariance çš„èƒ½åŠ›ï¼Œå¼ºè¡Œè®©å®ƒè®°ä½ä¸åŒå°ºå¯¸çš„ç›®æ ‡ï¼Œä¼šæŸå®³å®ƒ Learning Semantic çš„èƒ½åŠ›ï¼Œä»è€Œè™½ç„¶ Data ç»è¿‡ Image Pyramid æ•°é‡å¢åŠ äº†ä¼šå¸¦æ¥ä¸€ç‚¹å¢ç›Šï¼Œä¹Ÿéšç€ Learn åˆ°çš„ Semantic èƒ½åŠ›çš„æŸå¤±ä¸‹é™äº†ã€‚</li>
<li>è¿™è¦è¦æ±‚æˆ‘ä»¬ç†æƒ³çš„ detectorï¼Œå³èƒ½å¤Ÿåˆ©ç”¨ä¸Šæ‰€æœ‰çš„æ ·æœ¬ï¼Œä½†å–‚å®ƒçš„è¿™äº›æ ·æœ¬åˆèƒ½å¤Ÿéƒ½å¤„äºåˆé€‚çš„å°ºåº¦å†…ï¼Œä»è€Œèƒ½å¤Ÿè®© CNN æŠŠèƒ½åŠ›éƒ½æ”¾åœ¨ Learning Semantic Information ä¸Šã€‚</li>
</ol>
<blockquote>
<p>æ‰€ä»¥ç”± DNN å­¦åˆ°çš„ç‰¹å¾ä¸å…·æœ‰ï¼š æ—‹è½¬ä¸å˜æ€§ï¼Œå°ºåº¦ä¸å˜æ€§ï¼Ÿéƒ½æ˜¯æ•°æ®å †èµ·æ¥çš„å‡è±¡ï¼Œæˆ–è€…è¯´æ˜¯é€šè¿‡ capacity ç”±ä¸åŒ neuron æ­»è®°ç¡¬èƒŒ</p>
</blockquote>
<h2 id="ç»“è®º"><a href="#ç»“è®º" class="headerlink" title="ç»“è®º"></a>ç»“è®º</h2><ol>
<li>å¯¹äº Scale-Variationï¼Œæœ‰ä¸¤ç§æ€è·¯ï¼Œä¸€ç§æ˜¯å¢å¤§å­¦åˆ° Scale-Variation çš„èƒ½åŠ›ï¼Œä»è€Œèƒ½å¤Ÿ handle Scale-Variationï¼Œå¦ä¸€ç§æ˜¯ å‡å°‘é¢å¯¹æ•°æ®ä¸­çš„ Scale-Variationï¼Œè¿™æ ·å°±ç›¸å½“äºæŠŠä»»åŠ¡ç»™ simplified çš„äº†ã€‚ä½œè€…é‡‡ç”¨äº†åé¢ä¸€ç§ï¼Œå¯ä»¥è¯´æ˜¯ç®€å•ç²—æš´ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯æ²»æ ‡ä¸æ²»æœ¬ã€‚</li>
<li>å¦‚æœè¦æƒ³èµ‹äºˆ CNN å°ºåº¦ä¸å˜æ€§ï¼Œè¿˜æ˜¯è¦è€ƒè™‘æ€ä¹ˆæ ·çš„ç»“æ„åœ¨è®¾è®¡ä¸Šè€ƒè™‘äº† scale invarianceï¼Œä»¥åŠæ€ä¹ˆä» data ä¸­æŠ½å–å‡ºæˆ–è€…è¯´å­¦ä¹ åˆ°è¿™ä¸ª scale invarianceã€‚</li>
<li>é™¤äº†å°ºåº¦ä¸å˜æ€§ï¼ŒCNN å…¶å®ä¹Ÿå­¦ä¸åˆ°æ—‹è½¬ä¸å˜æ€§ï¼Œå¦‚æœä½ çš„ target dataset é‡Œé¢æ—‹è½¬ä¸å˜æ€§å¾ˆé‡è¦ï¼Œé‚£å¯ä»¥è€ƒè™‘é‡‡å–è·Ÿæœ¬æ–‡ä¸€æ ·çš„æ“ä½œã€‚</li>
</ol>
<h2 id="æ„Ÿæƒ³"><a href="#æ„Ÿæƒ³" class="headerlink" title="æ„Ÿæƒ³"></a>æ„Ÿæƒ³</h2><p>æˆ‘å¾ˆå–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œå®ƒç»™äº†æˆ‘ä»¬è¿™äº›åšåº”ç”¨çš„äººä¸€ä¸ªæ¸…æ™°çš„æ€ä¹ˆåšåº”ç”¨ç ”ç©¶çš„èŒƒå¼ã€‚é€šè¿‡ä»”ç»†åˆ†æç°åœ¨å­˜åœ¨çš„é—®é¢˜èƒŒåçš„åŸå› ï¼Œç„¶åæ‰¾å‡ºå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜çš„æ‰‹æ®µï¼Œè€Œä¸æ˜¯å †å ä¸€äº› fancy æ—¶é«¦çš„ä¸œè¥¿ï¼Œæ˜¯å€¼å¾—æˆ‘å­¦ä¹ çš„æ¦œæ ·ğŸ‘ã€‚</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>ä¸‹é¢æ˜¯ä¸€äº›å†™è¿™ç¯‡ç¬”è®°æ—¶çš„ä¸€äº›å‚è€ƒèµ„æ–™ï¼Œå¯¹æˆ‘å°è¯•ç†è§£ SNIP æä¾›äº†å¾ˆå¤§çš„å¸®åŠ©ã€‚</p>
<p>[1] <a href="https://zhuanlan.zhihu.com/p/36431183" target="_blank" rel="noopener">CVPR18 Detection æ–‡ç« é€‰ä»‹ï¼ˆä¸‹ï¼‰</a><br>[2] <a href="https://zhuanlan.zhihu.com/p/35956039" target="_blank" rel="noopener">ç›®æ ‡æ£€æµ‹è®ºæ–‡é˜…è¯»ï¼šAn Analysis of Scale Invariance in Object Detection â€“ SNIP</a><br>[3] <a href="https://arleyzhang.github.io/articles/f0c1556d/" target="_blank" rel="noopener">ç›®æ ‡æ£€æµ‹ - SNIPER-Efficient Multi-Scale Training - è®ºæ–‡ç¬”è®°</a></p>
<hr>
<p>å¦‚æœæ‚¨è§‰å¾—æˆ‘çš„æ–‡ç« å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œä¸å¦¨å°é¢æåŠ©ä¸€ä¸‹ï¼Œæ‚¨çš„é¼“åŠ±æ˜¯æˆ‘é•¿æœŸåšæŒçš„ä¸€å¤§åŠ¨åŠ›ã€‚</p>
<table>
<thead>
<tr>
<th style="text-align:left"><img src="http://ohm5uq281.bkt.clouddn.com/2018-01-23-Alipay_Middle.jpg" alt="Alipay_Middle"></th>
<th style="text-align:left"><img src="http://ohm5uq281.bkt.clouddn.com/2018-01-23-Wechat_Middle.png" alt="Wechat_Middle"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
</div></div><div class="post-main post-comment"><div id="disqus_thread"></div><script type="text/javascript">
var disqus_shortname = 'YimianDai';
var disqus_identifier = 'SNIP/';
var disqus_title = 'Notes on Scale Normalization for Image Pyramids (SNIP)';
var disqus_url = 'http://lowrank.science/SNIP/';
(function() {
   var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
   dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
   (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">Blog comments powered by <span class="logo-disqus">Disqus</span></a></div></article><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"/><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=
function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;
e=o.createElement(i);r=o.getElementsByTagName(i)[0];
e.src='//www.google-analytics.com/analytics.js';
r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
ga('create','UA-88794833-1');ga('send','pageview');</script></body></html>